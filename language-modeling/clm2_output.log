LoRA: Fine-tuning openai-community/gpt2 on wikitext (wikitext-2-raw-v1) using baseline model
11/26/2024 08:58:26 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/26/2024 08:58:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm2-gpt2-lora-wikitext-2-raw-v1/runs/Nov26_08-58-26_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm2-gpt2-lora-wikitext-2-raw-v1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm2-gpt2-lora-wikitext-2-raw-v1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
11/26/2024 08:58:26 - WARNING - datasets.load - Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 09:26:37 2024).
11/26/2024 08:58:26 - WARNING - datasets.packaged_modules.cache.cache - Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 09:26:37 2024).
Using custom data configuration wikitext-2-raw-v1
11/26/2024 08:58:26 - INFO - datasets.builder - Using custom data configuration wikitext-2-raw-v1
Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
11/26/2024 08:58:26 - INFO - datasets.info - Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
Overwrite dataset info from restored data version if exists.
11/26/2024 08:58:26 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/26/2024 08:58:26 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-26 08:58:26,674 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 08:58:26,677 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-26 08:58:26,695 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 08:58:26,695 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-26 08:58:26,695 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 08:58:26,695 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 08:58:26,695 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 08:58:26,695 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-26 08:58:26,849 >> loading weights file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-26 08:58:26,853 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-26 08:58:26,897 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-26 08:58:26,897 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-26 08:58:26,898 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-26 08:58:26,898 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-615616afca6f919c.arrow
11/26/2024 08:58:26 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-615616afca6f919c.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-69dfca79ecfc54e4.arrow
11/26/2024 08:58:26 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-69dfca79ecfc54e4.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-8741e82e5c31cd2f.arrow
11/26/2024 08:58:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-8741e82e5c31cd2f.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-3652bd785f05d459.arrow
11/26/2024 08:58:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-3652bd785f05d459.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-de9407b308ea1602.arrow
11/26/2024 08:58:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-de9407b308ea1602.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e42630efa0f0b097.arrow
11/26/2024 08:58:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e42630efa0f0b097.arrow
11/26/2024 08:58:28 - WARNING - evaluate.loading - Using the latest cached version of the module from /home/cs/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Nov 18 09:05:53 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
[WARNING|trainer.py:664] 2024-11-26 08:58:30,099 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-26 08:58:30,229 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-26 08:58:30,229 >>   Num examples = 2,318
[INFO|trainer.py:2337] 2024-11-26 08:58:30,229 >>   Num Epochs = 2
[INFO|trainer.py:2338] 2024-11-26 08:58:30,229 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-26 08:58:30,229 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-26 08:58:30,229 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-26 08:58:30,229 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-26 08:58:30,229 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-26 08:58:30,229 >>   Number of trainable parameters = 589,824
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:06<22:31,  6.79s/it]  1%|          | 2/200 [00:07<10:21,  3.14s/it]  2%|▏         | 3/200 [00:07<06:30,  1.98s/it]  2%|▏         | 4/200 [00:08<04:41,  1.43s/it]  2%|▎         | 5/200 [00:09<03:40,  1.13s/it]  3%|▎         | 6/200 [00:09<03:04,  1.05it/s]  4%|▎         | 7/200 [00:10<02:42,  1.19it/s]  4%|▍         | 8/200 [00:10<02:25,  1.32it/s]  4%|▍         | 9/200 [00:11<02:14,  1.42it/s]  5%|▌         | 10/200 [00:12<02:06,  1.50it/s]  6%|▌         | 11/200 [00:12<02:01,  1.56it/s]  6%|▌         | 12/200 [00:13<01:57,  1.60it/s]  6%|▋         | 13/200 [00:13<01:54,  1.63it/s]  7%|▋         | 14/200 [00:14<01:52,  1.65it/s]  8%|▊         | 15/200 [00:15<01:50,  1.67it/s]  8%|▊         | 16/200 [00:15<01:49,  1.68it/s]  8%|▊         | 17/200 [00:16<01:53,  1.62it/s]  9%|▉         | 18/200 [00:16<01:50,  1.65it/s] 10%|▉         | 19/200 [00:17<01:49,  1.66it/s] 10%|█         | 20/200 [00:18<01:47,  1.67it/s] 10%|█         | 21/200 [00:18<01:45,  1.69it/s] 11%|█         | 22/200 [00:19<01:44,  1.70it/s] 12%|█▏        | 23/200 [00:19<01:43,  1.71it/s] 12%|█▏        | 24/200 [00:20<01:43,  1.71it/s] 12%|█▎        | 25/200 [00:20<01:42,  1.71it/s] 13%|█▎        | 26/200 [00:21<01:41,  1.71it/s] 14%|█▎        | 27/200 [00:22<01:41,  1.71it/s] 14%|█▍        | 28/200 [00:22<01:40,  1.71it/s] 14%|█▍        | 29/200 [00:23<01:39,  1.71it/s] 15%|█▌        | 30/200 [00:23<01:39,  1.71it/s] 16%|█▌        | 31/200 [00:24<01:38,  1.72it/s] 16%|█▌        | 32/200 [00:25<01:38,  1.71it/s] 16%|█▋        | 33/200 [00:25<01:37,  1.71it/s] 17%|█▋        | 34/200 [00:26<01:37,  1.71it/s] 18%|█▊        | 35/200 [00:26<01:36,  1.70it/s] 18%|█▊        | 36/200 [00:27<01:36,  1.71it/s] 18%|█▊        | 37/200 [00:28<01:35,  1.71it/s] 19%|█▉        | 38/200 [00:28<01:40,  1.61it/s] 20%|█▉        | 39/200 [00:29<01:38,  1.64it/s] 20%|██        | 40/200 [00:29<01:36,  1.66it/s] 20%|██        | 41/200 [00:30<01:34,  1.68it/s] 21%|██        | 42/200 [00:31<01:33,  1.68it/s] 22%|██▏       | 43/200 [00:31<01:32,  1.69it/s] 22%|██▏       | 44/200 [00:32<01:32,  1.69it/s] 22%|██▎       | 45/200 [00:32<01:30,  1.70it/s] 23%|██▎       | 46/200 [00:33<01:30,  1.71it/s] 24%|██▎       | 47/200 [00:33<01:29,  1.71it/s] 24%|██▍       | 48/200 [00:34<01:29,  1.71it/s] 24%|██▍       | 49/200 [00:35<01:28,  1.71it/s] 25%|██▌       | 50/200 [00:35<01:27,  1.71it/s] 26%|██▌       | 51/200 [00:36<01:26,  1.72it/s] 26%|██▌       | 52/200 [00:36<01:26,  1.72it/s] 26%|██▋       | 53/200 [00:37<01:25,  1.72it/s] 27%|██▋       | 54/200 [00:38<01:24,  1.73it/s] 28%|██▊       | 55/200 [00:38<01:28,  1.64it/s] 28%|██▊       | 56/200 [00:39<01:26,  1.67it/s] 28%|██▊       | 57/200 [00:39<01:25,  1.68it/s] 29%|██▉       | 58/200 [00:40<01:24,  1.69it/s] 30%|██▉       | 59/200 [00:41<01:23,  1.70it/s] 30%|███       | 60/200 [00:41<01:22,  1.70it/s] 30%|███       | 61/200 [00:42<01:22,  1.69it/s] 31%|███       | 62/200 [00:42<01:21,  1.70it/s] 32%|███▏      | 63/200 [00:43<01:20,  1.70it/s] 32%|███▏      | 64/200 [00:43<01:20,  1.70it/s] 32%|███▎      | 65/200 [00:44<01:19,  1.70it/s] 33%|███▎      | 66/200 [00:45<01:18,  1.70it/s] 34%|███▎      | 67/200 [00:45<01:17,  1.71it/s] 34%|███▍      | 68/200 [00:46<01:17,  1.71it/s] 34%|███▍      | 69/200 [00:46<01:16,  1.70it/s] 35%|███▌      | 70/200 [00:47<01:16,  1.70it/s] 36%|███▌      | 71/200 [00:48<01:15,  1.71it/s] 36%|███▌      | 72/200 [00:48<01:15,  1.70it/s] 36%|███▋      | 73/200 [00:49<01:14,  1.71it/s] 37%|███▋      | 74/200 [00:49<01:13,  1.71it/s] 38%|███▊      | 75/200 [00:50<01:13,  1.70it/s] 38%|███▊      | 76/200 [00:51<01:16,  1.62it/s] 38%|███▊      | 77/200 [00:51<01:14,  1.65it/s] 39%|███▉      | 78/200 [00:52<01:13,  1.67it/s] 40%|███▉      | 79/200 [00:52<01:12,  1.68it/s] 40%|████      | 80/200 [00:53<01:12,  1.66it/s] 40%|████      | 81/200 [00:54<01:10,  1.68it/s] 41%|████      | 82/200 [00:54<01:09,  1.69it/s] 42%|████▏     | 83/200 [00:55<01:09,  1.69it/s] 42%|████▏     | 84/200 [00:55<01:08,  1.70it/s] 42%|████▎     | 85/200 [00:56<01:07,  1.71it/s] 43%|████▎     | 86/200 [00:56<01:06,  1.71it/s] 44%|████▎     | 87/200 [00:57<01:05,  1.72it/s] 44%|████▍     | 88/200 [00:58<01:05,  1.72it/s] 44%|████▍     | 89/200 [00:58<01:04,  1.72it/s] 45%|████▌     | 90/200 [00:59<01:03,  1.73it/s] 46%|████▌     | 91/200 [00:59<01:03,  1.72it/s] 46%|████▌     | 92/200 [01:00<01:02,  1.71it/s] 46%|████▋     | 93/200 [01:01<01:06,  1.60it/s] 47%|████▋     | 94/200 [01:01<01:04,  1.63it/s] 48%|████▊     | 95/200 [01:02<01:03,  1.66it/s] 48%|████▊     | 96/200 [01:02<01:02,  1.67it/s] 48%|████▊     | 97/200 [01:03<01:01,  1.69it/s] 49%|████▉     | 98/200 [01:04<01:00,  1.70it/s] 50%|████▉     | 99/200 [01:04<00:59,  1.70it/s] 50%|█████     | 100/200 [01:05<00:58,  1.70it/s] 50%|█████     | 101/200 [01:05<00:58,  1.70it/s] 51%|█████     | 102/200 [01:06<00:57,  1.71it/s] 52%|█████▏    | 103/200 [01:07<00:56,  1.71it/s] 52%|█████▏    | 104/200 [01:07<00:56,  1.71it/s] 52%|█████▎    | 105/200 [01:08<00:55,  1.71it/s] 53%|█████▎    | 106/200 [01:08<00:54,  1.72it/s] 54%|█████▎    | 107/200 [01:09<00:54,  1.71it/s] 54%|█████▍    | 108/200 [01:09<00:53,  1.71it/s] 55%|█████▍    | 109/200 [01:10<00:53,  1.71it/s] 55%|█████▌    | 110/200 [01:11<00:52,  1.72it/s] 56%|█████▌    | 111/200 [01:11<00:51,  1.71it/s] 56%|█████▌    | 112/200 [01:12<00:51,  1.71it/s] 56%|█████▋    | 113/200 [01:12<00:50,  1.71it/s] 57%|█████▋    | 114/200 [01:13<00:52,  1.63it/s] 57%|█████▊    | 115/200 [01:14<00:51,  1.65it/s] 58%|█████▊    | 116/200 [01:14<00:50,  1.67it/s] 58%|█████▊    | 117/200 [01:15<00:49,  1.69it/s] 59%|█████▉    | 118/200 [01:15<00:48,  1.70it/s] 60%|█████▉    | 119/200 [01:16<00:47,  1.70it/s] 60%|██████    | 120/200 [01:17<00:47,  1.70it/s] 60%|██████    | 121/200 [01:17<00:46,  1.70it/s] 61%|██████    | 122/200 [01:18<00:45,  1.70it/s] 62%|██████▏   | 123/200 [01:18<00:45,  1.71it/s] 62%|██████▏   | 124/200 [01:19<00:44,  1.71it/s] 62%|██████▎   | 125/200 [01:19<00:44,  1.70it/s] 63%|██████▎   | 126/200 [01:20<00:43,  1.70it/s] 64%|██████▎   | 127/200 [01:21<00:42,  1.71it/s] 64%|██████▍   | 128/200 [01:21<00:42,  1.70it/s] 64%|██████▍   | 129/200 [01:22<00:41,  1.71it/s] 65%|██████▌   | 130/200 [01:22<00:40,  1.71it/s] 66%|██████▌   | 131/200 [01:23<00:42,  1.63it/s] 66%|██████▌   | 132/200 [01:24<00:41,  1.66it/s] 66%|██████▋   | 133/200 [01:24<00:40,  1.67it/s] 67%|██████▋   | 134/200 [01:25<00:39,  1.68it/s] 68%|██████▊   | 135/200 [01:25<00:38,  1.69it/s] 68%|██████▊   | 136/200 [01:26<00:37,  1.69it/s] 68%|██████▊   | 137/200 [01:27<00:37,  1.70it/s] 69%|██████▉   | 138/200 [01:27<00:36,  1.71it/s] 70%|██████▉   | 139/200 [01:28<00:35,  1.71it/s] 70%|███████   | 140/200 [01:28<00:35,  1.71it/s] 70%|███████   | 141/200 [01:29<00:34,  1.70it/s] 71%|███████   | 142/200 [01:30<00:34,  1.70it/s] 72%|███████▏  | 143/200 [01:30<00:33,  1.70it/s] 72%|███████▏  | 144/200 [01:31<00:32,  1.71it/s] 72%|███████▎  | 145/200 [01:31<00:31,  1.76it/s] 73%|███████▎  | 146/200 [01:32<00:31,  1.73it/s] 74%|███████▎  | 147/200 [01:32<00:30,  1.72it/s] 74%|███████▍  | 148/200 [01:33<00:30,  1.71it/s] 74%|███████▍  | 149/200 [01:34<00:29,  1.71it/s] 75%|███████▌  | 150/200 [01:34<00:29,  1.70it/s] 76%|███████▌  | 151/200 [01:35<00:28,  1.70it/s] 76%|███████▌  | 152/200 [01:36<00:30,  1.60it/s] 76%|███████▋  | 153/200 [01:36<00:28,  1.63it/s] 77%|███████▋  | 154/200 [01:37<00:27,  1.66it/s] 78%|███████▊  | 155/200 [01:37<00:26,  1.67it/s] 78%|███████▊  | 156/200 [01:38<00:26,  1.68it/s] 78%|███████▊  | 157/200 [01:38<00:25,  1.69it/s] 79%|███████▉  | 158/200 [01:39<00:24,  1.69it/s] 80%|███████▉  | 159/200 [01:40<00:24,  1.69it/s] 80%|████████  | 160/200 [01:40<00:23,  1.69it/s] 80%|████████  | 161/200 [01:41<00:22,  1.70it/s] 81%|████████  | 162/200 [01:41<00:22,  1.70it/s] 82%|████████▏ | 163/200 [01:42<00:21,  1.70it/s] 82%|████████▏ | 164/200 [01:43<00:21,  1.70it/s] 82%|████████▎ | 165/200 [01:43<00:20,  1.70it/s] 83%|████████▎ | 166/200 [01:44<00:20,  1.70it/s] 84%|████████▎ | 167/200 [01:44<00:19,  1.70it/s] 84%|████████▍ | 168/200 [01:45<00:18,  1.70it/s] 84%|████████▍ | 169/200 [01:46<00:19,  1.62it/s] 85%|████████▌ | 170/200 [01:46<00:18,  1.65it/s] 86%|████████▌ | 171/200 [01:47<00:17,  1.66it/s] 86%|████████▌ | 172/200 [01:47<00:16,  1.66it/s] 86%|████████▋ | 173/200 [01:48<00:16,  1.67it/s] 87%|████████▋ | 174/200 [01:49<00:15,  1.69it/s] 88%|████████▊ | 175/200 [01:49<00:14,  1.70it/s] 88%|████████▊ | 176/200 [01:50<00:14,  1.70it/s] 88%|████████▊ | 177/200 [01:50<00:13,  1.70it/s] 89%|████████▉ | 178/200 [01:51<00:12,  1.70it/s] 90%|████████▉ | 179/200 [01:51<00:12,  1.70it/s] 90%|█████████ | 180/200 [01:52<00:11,  1.70it/s] 90%|█████████ | 181/200 [01:53<00:11,  1.69it/s] 91%|█████████ | 182/200 [01:53<00:10,  1.70it/s] 92%|█████████▏| 183/200 [01:54<00:09,  1.71it/s] 92%|█████████▏| 184/200 [01:54<00:09,  1.71it/s] 92%|█████████▎| 185/200 [01:55<00:08,  1.71it/s] 93%|█████████▎| 186/200 [01:56<00:08,  1.71it/s] 94%|█████████▎| 187/200 [01:56<00:07,  1.71it/s] 94%|█████████▍| 188/200 [01:57<00:06,  1.71it/s] 94%|█████████▍| 189/200 [01:57<00:06,  1.72it/s] 95%|█████████▌| 190/200 [01:58<00:06,  1.61it/s] 96%|█████████▌| 191/200 [01:59<00:05,  1.64it/s] 96%|█████████▌| 192/200 [01:59<00:04,  1.66it/s] 96%|█████████▋| 193/200 [02:00<00:04,  1.68it/s] 97%|█████████▋| 194/200 [02:00<00:03,  1.69it/s] 98%|█████████▊| 195/200 [02:01<00:02,  1.69it/s] 98%|█████████▊| 196/200 [02:02<00:02,  1.70it/s] 98%|█████████▊| 197/200 [02:02<00:01,  1.70it/s] 99%|█████████▉| 198/200 [02:03<00:01,  1.70it/s]100%|█████████▉| 199/200 [02:03<00:00,  1.70it/s]100%|██████████| 200/200 [02:04<00:00,  1.71it/s][INFO|trainer.py:3846] 2024-11-26 09:00:34,598 >> Saving model checkpoint to /tmp/test-clm2-gpt2-lora-wikitext-2-raw-v1/checkpoint-200
[INFO|configuration_utils.py:690] 2024-11-26 09:00:34,608 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:00:34,609 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:00:34,615 >> tokenizer config file saved in /tmp/test-clm2-gpt2-lora-wikitext-2-raw-v1/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:00:34,615 >> Special tokens file saved in /tmp/test-clm2-gpt2-lora-wikitext-2-raw-v1/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-26 09:00:34,668 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [02:04<00:00,  1.71it/s]100%|██████████| 200/200 [02:04<00:00,  1.61it/s]
[INFO|trainer.py:3846] 2024-11-26 09:00:34,669 >> Saving model checkpoint to /tmp/test-clm2-gpt2-lora-wikitext-2-raw-v1
[INFO|configuration_utils.py:690] 2024-11-26 09:00:34,677 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:00:34,678 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:00:34,682 >> tokenizer config file saved in /tmp/test-clm2-gpt2-lora-wikitext-2-raw-v1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:00:34,682 >> Special tokens file saved in /tmp/test-clm2-gpt2-lora-wikitext-2-raw-v1/special_tokens_map.json
{'train_runtime': 124.4386, 'train_samples_per_second': 25.715, 'train_steps_per_second': 1.607, 'train_loss': 3.125559997558594, 'epoch': 1.38}
***** train metrics *****
  epoch                    =     1.3793
  total_flos               =  1567241GF
  train_loss               =     3.1256
  train_runtime            = 0:02:04.43
  train_samples            =       2318
  train_samples_per_second =     25.715
  train_steps_per_second   =      1.607
11/26/2024 09:00:34 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-26 09:00:34,726 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-26 09:00:34,726 >>   Num examples = 240
[INFO|trainer.py:4167] 2024-11-26 09:00:34,726 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.85it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.02it/s] 50%|█████     | 4/8 [00:02<00:02,  1.75it/s] 62%|██████▎   | 5/8 [00:02<00:01,  1.62it/s] 75%|███████▌  | 6/8 [00:03<00:01,  1.56it/s] 88%|████████▊ | 7/8 [00:04<00:00,  1.52it/s]100%|██████████| 8/8 [00:04<00:00,  1.74it/s]100%|██████████| 8/8 [00:05<00:00,  1.59it/s]
***** eval metrics *****
  epoch                   =     1.3793
  eval_accuracy           =      0.422
  eval_loss               =     3.0754
  eval_runtime            = 0:00:05.78
  eval_samples            =        240
  eval_samples_per_second =     41.462
  eval_steps_per_second   =      1.382
  perplexity              =    21.6584
------------------------------------
LagEmbed: Training openai-community/gpt2 on wikitext (wikitext-2-raw-v1) with LagEmbed (in_channels=768, n_components=2, dof=4)
11/26/2024 09:00:45 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/26/2024 09:00:45 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1/runs/Nov26_09-00-45_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
11/26/2024 09:00:45 - WARNING - datasets.load - Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 09:26:37 2024).
11/26/2024 09:00:45 - WARNING - datasets.packaged_modules.cache.cache - Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 09:26:37 2024).
Using custom data configuration wikitext-2-raw-v1
11/26/2024 09:00:45 - INFO - datasets.builder - Using custom data configuration wikitext-2-raw-v1
Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
11/26/2024 09:00:45 - INFO - datasets.info - Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
Overwrite dataset info from restored data version if exists.
11/26/2024 09:00:45 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/26/2024 09:00:45 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-26 09:00:45,587 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:00:45,589 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:00:45,607 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:00:45,607 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:00:45,607 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:00:45,607 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:00:45,607 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:00:45,607 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-26 09:00:45,705 >> loading weights file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-26 09:00:45,708 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-26 09:00:45,736 >> All model checkpoint weights were used when initializing LagGPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-26 09:00:45,736 >> All the weights of LagGPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LagGPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-26 09:00:45,737 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-26 09:00:45,737 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-615616afca6f919c.arrow
11/26/2024 09:00:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-615616afca6f919c.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-69dfca79ecfc54e4.arrow
11/26/2024 09:00:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-69dfca79ecfc54e4.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-8741e82e5c31cd2f.arrow
11/26/2024 09:00:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-8741e82e5c31cd2f.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-3652bd785f05d459.arrow
11/26/2024 09:00:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-3652bd785f05d459.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-de9407b308ea1602.arrow
11/26/2024 09:00:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-de9407b308ea1602.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e42630efa0f0b097.arrow
11/26/2024 09:00:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e42630efa0f0b097.arrow
11/26/2024 09:00:47 - WARNING - evaluate.loading - Using the latest cached version of the module from /home/cs/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Nov 18 09:05:53 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
[WARNING|trainer.py:664] 2024-11-26 09:00:48,357 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-26 09:00:48,475 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-26 09:00:48,475 >>   Num examples = 2,318
[INFO|trainer.py:2337] 2024-11-26 09:00:48,475 >>   Num Epochs = 3
[INFO|trainer.py:2338] 2024-11-26 09:00:48,475 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:2340] 2024-11-26 09:00:48,475 >>   Training with DataParallel so batch size has been adjusted to: 32
[INFO|trainer.py:2341] 2024-11-26 09:00:48,475 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2342] 2024-11-26 09:00:48,475 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-26 09:00:48,475 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-26 09:00:48,476 >>   Number of trainable parameters = 403,594
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:05<17:23,  5.24s/it]  1%|          | 2/200 [00:05<08:20,  2.53s/it]  2%|▏         | 3/200 [00:06<05:26,  1.66s/it]  2%|▏         | 4/200 [00:07<04:05,  1.25s/it]  2%|▎         | 5/200 [00:07<03:19,  1.03s/it]  3%|▎         | 6/200 [00:08<02:52,  1.12it/s]  4%|▎         | 7/200 [00:08<02:34,  1.25it/s]  4%|▍         | 8/200 [00:09<02:23,  1.34it/s]  4%|▍         | 9/200 [00:10<02:15,  1.41it/s]  5%|▌         | 10/200 [00:10<02:10,  1.46it/s]  6%|▌         | 11/200 [00:11<02:05,  1.50it/s]  6%|▌         | 12/200 [00:12<02:02,  1.53it/s]  6%|▋         | 13/200 [00:12<02:07,  1.46it/s]  7%|▋         | 14/200 [00:13<02:11,  1.41it/s]  8%|▊         | 15/200 [00:14<02:15,  1.37it/s]  8%|▊         | 16/200 [00:15<02:16,  1.35it/s]  8%|▊         | 17/200 [00:15<02:17,  1.33it/s]  9%|▉         | 18/200 [00:16<02:17,  1.32it/s] 10%|▉         | 19/200 [00:17<02:18,  1.31it/s] 10%|█         | 20/200 [00:18<02:18,  1.30it/s] 10%|█         | 21/200 [00:19<02:17,  1.30it/s] 11%|█         | 22/200 [00:19<02:17,  1.29it/s] 12%|█▏        | 23/200 [00:20<02:16,  1.30it/s] 12%|█▏        | 24/200 [00:21<02:15,  1.30it/s] 12%|█▎        | 25/200 [00:22<02:15,  1.29it/s] 13%|█▎        | 26/200 [00:22<02:15,  1.29it/s] 14%|█▎        | 27/200 [00:23<02:14,  1.28it/s] 14%|█▍        | 28/200 [00:24<02:14,  1.28it/s] 14%|█▍        | 29/200 [00:25<02:13,  1.28it/s] 15%|█▌        | 30/200 [00:26<02:12,  1.28it/s] 16%|█▌        | 31/200 [00:26<02:11,  1.28it/s] 16%|█▌        | 32/200 [00:27<02:11,  1.28it/s] 16%|█▋        | 33/200 [00:28<02:10,  1.28it/s] 17%|█▋        | 34/200 [00:29<02:09,  1.28it/s] 18%|█▊        | 35/200 [00:29<02:07,  1.29it/s] 18%|█▊        | 36/200 [00:30<02:09,  1.26it/s] 18%|█▊        | 37/200 [00:31<02:07,  1.27it/s] 19%|█▉        | 38/200 [00:32<02:06,  1.28it/s] 20%|█▉        | 39/200 [00:33<02:04,  1.29it/s] 20%|██        | 40/200 [00:33<02:04,  1.29it/s] 20%|██        | 41/200 [00:34<02:03,  1.29it/s] 21%|██        | 42/200 [00:35<02:02,  1.29it/s] 22%|██▏       | 43/200 [00:36<02:00,  1.30it/s] 22%|██▏       | 44/200 [00:36<01:59,  1.30it/s] 22%|██▎       | 45/200 [00:37<01:58,  1.31it/s] 23%|██▎       | 46/200 [00:38<01:58,  1.30it/s] 24%|██▎       | 47/200 [00:39<01:57,  1.30it/s] 24%|██▍       | 48/200 [00:40<01:57,  1.29it/s] 24%|██▍       | 49/200 [00:40<01:55,  1.30it/s] 25%|██▌       | 50/200 [00:41<01:55,  1.30it/s] 26%|██▌       | 51/200 [00:42<01:54,  1.30it/s] 26%|██▌       | 52/200 [00:43<01:53,  1.30it/s] 26%|██▋       | 53/200 [00:43<01:53,  1.30it/s] 27%|██▋       | 54/200 [00:44<01:53,  1.29it/s] 28%|██▊       | 55/200 [00:45<01:53,  1.28it/s] 28%|██▊       | 56/200 [00:46<01:51,  1.29it/s] 28%|██▊       | 57/200 [00:47<01:51,  1.28it/s] 29%|██▉       | 58/200 [00:47<01:51,  1.28it/s] 30%|██▉       | 59/200 [00:48<01:49,  1.28it/s] 30%|███       | 60/200 [00:49<01:49,  1.28it/s] 30%|███       | 61/200 [00:50<01:48,  1.28it/s] 31%|███       | 62/200 [00:50<01:46,  1.29it/s] 32%|███▏      | 63/200 [00:51<01:45,  1.30it/s] 32%|███▏      | 64/200 [00:52<01:45,  1.29it/s] 32%|███▎      | 65/200 [00:53<01:44,  1.29it/s] 33%|███▎      | 66/200 [00:53<01:43,  1.29it/s] 34%|███▎      | 67/200 [00:54<01:43,  1.29it/s] 34%|███▍      | 68/200 [00:55<01:41,  1.30it/s] 34%|███▍      | 69/200 [00:56<01:40,  1.30it/s] 35%|███▌      | 70/200 [00:57<01:40,  1.30it/s] 36%|███▌      | 71/200 [00:57<01:40,  1.29it/s] 36%|███▌      | 72/200 [00:58<01:38,  1.29it/s] 36%|███▋      | 73/200 [00:59<01:24,  1.49it/s] 37%|███▋      | 74/200 [00:59<01:28,  1.42it/s] 38%|███▊      | 75/200 [01:00<01:30,  1.38it/s] 38%|███▊      | 76/200 [01:01<01:31,  1.35it/s] 38%|███▊      | 77/200 [01:02<01:32,  1.33it/s] 39%|███▉      | 78/200 [01:02<01:31,  1.33it/s] 40%|███▉      | 79/200 [01:03<01:31,  1.32it/s] 40%|████      | 80/200 [01:04<01:31,  1.31it/s] 40%|████      | 81/200 [01:05<01:31,  1.31it/s] 41%|████      | 82/200 [01:06<01:30,  1.30it/s] 42%|████▏     | 83/200 [01:06<01:29,  1.30it/s] 42%|████▏     | 84/200 [01:07<01:29,  1.30it/s] 42%|████▎     | 85/200 [01:08<01:28,  1.29it/s] 43%|████▎     | 86/200 [01:09<01:28,  1.29it/s] 44%|████▎     | 87/200 [01:09<01:27,  1.29it/s] 44%|████▍     | 88/200 [01:10<01:27,  1.29it/s] 44%|████▍     | 89/200 [01:11<01:25,  1.29it/s] 45%|████▌     | 90/200 [01:12<01:24,  1.30it/s] 46%|████▌     | 91/200 [01:12<01:24,  1.29it/s] 46%|████▌     | 92/200 [01:13<01:23,  1.30it/s] 46%|████▋     | 93/200 [01:14<01:22,  1.30it/s] 47%|████▋     | 94/200 [01:15<01:21,  1.30it/s] 48%|████▊     | 95/200 [01:16<01:20,  1.30it/s] 48%|████▊     | 96/200 [01:16<01:19,  1.32it/s] 48%|████▊     | 97/200 [01:17<01:18,  1.31it/s] 49%|████▉     | 98/200 [01:18<01:18,  1.31it/s] 50%|████▉     | 99/200 [01:19<01:17,  1.31it/s] 50%|█████     | 100/200 [01:19<01:17,  1.30it/s] 50%|█████     | 101/200 [01:20<01:16,  1.30it/s] 51%|█████     | 102/200 [01:21<01:15,  1.30it/s] 52%|█████▏    | 103/200 [01:22<01:14,  1.30it/s] 52%|█████▏    | 104/200 [01:22<01:13,  1.31it/s] 52%|█████▎    | 105/200 [01:23<01:12,  1.30it/s] 53%|█████▎    | 106/200 [01:24<01:12,  1.30it/s] 54%|█████▎    | 107/200 [01:25<01:14,  1.24it/s] 54%|█████▍    | 108/200 [01:26<01:13,  1.25it/s] 55%|█████▍    | 109/200 [01:26<01:11,  1.27it/s] 55%|█████▌    | 110/200 [01:27<01:10,  1.28it/s] 56%|█████▌    | 111/200 [01:28<01:09,  1.29it/s] 56%|█████▌    | 112/200 [01:29<01:08,  1.29it/s] 56%|█████▋    | 113/200 [01:29<01:07,  1.29it/s] 57%|█████▋    | 114/200 [01:30<01:06,  1.29it/s] 57%|█████▊    | 115/200 [01:31<01:05,  1.30it/s] 58%|█████▊    | 116/200 [01:32<01:04,  1.30it/s] 58%|█████▊    | 117/200 [01:33<01:04,  1.29it/s] 59%|█████▉    | 118/200 [01:33<01:03,  1.29it/s] 60%|█████▉    | 119/200 [01:34<01:02,  1.29it/s] 60%|██████    | 120/200 [01:35<01:01,  1.31it/s] 60%|██████    | 121/200 [01:36<01:00,  1.31it/s] 61%|██████    | 122/200 [01:36<00:59,  1.31it/s] 62%|██████▏   | 123/200 [01:37<00:59,  1.30it/s] 62%|██████▏   | 124/200 [01:38<00:58,  1.30it/s] 62%|██████▎   | 125/200 [01:39<00:57,  1.30it/s] 63%|██████▎   | 126/200 [01:39<00:56,  1.30it/s] 64%|██████▎   | 127/200 [01:40<00:55,  1.30it/s] 64%|██████▍   | 128/200 [01:41<00:55,  1.30it/s] 64%|██████▍   | 129/200 [01:42<00:54,  1.30it/s] 65%|██████▌   | 130/200 [01:43<00:53,  1.30it/s] 66%|██████▌   | 131/200 [01:43<00:53,  1.30it/s] 66%|██████▌   | 132/200 [01:44<00:52,  1.30it/s] 66%|██████▋   | 133/200 [01:45<00:51,  1.30it/s] 67%|██████▋   | 134/200 [01:46<00:51,  1.29it/s] 68%|██████▊   | 135/200 [01:46<00:50,  1.30it/s] 68%|██████▊   | 136/200 [01:47<00:49,  1.30it/s] 68%|██████▊   | 137/200 [01:48<00:48,  1.30it/s] 69%|██████▉   | 138/200 [01:49<00:47,  1.30it/s] 70%|██████▉   | 139/200 [01:50<00:47,  1.30it/s] 70%|███████   | 140/200 [01:50<00:46,  1.29it/s] 70%|███████   | 141/200 [01:51<00:45,  1.29it/s] 71%|███████   | 142/200 [01:52<00:44,  1.29it/s] 72%|███████▏  | 143/200 [01:53<00:44,  1.29it/s] 72%|███████▏  | 144/200 [01:53<00:43,  1.29it/s] 72%|███████▎  | 145/200 [01:54<00:42,  1.29it/s] 73%|███████▎  | 146/200 [01:55<00:36,  1.49it/s] 74%|███████▎  | 147/200 [01:55<00:37,  1.41it/s] 74%|███████▍  | 148/200 [01:56<00:37,  1.37it/s] 74%|███████▍  | 149/200 [01:57<00:37,  1.35it/s] 75%|███████▌  | 150/200 [01:58<00:37,  1.33it/s] 76%|███████▌  | 151/200 [01:58<00:37,  1.32it/s] 76%|███████▌  | 152/200 [01:59<00:36,  1.31it/s] 76%|███████▋  | 153/200 [02:00<00:35,  1.31it/s] 77%|███████▋  | 154/200 [02:01<00:35,  1.30it/s] 78%|███████▊  | 155/200 [02:02<00:34,  1.29it/s] 78%|███████▊  | 156/200 [02:02<00:34,  1.29it/s] 78%|███████▊  | 157/200 [02:03<00:33,  1.29it/s] 79%|███████▉  | 158/200 [02:04<00:32,  1.29it/s] 80%|███████▉  | 159/200 [02:05<00:31,  1.30it/s] 80%|████████  | 160/200 [02:05<00:30,  1.29it/s] 80%|████████  | 161/200 [02:06<00:30,  1.29it/s] 81%|████████  | 162/200 [02:07<00:29,  1.30it/s] 82%|████████▏ | 163/200 [02:08<00:28,  1.30it/s] 82%|████████▏ | 164/200 [02:09<00:27,  1.29it/s] 82%|████████▎ | 165/200 [02:09<00:27,  1.29it/s] 83%|████████▎ | 166/200 [02:10<00:26,  1.29it/s] 84%|████████▎ | 167/200 [02:11<00:26,  1.24it/s] 84%|████████▍ | 168/200 [02:12<00:25,  1.25it/s] 84%|████████▍ | 169/200 [02:13<00:24,  1.27it/s] 85%|████████▌ | 170/200 [02:13<00:23,  1.28it/s] 86%|████████▌ | 171/200 [02:14<00:22,  1.28it/s] 86%|████████▌ | 172/200 [02:15<00:21,  1.29it/s] 86%|████████▋ | 173/200 [02:16<00:20,  1.30it/s] 87%|████████▋ | 174/200 [02:16<00:19,  1.30it/s] 88%|████████▊ | 175/200 [02:17<00:19,  1.29it/s] 88%|████████▊ | 176/200 [02:18<00:18,  1.29it/s] 88%|████████▊ | 177/200 [02:19<00:17,  1.28it/s] 89%|████████▉ | 178/200 [02:19<00:17,  1.29it/s] 90%|████████▉ | 179/200 [02:20<00:16,  1.29it/s] 90%|█████████ | 180/200 [02:21<00:15,  1.29it/s] 90%|█████████ | 181/200 [02:22<00:14,  1.29it/s] 91%|█████████ | 182/200 [02:23<00:13,  1.29it/s] 92%|█████████▏| 183/200 [02:23<00:13,  1.29it/s] 92%|█████████▏| 184/200 [02:24<00:12,  1.29it/s] 92%|█████████▎| 185/200 [02:25<00:11,  1.28it/s] 93%|█████████▎| 186/200 [02:26<00:10,  1.28it/s] 94%|█████████▎| 187/200 [02:26<00:10,  1.29it/s] 94%|█████████▍| 188/200 [02:27<00:09,  1.30it/s] 94%|█████████▍| 189/200 [02:28<00:08,  1.29it/s] 95%|█████████▌| 190/200 [02:29<00:07,  1.29it/s] 96%|█████████▌| 191/200 [02:30<00:06,  1.29it/s] 96%|█████████▌| 192/200 [02:30<00:06,  1.28it/s] 96%|█████████▋| 193/200 [02:31<00:05,  1.28it/s] 97%|█████████▋| 194/200 [02:32<00:04,  1.29it/s] 98%|█████████▊| 195/200 [02:33<00:03,  1.29it/s] 98%|█████████▊| 196/200 [02:33<00:03,  1.29it/s] 98%|█████████▊| 197/200 [02:34<00:02,  1.29it/s] 99%|█████████▉| 198/200 [02:35<00:01,  1.29it/s]100%|█████████▉| 199/200 [02:36<00:00,  1.30it/s]100%|██████████| 200/200 [02:37<00:00,  1.30it/s][INFO|trainer.py:3846] 2024-11-26 09:03:25,506 >> Saving model checkpoint to /tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1/checkpoint-200
[INFO|configuration_utils.py:416] 2024-11-26 09:03:25,507 >> Configuration saved in /tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1/checkpoint-200/config.json
[INFO|configuration_utils.py:873] 2024-11-26 09:03:25,508 >> Configuration saved in /tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-26 09:03:26,566 >> Model weights saved in /tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1/checkpoint-200/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:03:26,567 >> tokenizer config file saved in /tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:03:26,568 >> Special tokens file saved in /tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-26 09:03:26,621 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [02:38<00:00,  1.30it/s]100%|██████████| 200/200 [02:38<00:00,  1.26it/s]
[INFO|trainer.py:3846] 2024-11-26 09:03:26,622 >> Saving model checkpoint to /tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1
[INFO|configuration_utils.py:416] 2024-11-26 09:03:26,624 >> Configuration saved in /tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:873] 2024-11-26 09:03:26,624 >> Configuration saved in /tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-26 09:03:27,642 >> Model weights saved in /tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:03:27,643 >> tokenizer config file saved in /tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:03:27,643 >> Special tokens file saved in /tmp/test-clm2-gpt2-lagembed-wikitext-2-raw-v1/special_tokens_map.json
{'train_runtime': 158.1457, 'train_samples_per_second': 40.469, 'train_steps_per_second': 1.265, 'train_loss': 2.996576232910156, 'epoch': 2.74}
***** train metrics *****
  epoch                    =     2.7397
  total_flos               =  3112019GF
  train_loss               =     2.9966
  train_runtime            = 0:02:38.14
  train_samples            =       2318
  train_samples_per_second =     40.469
  train_steps_per_second   =      1.265
11/26/2024 09:03:27 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-26 09:03:27,691 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-26 09:03:27,691 >>   Num examples = 240
[INFO|trainer.py:4167] 2024-11-26 09:03:27,691 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.93it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.08it/s] 50%|█████     | 4/8 [00:02<00:02,  1.79it/s] 62%|██████▎   | 5/8 [00:02<00:01,  1.66it/s] 75%|███████▌  | 6/8 [00:03<00:01,  1.58it/s] 88%|████████▊ | 7/8 [00:04<00:00,  1.54it/s]100%|██████████| 8/8 [00:04<00:00,  1.76it/s]100%|██████████| 8/8 [00:04<00:00,  1.62it/s]
***** eval metrics *****
  epoch                   =     2.7397
  eval_accuracy           =     0.4227
  eval_loss               =     3.0725
  eval_runtime            = 0:00:05.65
  eval_samples            =        240
  eval_samples_per_second =     42.446
  eval_steps_per_second   =      1.415
  perplexity              =    21.5969
------------------------------------
LoRA: Fine-tuning openai-community/gpt2 on wikitext (wikitext-103-raw-v1) using baseline model
11/26/2024 09:03:38 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/26/2024 09:03:38 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm2-gpt2-lora-wikitext-103-raw-v1/runs/Nov26_09-03-38_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm2-gpt2-lora-wikitext-103-raw-v1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm2-gpt2-lora-wikitext-103-raw-v1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
11/26/2024 09:03:38 - WARNING - datasets.load - Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-103-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 10:47:57 2024).
11/26/2024 09:03:38 - WARNING - datasets.packaged_modules.cache.cache - Found the latest cached dataset configuration 'wikitext-103-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 10:47:57 2024).
Using custom data configuration wikitext-103-raw-v1
11/26/2024 09:03:38 - INFO - datasets.builder - Using custom data configuration wikitext-103-raw-v1
Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
11/26/2024 09:03:38 - INFO - datasets.info - Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
Overwrite dataset info from restored data version if exists.
11/26/2024 09:03:38 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/26/2024 09:03:38 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-26 09:03:39,088 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:03:39,089 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-base-wikitext-103-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:03:39,095 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:03:39,095 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:03:39,095 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:03:39,095 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:03:39,095 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:03:39,095 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-26 09:03:39,176 >> loading weights file /tmp/test-clm-gpt2-base-wikitext-103-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-26 09:03:39,179 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-26 09:03:39,207 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-26 09:03:39,207 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-base-wikitext-103-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-26 09:03:39,208 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-103-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-26 09:03:39,208 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e6644fbc2010e737.arrow
11/26/2024 09:03:39 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e6644fbc2010e737.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-fb2be7315144d6ce.arrow
11/26/2024 09:03:39 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-fb2be7315144d6ce.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-00e01a7ca3ac3883.arrow
11/26/2024 09:03:39 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-00e01a7ca3ac3883.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5c5729770c73f88b.arrow
11/26/2024 09:03:39 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5c5729770c73f88b.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-923a2f1142745f49.arrow
11/26/2024 09:03:40 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-923a2f1142745f49.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-77276478acb46eeb.arrow
11/26/2024 09:03:40 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-77276478acb46eeb.arrow
11/26/2024 09:03:42 - WARNING - evaluate.loading - Using the latest cached version of the module from /home/cs/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Nov 18 09:05:53 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
[WARNING|trainer.py:664] 2024-11-26 09:03:43,135 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-26 09:03:43,256 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-26 09:03:43,256 >>   Num examples = 114,248
[INFO|trainer.py:2337] 2024-11-26 09:03:43,256 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-26 09:03:43,256 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-26 09:03:43,256 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-26 09:03:43,256 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-26 09:03:43,256 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-26 09:03:43,256 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-26 09:03:43,257 >>   Number of trainable parameters = 589,824
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:07<23:19,  7.03s/it]  1%|          | 2/200 [00:07<10:42,  3.25s/it]  2%|▏         | 3/200 [00:08<06:42,  2.04s/it]  2%|▏         | 4/200 [00:08<04:48,  1.47s/it]  2%|▎         | 5/200 [00:09<03:45,  1.16s/it]  3%|▎         | 6/200 [00:10<03:07,  1.03it/s]  4%|▎         | 7/200 [00:10<02:43,  1.18it/s]  4%|▍         | 8/200 [00:11<02:27,  1.30it/s]  4%|▍         | 9/200 [00:11<02:16,  1.40it/s]  5%|▌         | 10/200 [00:12<02:09,  1.47it/s]  6%|▌         | 11/200 [00:13<02:05,  1.51it/s]  6%|▌         | 12/200 [00:13<02:00,  1.56it/s]  6%|▋         | 13/200 [00:14<01:57,  1.59it/s]  7%|▋         | 14/200 [00:14<01:55,  1.61it/s]  8%|▊         | 15/200 [00:15<01:53,  1.62it/s]  8%|▊         | 16/200 [00:16<01:52,  1.64it/s]  8%|▊         | 17/200 [00:16<01:50,  1.65it/s]  9%|▉         | 18/200 [00:17<01:50,  1.65it/s] 10%|▉         | 19/200 [00:17<01:48,  1.66it/s] 10%|█         | 20/200 [00:18<01:48,  1.66it/s] 10%|█         | 21/200 [00:18<01:44,  1.71it/s] 11%|█         | 22/200 [00:19<01:45,  1.69it/s] 12%|█▏        | 23/200 [00:20<01:44,  1.69it/s] 12%|█▏        | 24/200 [00:20<01:44,  1.69it/s] 12%|█▎        | 25/200 [00:21<01:43,  1.68it/s] 13%|█▎        | 26/200 [00:21<01:43,  1.67it/s] 14%|█▎        | 27/200 [00:22<01:43,  1.67it/s] 14%|█▍        | 28/200 [00:23<01:42,  1.68it/s] 14%|█▍        | 29/200 [00:23<01:42,  1.67it/s] 15%|█▌        | 30/200 [00:24<01:41,  1.67it/s] 16%|█▌        | 31/200 [00:24<01:40,  1.68it/s] 16%|█▌        | 32/200 [00:25<01:40,  1.68it/s] 16%|█▋        | 33/200 [00:26<01:39,  1.68it/s] 17%|█▋        | 34/200 [00:26<01:39,  1.67it/s] 18%|█▊        | 35/200 [00:27<01:39,  1.66it/s] 18%|█▊        | 36/200 [00:27<01:38,  1.66it/s] 18%|█▊        | 37/200 [00:28<01:44,  1.56it/s] 19%|█▉        | 38/200 [00:29<01:41,  1.59it/s] 20%|█▉        | 39/200 [00:29<01:39,  1.62it/s] 20%|██        | 40/200 [00:30<01:37,  1.64it/s] 20%|██        | 41/200 [00:31<01:36,  1.65it/s] 21%|██        | 42/200 [00:31<01:34,  1.67it/s] 22%|██▏       | 43/200 [00:32<01:34,  1.67it/s] 22%|██▏       | 44/200 [00:32<01:33,  1.67it/s] 22%|██▎       | 45/200 [00:33<01:32,  1.67it/s] 23%|██▎       | 46/200 [00:34<01:32,  1.67it/s] 24%|██▎       | 47/200 [00:34<01:31,  1.67it/s] 24%|██▍       | 48/200 [00:35<01:30,  1.67it/s] 24%|██▍       | 49/200 [00:35<01:30,  1.67it/s] 25%|██▌       | 50/200 [00:36<01:29,  1.67it/s] 26%|██▌       | 51/200 [00:37<01:28,  1.68it/s] 26%|██▌       | 52/200 [00:37<01:28,  1.68it/s] 26%|██▋       | 53/200 [00:38<01:27,  1.68it/s] 27%|██▋       | 54/200 [00:38<01:27,  1.68it/s] 28%|██▊       | 55/200 [00:39<01:26,  1.67it/s] 28%|██▊       | 56/200 [00:40<01:26,  1.67it/s] 28%|██▊       | 57/200 [00:40<01:25,  1.67it/s] 29%|██▉       | 58/200 [00:41<01:24,  1.67it/s] 30%|██▉       | 59/200 [00:41<01:22,  1.71it/s] 30%|███       | 60/200 [00:42<01:22,  1.69it/s] 30%|███       | 61/200 [00:42<01:22,  1.68it/s] 31%|███       | 62/200 [00:43<01:22,  1.67it/s] 32%|███▏      | 63/200 [00:44<01:21,  1.68it/s] 32%|███▏      | 64/200 [00:44<01:21,  1.67it/s] 32%|███▎      | 65/200 [00:45<01:20,  1.67it/s] 33%|███▎      | 66/200 [00:45<01:20,  1.67it/s] 34%|███▎      | 67/200 [00:46<01:19,  1.68it/s] 34%|███▍      | 68/200 [00:47<01:18,  1.68it/s] 34%|███▍      | 69/200 [00:47<01:18,  1.68it/s] 35%|███▌      | 70/200 [00:48<01:17,  1.68it/s] 36%|███▌      | 71/200 [00:48<01:16,  1.68it/s] 36%|███▌      | 72/200 [00:49<01:16,  1.68it/s] 36%|███▋      | 73/200 [00:50<01:15,  1.68it/s] 37%|███▋      | 74/200 [00:50<01:15,  1.67it/s] 38%|███▊      | 75/200 [00:51<01:20,  1.55it/s] 38%|███▊      | 76/200 [00:52<01:18,  1.59it/s] 38%|███▊      | 77/200 [00:52<01:16,  1.62it/s] 39%|███▉      | 78/200 [00:53<01:14,  1.64it/s] 40%|███▉      | 79/200 [00:53<01:13,  1.64it/s] 40%|████      | 80/200 [00:54<01:12,  1.65it/s] 40%|████      | 81/200 [00:55<01:11,  1.66it/s] 41%|████      | 82/200 [00:55<01:11,  1.66it/s] 42%|████▏     | 83/200 [00:56<01:10,  1.66it/s] 42%|████▏     | 84/200 [00:56<01:09,  1.66it/s] 42%|████▎     | 85/200 [00:57<01:09,  1.66it/s] 43%|████▎     | 86/200 [00:58<01:08,  1.66it/s] 44%|████▎     | 87/200 [00:58<01:07,  1.67it/s] 44%|████▍     | 88/200 [00:59<01:07,  1.67it/s] 44%|████▍     | 89/200 [00:59<01:06,  1.66it/s] 45%|████▌     | 90/200 [01:00<01:05,  1.67it/s] 46%|████▌     | 91/200 [01:01<01:05,  1.68it/s] 46%|████▌     | 92/200 [01:01<01:04,  1.68it/s] 46%|████▋     | 93/200 [01:02<01:03,  1.68it/s] 47%|████▋     | 94/200 [01:02<01:02,  1.68it/s] 48%|████▊     | 95/200 [01:03<01:02,  1.68it/s] 48%|████▊     | 96/200 [01:04<01:01,  1.69it/s] 48%|████▊     | 97/200 [01:04<01:00,  1.72it/s] 49%|████▉     | 98/200 [01:05<00:59,  1.71it/s] 50%|████▉     | 99/200 [01:05<00:59,  1.70it/s] 50%|█████     | 100/200 [01:06<00:58,  1.70it/s] 50%|█████     | 101/200 [01:06<00:58,  1.69it/s] 51%|█████     | 102/200 [01:07<00:58,  1.68it/s] 52%|█████▏    | 103/200 [01:08<00:58,  1.67it/s] 52%|█████▏    | 104/200 [01:08<00:57,  1.67it/s] 52%|█████▎    | 105/200 [01:09<00:57,  1.67it/s] 53%|█████▎    | 106/200 [01:09<00:56,  1.67it/s] 54%|█████▎    | 107/200 [01:10<00:55,  1.66it/s] 54%|█████▍    | 108/200 [01:11<00:54,  1.68it/s] 55%|█████▍    | 109/200 [01:11<00:54,  1.68it/s] 55%|█████▌    | 110/200 [01:12<00:53,  1.67it/s] 56%|█████▌    | 111/200 [01:12<00:53,  1.68it/s] 56%|█████▌    | 112/200 [01:13<00:52,  1.68it/s] 56%|█████▋    | 113/200 [01:14<00:55,  1.58it/s] 57%|█████▋    | 114/200 [01:14<00:53,  1.61it/s] 57%|█████▊    | 115/200 [01:15<00:52,  1.63it/s] 58%|█████▊    | 116/200 [01:16<00:51,  1.64it/s] 58%|█████▊    | 117/200 [01:16<00:50,  1.65it/s] 59%|█████▉    | 118/200 [01:17<00:49,  1.65it/s] 60%|█████▉    | 119/200 [01:17<00:48,  1.66it/s] 60%|██████    | 120/200 [01:18<00:48,  1.66it/s] 60%|██████    | 121/200 [01:19<00:47,  1.67it/s] 61%|██████    | 122/200 [01:19<00:46,  1.67it/s] 62%|██████▏   | 123/200 [01:20<00:46,  1.67it/s] 62%|██████▏   | 124/200 [01:20<00:45,  1.68it/s] 62%|██████▎   | 125/200 [01:21<00:44,  1.67it/s] 63%|██████▎   | 126/200 [01:22<00:44,  1.68it/s] 64%|██████▎   | 127/200 [01:22<00:43,  1.68it/s] 64%|██████▍   | 128/200 [01:23<00:42,  1.68it/s] 64%|██████▍   | 129/200 [01:23<00:42,  1.68it/s] 65%|██████▌   | 130/200 [01:24<00:41,  1.68it/s] 66%|██████▌   | 131/200 [01:25<00:41,  1.68it/s] 66%|██████▌   | 132/200 [01:25<00:40,  1.68it/s] 66%|██████▋   | 133/200 [01:26<00:40,  1.67it/s] 67%|██████▋   | 134/200 [01:26<00:39,  1.68it/s] 68%|██████▊   | 135/200 [01:27<00:37,  1.73it/s] 68%|██████▊   | 136/200 [01:27<00:37,  1.71it/s] 68%|██████▊   | 137/200 [01:28<00:37,  1.70it/s] 69%|██████▉   | 138/200 [01:29<00:36,  1.69it/s] 70%|██████▉   | 139/200 [01:29<00:36,  1.69it/s] 70%|███████   | 140/200 [01:30<00:35,  1.69it/s] 70%|███████   | 141/200 [01:30<00:35,  1.68it/s] 71%|███████   | 142/200 [01:31<00:34,  1.67it/s] 72%|███████▏  | 143/200 [01:32<00:34,  1.66it/s] 72%|███████▏  | 144/200 [01:32<00:33,  1.66it/s] 72%|███████▎  | 145/200 [01:33<00:33,  1.65it/s] 73%|███████▎  | 146/200 [01:34<00:33,  1.63it/s] 74%|███████▎  | 147/200 [01:34<00:32,  1.62it/s] 74%|███████▍  | 148/200 [01:35<00:32,  1.61it/s] 74%|███████▍  | 149/200 [01:35<00:31,  1.61it/s] 75%|███████▌  | 150/200 [01:36<00:30,  1.62it/s] 76%|███████▌  | 151/200 [01:37<00:31,  1.54it/s] 76%|███████▌  | 152/200 [01:37<00:30,  1.57it/s] 76%|███████▋  | 153/200 [01:38<00:29,  1.59it/s] 77%|███████▋  | 154/200 [01:39<00:28,  1.60it/s] 78%|███████▊  | 155/200 [01:39<00:28,  1.60it/s] 78%|███████▊  | 156/200 [01:40<00:27,  1.61it/s] 78%|███████▊  | 157/200 [01:40<00:26,  1.62it/s] 79%|███████▉  | 158/200 [01:41<00:25,  1.63it/s] 80%|███████▉  | 159/200 [01:42<00:25,  1.63it/s] 80%|████████  | 160/200 [01:42<00:24,  1.62it/s] 80%|████████  | 161/200 [01:43<00:23,  1.64it/s] 81%|████████  | 162/200 [01:43<00:23,  1.64it/s] 82%|████████▏ | 163/200 [01:44<00:22,  1.64it/s] 82%|████████▏ | 164/200 [01:45<00:22,  1.64it/s] 82%|████████▎ | 165/200 [01:45<00:21,  1.64it/s] 83%|████████▎ | 166/200 [01:46<00:20,  1.65it/s] 84%|████████▎ | 167/200 [01:46<00:19,  1.65it/s] 84%|████████▍ | 168/200 [01:47<00:19,  1.66it/s] 84%|████████▍ | 169/200 [01:48<00:18,  1.67it/s] 85%|████████▌ | 170/200 [01:48<00:18,  1.65it/s] 86%|████████▌ | 171/200 [01:49<00:17,  1.66it/s] 86%|████████▌ | 172/200 [01:49<00:16,  1.65it/s] 86%|████████▋ | 173/200 [01:50<00:15,  1.70it/s] 87%|████████▋ | 174/200 [01:51<00:15,  1.69it/s] 88%|████████▊ | 175/200 [01:51<00:14,  1.69it/s] 88%|████████▊ | 176/200 [01:52<00:14,  1.67it/s] 88%|████████▊ | 177/200 [01:52<00:13,  1.66it/s] 89%|████████▉ | 178/200 [01:53<00:13,  1.65it/s] 90%|████████▉ | 179/200 [01:54<00:12,  1.64it/s] 90%|█████████ | 180/200 [01:54<00:12,  1.64it/s] 90%|█████████ | 181/200 [01:55<00:11,  1.65it/s] 91%|█████████ | 182/200 [01:56<00:11,  1.63it/s] 92%|█████████▏| 183/200 [01:56<00:10,  1.63it/s] 92%|█████████▏| 184/200 [01:57<00:09,  1.63it/s] 92%|█████████▎| 185/200 [01:57<00:09,  1.64it/s] 93%|█████████▎| 186/200 [01:58<00:08,  1.64it/s] 94%|█████████▎| 187/200 [01:59<00:07,  1.65it/s] 94%|█████████▍| 188/200 [01:59<00:07,  1.65it/s] 94%|█████████▍| 189/200 [02:00<00:07,  1.55it/s] 95%|█████████▌| 190/200 [02:01<00:06,  1.58it/s] 96%|█████████▌| 191/200 [02:01<00:05,  1.60it/s] 96%|█████████▌| 192/200 [02:02<00:04,  1.62it/s] 96%|█████████▋| 193/200 [02:02<00:04,  1.63it/s] 97%|█████████▋| 194/200 [02:03<00:03,  1.63it/s] 98%|█████████▊| 195/200 [02:04<00:03,  1.63it/s] 98%|█████████▊| 196/200 [02:04<00:02,  1.65it/s] 98%|█████████▊| 197/200 [02:05<00:01,  1.64it/s] 99%|█████████▉| 198/200 [02:05<00:01,  1.64it/s]100%|█████████▉| 199/200 [02:06<00:00,  1.63it/s]100%|██████████| 200/200 [02:07<00:00,  1.61it/s][INFO|trainer.py:3846] 2024-11-26 09:05:50,378 >> Saving model checkpoint to /tmp/test-clm2-gpt2-lora-wikitext-103-raw-v1/checkpoint-200
[INFO|configuration_utils.py:690] 2024-11-26 09:05:50,389 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:05:50,390 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:05:50,396 >> tokenizer config file saved in /tmp/test-clm2-gpt2-lora-wikitext-103-raw-v1/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:05:50,396 >> Special tokens file saved in /tmp/test-clm2-gpt2-lora-wikitext-103-raw-v1/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-26 09:05:50,450 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [02:07<00:00,  1.61it/s]100%|██████████| 200/200 [02:07<00:00,  1.57it/s]
[INFO|trainer.py:3846] 2024-11-26 09:05:50,453 >> Saving model checkpoint to /tmp/test-clm2-gpt2-lora-wikitext-103-raw-v1
[INFO|configuration_utils.py:690] 2024-11-26 09:05:50,463 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:05:50,463 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:05:50,468 >> tokenizer config file saved in /tmp/test-clm2-gpt2-lora-wikitext-103-raw-v1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:05:50,468 >> Special tokens file saved in /tmp/test-clm2-gpt2-lora-wikitext-103-raw-v1/special_tokens_map.json
{'train_runtime': 127.1939, 'train_samples_per_second': 25.158, 'train_steps_per_second': 1.572, 'train_loss': 3.1560348510742187, 'epoch': 0.03}
***** train metrics *****
  epoch                    =      0.028
  total_flos               =  1568221GF
  train_loss               =      3.156
  train_runtime            = 0:02:07.19
  train_samples            =     114248
  train_samples_per_second =     25.158
  train_steps_per_second   =      1.572
11/26/2024 09:05:50 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-26 09:05:50,515 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-26 09:05:50,515 >>   Num examples = 240
[INFO|trainer.py:4167] 2024-11-26 09:05:50,515 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.64it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.93it/s] 50%|█████     | 4/8 [00:02<00:02,  1.71it/s] 62%|██████▎   | 5/8 [00:02<00:01,  1.58it/s] 75%|███████▌  | 6/8 [00:03<00:01,  1.54it/s] 88%|████████▊ | 7/8 [00:04<00:00,  1.49it/s]100%|██████████| 8/8 [00:04<00:00,  1.71it/s]100%|██████████| 8/8 [00:05<00:00,  1.55it/s]
***** eval metrics *****
  epoch                   =      0.028
  eval_accuracy           =     0.4231
  eval_loss               =     3.0643
  eval_runtime            = 0:00:05.89
  eval_samples            =        240
  eval_samples_per_second =     40.697
  eval_steps_per_second   =      1.357
  perplexity              =    21.4191
------------------------------------
LagEmbed: Training openai-community/gpt2 on wikitext (wikitext-103-raw-v1) with LagEmbed (in_channels=768, n_components=2, dof=4)
11/26/2024 09:06:02 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/26/2024 09:06:02 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1/runs/Nov26_09-06-02_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
11/26/2024 09:06:02 - WARNING - datasets.load - Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-103-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 10:47:57 2024).
11/26/2024 09:06:02 - WARNING - datasets.packaged_modules.cache.cache - Found the latest cached dataset configuration 'wikitext-103-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 10:47:57 2024).
Using custom data configuration wikitext-103-raw-v1
11/26/2024 09:06:02 - INFO - datasets.builder - Using custom data configuration wikitext-103-raw-v1
Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
11/26/2024 09:06:02 - INFO - datasets.info - Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
Overwrite dataset info from restored data version if exists.
11/26/2024 09:06:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/26/2024 09:06:02 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-26 09:06:02,759 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:06:02,760 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-base-wikitext-103-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:06:02,774 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:06:02,774 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:06:02,774 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:06:02,774 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:06:02,774 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:06:02,774 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-26 09:06:02,886 >> loading weights file /tmp/test-clm-gpt2-base-wikitext-103-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-26 09:06:02,890 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-26 09:06:02,922 >> All model checkpoint weights were used when initializing LagGPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-26 09:06:02,922 >> All the weights of LagGPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-base-wikitext-103-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LagGPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-26 09:06:02,923 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-103-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-26 09:06:02,923 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e6644fbc2010e737.arrow
11/26/2024 09:06:02 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e6644fbc2010e737.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-fb2be7315144d6ce.arrow
11/26/2024 09:06:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-fb2be7315144d6ce.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-00e01a7ca3ac3883.arrow
11/26/2024 09:06:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-00e01a7ca3ac3883.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5c5729770c73f88b.arrow
11/26/2024 09:06:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5c5729770c73f88b.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-923a2f1142745f49.arrow
11/26/2024 09:06:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-923a2f1142745f49.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-77276478acb46eeb.arrow
11/26/2024 09:06:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-77276478acb46eeb.arrow
11/26/2024 09:06:04 - WARNING - evaluate.loading - Using the latest cached version of the module from /home/cs/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Nov 18 09:05:53 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
[WARNING|trainer.py:664] 2024-11-26 09:06:06,041 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-26 09:06:06,172 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-26 09:06:06,172 >>   Num examples = 114,248
[INFO|trainer.py:2337] 2024-11-26 09:06:06,172 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-26 09:06:06,172 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:2340] 2024-11-26 09:06:06,172 >>   Training with DataParallel so batch size has been adjusted to: 32
[INFO|trainer.py:2341] 2024-11-26 09:06:06,172 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2342] 2024-11-26 09:06:06,172 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-26 09:06:06,172 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-26 09:06:06,172 >>   Number of trainable parameters = 403,594
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:07<24:02,  7.25s/it]  1%|          | 2/200 [00:08<11:24,  3.46s/it]  2%|▏         | 3/200 [00:08<07:21,  2.24s/it]  2%|▏         | 4/200 [00:09<05:27,  1.67s/it]  2%|▎         | 5/200 [00:10<04:25,  1.36s/it]  3%|▎         | 6/200 [00:11<03:47,  1.17s/it]  4%|▎         | 7/200 [00:12<03:24,  1.06s/it]  4%|▍         | 8/200 [00:12<03:08,  1.02it/s]  4%|▍         | 9/200 [00:13<02:56,  1.09it/s]  5%|▌         | 10/200 [00:14<02:47,  1.13it/s]  6%|▌         | 11/200 [00:15<02:41,  1.17it/s]  6%|▌         | 12/200 [00:16<02:36,  1.20it/s]  6%|▋         | 13/200 [00:16<02:33,  1.22it/s]  7%|▋         | 14/200 [00:17<02:31,  1.22it/s]  8%|▊         | 15/200 [00:18<02:29,  1.24it/s]  8%|▊         | 16/200 [00:19<02:28,  1.24it/s]  8%|▊         | 17/200 [00:20<02:27,  1.24it/s]  9%|▉         | 18/200 [00:20<02:26,  1.24it/s] 10%|▉         | 19/200 [00:21<02:25,  1.25it/s] 10%|█         | 20/200 [00:22<02:23,  1.26it/s] 10%|█         | 21/200 [00:23<02:22,  1.26it/s] 11%|█         | 22/200 [00:24<02:20,  1.26it/s] 12%|█▏        | 23/200 [00:24<02:19,  1.27it/s] 12%|█▏        | 24/200 [00:25<02:18,  1.27it/s] 12%|█▎        | 25/200 [00:26<02:16,  1.28it/s] 13%|█▎        | 26/200 [00:27<02:15,  1.28it/s] 14%|█▎        | 27/200 [00:27<02:15,  1.28it/s] 14%|█▍        | 28/200 [00:28<02:14,  1.28it/s] 14%|█▍        | 29/200 [00:29<02:13,  1.28it/s] 15%|█▌        | 30/200 [00:30<02:12,  1.28it/s] 16%|█▌        | 31/200 [00:31<02:11,  1.28it/s] 16%|█▌        | 32/200 [00:31<02:11,  1.28it/s] 16%|█▋        | 33/200 [00:32<02:10,  1.28it/s] 17%|█▋        | 34/200 [00:33<02:09,  1.28it/s] 18%|█▊        | 35/200 [00:34<02:08,  1.29it/s] 18%|█▊        | 36/200 [00:34<02:07,  1.29it/s] 18%|█▊        | 37/200 [00:35<02:06,  1.29it/s] 19%|█▉        | 38/200 [00:36<02:05,  1.29it/s] 20%|█▉        | 39/200 [00:37<02:04,  1.29it/s] 20%|██        | 40/200 [00:38<02:03,  1.29it/s] 20%|██        | 41/200 [00:38<02:02,  1.29it/s] 21%|██        | 42/200 [00:39<02:01,  1.30it/s] 22%|██▏       | 43/200 [00:40<02:07,  1.23it/s] 22%|██▏       | 44/200 [00:41<02:05,  1.24it/s] 22%|██▎       | 45/200 [00:42<02:02,  1.26it/s] 23%|██▎       | 46/200 [00:42<02:01,  1.27it/s] 24%|██▎       | 47/200 [00:43<02:00,  1.26it/s] 24%|██▍       | 48/200 [00:44<02:00,  1.26it/s] 24%|██▍       | 49/200 [00:45<01:59,  1.26it/s] 25%|██▌       | 50/200 [00:45<01:59,  1.26it/s] 26%|██▌       | 51/200 [00:46<01:59,  1.25it/s] 26%|██▌       | 52/200 [00:47<01:58,  1.25it/s] 26%|██▋       | 53/200 [00:48<01:57,  1.26it/s] 27%|██▋       | 54/200 [00:49<01:55,  1.26it/s] 28%|██▊       | 55/200 [00:49<01:55,  1.26it/s] 28%|██▊       | 56/200 [00:50<01:54,  1.26it/s] 28%|██▊       | 57/200 [00:51<01:52,  1.27it/s] 29%|██▉       | 58/200 [00:52<01:51,  1.27it/s] 30%|██▉       | 59/200 [00:53<01:50,  1.27it/s] 30%|███       | 60/200 [00:53<01:50,  1.27it/s] 30%|███       | 61/200 [00:54<01:50,  1.26it/s] 31%|███       | 62/200 [00:55<01:49,  1.26it/s] 32%|███▏      | 63/200 [00:56<01:48,  1.27it/s] 32%|███▏      | 64/200 [00:57<01:46,  1.27it/s] 32%|███▎      | 65/200 [00:57<01:46,  1.27it/s] 33%|███▎      | 66/200 [00:58<01:44,  1.28it/s] 34%|███▎      | 67/200 [00:59<01:43,  1.28it/s] 34%|███▍      | 68/200 [01:00<01:43,  1.28it/s] 34%|███▍      | 69/200 [01:00<01:42,  1.28it/s] 35%|███▌      | 70/200 [01:01<01:41,  1.28it/s] 36%|███▌      | 71/200 [01:02<01:40,  1.28it/s] 36%|███▌      | 72/200 [01:03<01:39,  1.29it/s] 36%|███▋      | 73/200 [01:04<01:38,  1.28it/s] 37%|███▋      | 74/200 [01:04<01:37,  1.29it/s] 38%|███▊      | 75/200 [01:05<01:36,  1.29it/s] 38%|███▊      | 76/200 [01:06<01:36,  1.29it/s] 38%|███▊      | 77/200 [01:07<01:35,  1.28it/s] 39%|███▉      | 78/200 [01:07<01:34,  1.28it/s] 40%|███▉      | 79/200 [01:08<01:33,  1.29it/s] 40%|████      | 80/200 [01:09<01:32,  1.30it/s] 40%|████      | 81/200 [01:10<01:35,  1.24it/s] 41%|████      | 82/200 [01:11<01:34,  1.25it/s] 42%|████▏     | 83/200 [01:11<01:32,  1.27it/s] 42%|████▏     | 84/200 [01:12<01:31,  1.27it/s] 42%|████▎     | 85/200 [01:13<01:29,  1.28it/s] 43%|████▎     | 86/200 [01:14<01:28,  1.28it/s] 44%|████▎     | 87/200 [01:15<01:28,  1.28it/s] 44%|████▍     | 88/200 [01:15<01:27,  1.28it/s] 44%|████▍     | 89/200 [01:16<01:26,  1.29it/s] 45%|████▌     | 90/200 [01:17<01:25,  1.29it/s] 46%|████▌     | 91/200 [01:18<01:24,  1.30it/s] 46%|████▌     | 92/200 [01:18<01:23,  1.30it/s] 46%|████▋     | 93/200 [01:19<01:22,  1.30it/s] 47%|████▋     | 94/200 [01:20<01:21,  1.30it/s] 48%|████▊     | 95/200 [01:21<01:20,  1.30it/s] 48%|████▊     | 96/200 [01:21<01:20,  1.29it/s] 48%|████▊     | 97/200 [01:22<01:19,  1.29it/s] 49%|████▉     | 98/200 [01:23<01:19,  1.29it/s] 50%|████▉     | 99/200 [01:24<01:18,  1.29it/s] 50%|█████     | 100/200 [01:25<01:18,  1.28it/s] 50%|█████     | 101/200 [01:25<01:17,  1.27it/s] 51%|█████     | 102/200 [01:26<01:16,  1.28it/s] 52%|█████▏    | 103/200 [01:27<01:16,  1.27it/s] 52%|█████▏    | 104/200 [01:28<01:15,  1.28it/s] 52%|█████▎    | 105/200 [01:29<01:14,  1.27it/s] 53%|█████▎    | 106/200 [01:29<01:14,  1.26it/s] 54%|█████▎    | 107/200 [01:30<01:13,  1.27it/s] 54%|█████▍    | 108/200 [01:31<01:12,  1.27it/s] 55%|█████▍    | 109/200 [01:32<01:11,  1.27it/s] 55%|█████▌    | 110/200 [01:32<01:10,  1.27it/s] 56%|█████▌    | 111/200 [01:33<01:09,  1.27it/s] 56%|█████▌    | 112/200 [01:34<01:09,  1.27it/s] 56%|█████▋    | 113/200 [01:35<01:08,  1.28it/s] 57%|█████▋    | 114/200 [01:36<01:07,  1.28it/s] 57%|█████▊    | 115/200 [01:36<01:06,  1.27it/s] 58%|█████▊    | 116/200 [01:37<01:05,  1.28it/s] 58%|█████▊    | 117/200 [01:38<01:05,  1.27it/s] 59%|█████▉    | 118/200 [01:39<01:04,  1.27it/s] 60%|█████▉    | 119/200 [01:40<01:04,  1.26it/s] 60%|██████    | 120/200 [01:40<01:03,  1.26it/s] 60%|██████    | 121/200 [01:41<01:02,  1.26it/s] 61%|██████    | 122/200 [01:42<01:01,  1.26it/s] 62%|██████▏   | 123/200 [01:43<01:00,  1.27it/s] 62%|██████▏   | 124/200 [01:43<00:59,  1.27it/s] 62%|██████▎   | 125/200 [01:44<00:59,  1.27it/s] 63%|██████▎   | 126/200 [01:45<00:58,  1.27it/s] 64%|██████▎   | 127/200 [01:46<00:57,  1.27it/s] 64%|██████▍   | 128/200 [01:47<00:56,  1.27it/s] 64%|██████▍   | 129/200 [01:47<00:55,  1.27it/s] 65%|██████▌   | 130/200 [01:48<00:54,  1.28it/s] 66%|██████▌   | 131/200 [01:49<00:54,  1.28it/s] 66%|██████▌   | 132/200 [01:50<00:53,  1.27it/s] 66%|██████▋   | 133/200 [01:51<00:52,  1.27it/s] 67%|██████▋   | 134/200 [01:51<00:51,  1.27it/s] 68%|██████▊   | 135/200 [01:52<00:51,  1.27it/s] 68%|██████▊   | 136/200 [01:53<00:50,  1.27it/s] 68%|██████▊   | 137/200 [01:54<00:49,  1.28it/s] 69%|██████▉   | 138/200 [01:54<00:48,  1.28it/s] 70%|██████▉   | 139/200 [01:55<00:47,  1.28it/s] 70%|███████   | 140/200 [01:56<00:46,  1.28it/s] 70%|███████   | 141/200 [01:57<00:46,  1.28it/s] 71%|███████   | 142/200 [01:58<00:45,  1.27it/s] 72%|███████▏  | 143/200 [01:58<00:44,  1.27it/s] 72%|███████▏  | 144/200 [01:59<00:43,  1.28it/s] 72%|███████▎  | 145/200 [02:00<00:43,  1.28it/s] 73%|███████▎  | 146/200 [02:01<00:42,  1.27it/s] 74%|███████▎  | 147/200 [02:02<00:41,  1.27it/s] 74%|███████▍  | 148/200 [02:02<00:41,  1.27it/s] 74%|███████▍  | 149/200 [02:03<00:40,  1.27it/s] 75%|███████▌  | 150/200 [02:04<00:39,  1.27it/s] 76%|███████▌  | 151/200 [02:05<00:38,  1.26it/s] 76%|███████▌  | 152/200 [02:06<00:38,  1.26it/s] 76%|███████▋  | 153/200 [02:06<00:38,  1.22it/s] 77%|███████▋  | 154/200 [02:07<00:37,  1.23it/s] 78%|███████▊  | 155/200 [02:08<00:36,  1.24it/s] 78%|███████▊  | 156/200 [02:09<00:35,  1.24it/s] 78%|███████▊  | 157/200 [02:10<00:34,  1.25it/s] 79%|███████▉  | 158/200 [02:10<00:33,  1.26it/s] 80%|███████▉  | 159/200 [02:11<00:32,  1.26it/s] 80%|████████  | 160/200 [02:12<00:31,  1.26it/s] 80%|████████  | 161/200 [02:13<00:30,  1.26it/s] 81%|████████  | 162/200 [02:14<00:30,  1.26it/s] 82%|████████▏ | 163/200 [02:14<00:29,  1.26it/s] 82%|████████▏ | 164/200 [02:15<00:28,  1.26it/s] 82%|████████▎ | 165/200 [02:16<00:27,  1.27it/s] 83%|████████▎ | 166/200 [02:17<00:26,  1.26it/s] 84%|████████▎ | 167/200 [02:17<00:26,  1.26it/s] 84%|████████▍ | 168/200 [02:18<00:25,  1.26it/s] 84%|████████▍ | 169/200 [02:19<00:24,  1.27it/s] 85%|████████▌ | 170/200 [02:20<00:23,  1.27it/s] 86%|████████▌ | 171/200 [02:21<00:22,  1.27it/s] 86%|████████▌ | 172/200 [02:21<00:22,  1.27it/s] 86%|████████▋ | 173/200 [02:22<00:21,  1.27it/s] 87%|████████▋ | 174/200 [02:23<00:20,  1.27it/s] 88%|████████▊ | 175/200 [02:24<00:19,  1.27it/s] 88%|████████▊ | 176/200 [02:25<00:18,  1.27it/s] 88%|████████▊ | 177/200 [02:25<00:18,  1.27it/s] 89%|████████▉ | 178/200 [02:26<00:17,  1.27it/s] 90%|████████▉ | 179/200 [02:27<00:16,  1.27it/s] 90%|█████████ | 180/200 [02:28<00:15,  1.27it/s] 90%|█████████ | 181/200 [02:29<00:14,  1.27it/s] 91%|█████████ | 182/200 [02:29<00:14,  1.27it/s] 92%|█████████▏| 183/200 [02:30<00:13,  1.26it/s] 92%|█████████▏| 184/200 [02:31<00:12,  1.27it/s] 92%|█████████▎| 185/200 [02:32<00:11,  1.28it/s] 93%|█████████▎| 186/200 [02:32<00:10,  1.28it/s] 94%|█████████▎| 187/200 [02:33<00:10,  1.28it/s] 94%|█████████▍| 188/200 [02:34<00:09,  1.28it/s] 94%|█████████▍| 189/200 [02:35<00:08,  1.28it/s] 95%|█████████▌| 190/200 [02:36<00:07,  1.28it/s] 96%|█████████▌| 191/200 [02:36<00:07,  1.28it/s] 96%|█████████▌| 192/200 [02:37<00:06,  1.28it/s] 96%|█████████▋| 193/200 [02:38<00:05,  1.28it/s] 97%|█████████▋| 194/200 [02:39<00:04,  1.27it/s] 98%|█████████▊| 195/200 [02:39<00:03,  1.27it/s] 98%|█████████▊| 196/200 [02:40<00:03,  1.27it/s] 98%|█████████▊| 197/200 [02:41<00:02,  1.27it/s] 99%|█████████▉| 198/200 [02:42<00:01,  1.27it/s]100%|█████████▉| 199/200 [02:43<00:00,  1.27it/s]100%|██████████| 200/200 [02:43<00:00,  1.27it/s][INFO|trainer.py:3846] 2024-11-26 09:08:50,088 >> Saving model checkpoint to /tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1/checkpoint-200
[INFO|configuration_utils.py:416] 2024-11-26 09:08:50,089 >> Configuration saved in /tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1/checkpoint-200/config.json
[INFO|configuration_utils.py:873] 2024-11-26 09:08:50,090 >> Configuration saved in /tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-26 09:08:51,122 >> Model weights saved in /tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1/checkpoint-200/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:08:51,123 >> tokenizer config file saved in /tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:08:51,123 >> Special tokens file saved in /tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-26 09:08:51,173 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [02:44<00:00,  1.27it/s]100%|██████████| 200/200 [02:44<00:00,  1.21it/s]
[INFO|trainer.py:3846] 2024-11-26 09:08:51,175 >> Saving model checkpoint to /tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1
[INFO|configuration_utils.py:416] 2024-11-26 09:08:51,176 >> Configuration saved in /tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:873] 2024-11-26 09:08:51,176 >> Configuration saved in /tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-26 09:08:52,294 >> Model weights saved in /tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:08:52,295 >> tokenizer config file saved in /tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:08:52,295 >> Special tokens file saved in /tmp/test-clm2-gpt2-lagembed-wikitext-103-raw-v1/special_tokens_map.json
{'train_runtime': 165.0008, 'train_samples_per_second': 38.788, 'train_steps_per_second': 1.212, 'train_loss': 3.0389154052734373, 'epoch': 0.06}
***** train metrics *****
  epoch                    =      0.056
  total_flos               =  3129623GF
  train_loss               =     3.0389
  train_runtime            = 0:02:45.00
  train_samples            =     114248
  train_samples_per_second =     38.788
  train_steps_per_second   =      1.212
11/26/2024 09:08:52 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-26 09:08:52,342 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-26 09:08:52,342 >>   Num examples = 240
[INFO|trainer.py:4167] 2024-11-26 09:08:52,342 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.89it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.05it/s] 50%|█████     | 4/8 [00:02<00:02,  1.77it/s] 62%|██████▎   | 5/8 [00:02<00:01,  1.65it/s] 75%|███████▌  | 6/8 [00:03<00:01,  1.57it/s] 88%|████████▊ | 7/8 [00:04<00:00,  1.53it/s]100%|██████████| 8/8 [00:04<00:00,  1.75it/s]100%|██████████| 8/8 [00:04<00:00,  1.60it/s]
***** eval metrics *****
  epoch                   =      0.056
  eval_accuracy           =     0.4237
  eval_loss               =     3.0606
  eval_runtime            = 0:00:05.70
  eval_samples            =        240
  eval_samples_per_second =     42.092
  eval_steps_per_second   =      1.403
  perplexity              =    21.3399
------------------------------------
LoRA: Fine-tuning openai-community/gpt2-medium on wikitext (wikitext-2-raw-v1) using baseline model
11/26/2024 09:09:03 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/26/2024 09:09:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm2-gpt2-medium-lora-wikitext-2-raw-v1/runs/Nov26_09-09-03_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm2-gpt2-medium-lora-wikitext-2-raw-v1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm2-gpt2-medium-lora-wikitext-2-raw-v1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
11/26/2024 09:09:03 - WARNING - datasets.load - Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 09:26:37 2024).
11/26/2024 09:09:03 - WARNING - datasets.packaged_modules.cache.cache - Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 09:26:37 2024).
Using custom data configuration wikitext-2-raw-v1
11/26/2024 09:09:03 - INFO - datasets.builder - Using custom data configuration wikitext-2-raw-v1
Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
11/26/2024 09:09:03 - INFO - datasets.info - Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
Overwrite dataset info from restored data version if exists.
11/26/2024 09:09:03 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/26/2024 09:09:03 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-26 09:09:03,258 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:09:03,259 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:09:03,269 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:09:03,269 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:09:03,269 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:09:03,269 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:09:03,269 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:09:03,269 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-26 09:09:03,374 >> loading weights file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-26 09:09:03,380 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-26 09:09:03,442 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-26 09:09:03,442 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-26 09:09:03,444 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-26 09:09:03,444 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-0f2c01d7be648209.arrow
11/26/2024 09:09:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-0f2c01d7be648209.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-8c40282a74da6fa9.arrow
11/26/2024 09:09:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-8c40282a74da6fa9.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-0e495039892a0181.arrow
11/26/2024 09:09:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-0e495039892a0181.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-d3bc33d1e020fd56.arrow
11/26/2024 09:09:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-d3bc33d1e020fd56.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-1f63afd6607f6b8b.arrow
11/26/2024 09:09:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-1f63afd6607f6b8b.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-a37ce5b11bb20b8e.arrow
11/26/2024 09:09:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-a37ce5b11bb20b8e.arrow
11/26/2024 09:09:05 - WARNING - evaluate.loading - Using the latest cached version of the module from /home/cs/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Nov 18 09:05:53 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
[WARNING|trainer.py:664] 2024-11-26 09:09:06,285 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-26 09:09:06,406 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-26 09:09:06,406 >>   Num examples = 2,318
[INFO|trainer.py:2337] 2024-11-26 09:09:06,406 >>   Num Epochs = 2
[INFO|trainer.py:2338] 2024-11-26 09:09:06,406 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-26 09:09:06,406 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-26 09:09:06,406 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-26 09:09:06,406 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-26 09:09:06,406 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-26 09:09:06,407 >>   Number of trainable parameters = 786,432
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:05<18:50,  5.68s/it]  1%|          | 2/200 [00:06<09:42,  2.94s/it]  2%|▏         | 3/200 [00:07<06:46,  2.07s/it]  2%|▏         | 4/200 [00:08<05:24,  1.66s/it]  2%|▎         | 5/200 [00:09<04:38,  1.43s/it]  3%|▎         | 6/200 [00:10<04:10,  1.29s/it]  4%|▎         | 7/200 [00:12<04:09,  1.29s/it]  4%|▍         | 8/200 [00:13<04:07,  1.29s/it]  4%|▍         | 9/200 [00:14<04:05,  1.29s/it]  5%|▌         | 10/200 [00:15<04:03,  1.28s/it]  6%|▌         | 11/200 [00:17<04:02,  1.28s/it]  6%|▌         | 12/200 [00:18<04:01,  1.28s/it]  6%|▋         | 13/200 [00:19<03:59,  1.28s/it]  7%|▋         | 14/200 [00:21<04:00,  1.29s/it]  8%|▊         | 15/200 [00:22<03:58,  1.29s/it]  8%|▊         | 16/200 [00:23<03:56,  1.29s/it]  8%|▊         | 17/200 [00:24<03:55,  1.29s/it]  9%|▉         | 18/200 [00:26<03:53,  1.28s/it] 10%|▉         | 19/200 [00:27<03:51,  1.28s/it] 10%|█         | 20/200 [00:28<03:50,  1.28s/it] 10%|█         | 21/200 [00:30<03:51,  1.29s/it] 11%|█         | 22/200 [00:31<03:48,  1.29s/it] 12%|█▏        | 23/200 [00:32<03:47,  1.28s/it] 12%|█▏        | 24/200 [00:33<03:44,  1.28s/it] 12%|█▎        | 25/200 [00:35<03:43,  1.28s/it] 13%|█▎        | 26/200 [00:36<03:42,  1.28s/it] 14%|█▎        | 27/200 [00:37<03:40,  1.28s/it] 14%|█▍        | 28/200 [00:39<03:40,  1.28s/it] 14%|█▍        | 29/200 [00:40<03:39,  1.28s/it] 15%|█▌        | 30/200 [00:41<03:37,  1.28s/it] 16%|█▌        | 31/200 [00:42<03:35,  1.27s/it] 16%|█▌        | 32/200 [00:44<03:34,  1.28s/it] 16%|█▋        | 33/200 [00:45<03:33,  1.28s/it] 17%|█▋        | 34/200 [00:46<03:32,  1.28s/it] 18%|█▊        | 35/200 [00:47<03:30,  1.28s/it] 18%|█▊        | 36/200 [00:49<03:27,  1.27s/it] 18%|█▊        | 37/200 [00:50<03:25,  1.26s/it] 19%|█▉        | 38/200 [00:51<03:24,  1.26s/it] 20%|█▉        | 39/200 [00:52<03:23,  1.27s/it] 20%|██        | 40/200 [00:54<03:22,  1.27s/it] 20%|██        | 41/200 [00:55<03:21,  1.27s/it] 21%|██        | 42/200 [00:56<03:20,  1.27s/it] 22%|██▏       | 43/200 [00:58<03:21,  1.28s/it] 22%|██▏       | 44/200 [00:59<03:19,  1.28s/it] 22%|██▎       | 45/200 [01:00<03:18,  1.28s/it] 23%|██▎       | 46/200 [01:01<03:16,  1.28s/it] 24%|██▎       | 47/200 [01:03<03:15,  1.28s/it] 24%|██▍       | 48/200 [01:04<03:14,  1.28s/it] 24%|██▍       | 49/200 [01:05<03:12,  1.28s/it] 25%|██▌       | 50/200 [01:07<03:11,  1.27s/it] 26%|██▌       | 51/200 [01:08<03:11,  1.28s/it] 26%|██▌       | 52/200 [01:09<03:09,  1.28s/it] 26%|██▋       | 53/200 [01:10<03:07,  1.28s/it] 27%|██▋       | 54/200 [01:12<03:06,  1.27s/it] 28%|██▊       | 55/200 [01:13<03:04,  1.27s/it] 28%|██▊       | 56/200 [01:14<03:02,  1.27s/it] 28%|██▊       | 57/200 [01:15<03:01,  1.27s/it] 29%|██▉       | 58/200 [01:17<03:00,  1.27s/it] 30%|██▉       | 59/200 [01:18<03:00,  1.28s/it] 30%|███       | 60/200 [01:19<02:58,  1.28s/it] 30%|███       | 61/200 [01:21<02:57,  1.28s/it] 31%|███       | 62/200 [01:22<02:56,  1.28s/it] 32%|███▏      | 63/200 [01:23<02:55,  1.28s/it] 32%|███▏      | 64/200 [01:24<02:53,  1.28s/it] 32%|███▎      | 65/200 [01:26<02:53,  1.28s/it] 33%|███▎      | 66/200 [01:27<02:51,  1.28s/it] 34%|███▎      | 67/200 [01:28<02:49,  1.27s/it] 34%|███▍      | 68/200 [01:30<02:48,  1.27s/it] 34%|███▍      | 69/200 [01:31<02:46,  1.27s/it] 35%|███▌      | 70/200 [01:32<02:45,  1.27s/it] 36%|███▌      | 71/200 [01:33<02:44,  1.27s/it] 36%|███▌      | 72/200 [01:35<02:41,  1.26s/it] 36%|███▋      | 73/200 [01:36<02:40,  1.26s/it] 37%|███▋      | 74/200 [01:37<02:39,  1.27s/it] 38%|███▊      | 75/200 [01:38<02:38,  1.27s/it] 38%|███▊      | 76/200 [01:40<02:37,  1.27s/it] 38%|███▊      | 77/200 [01:41<02:36,  1.27s/it] 39%|███▉      | 78/200 [01:42<02:35,  1.27s/it] 40%|███▉      | 79/200 [01:44<02:34,  1.28s/it] 40%|████      | 80/200 [01:45<02:33,  1.28s/it] 40%|████      | 81/200 [01:46<02:32,  1.28s/it] 41%|████      | 82/200 [01:47<02:30,  1.28s/it] 42%|████▏     | 83/200 [01:49<02:29,  1.28s/it] 42%|████▏     | 84/200 [01:50<02:27,  1.27s/it] 42%|████▎     | 85/200 [01:51<02:26,  1.27s/it] 43%|████▎     | 86/200 [01:52<02:25,  1.27s/it] 44%|████▎     | 87/200 [01:54<02:25,  1.28s/it] 44%|████▍     | 88/200 [01:55<02:23,  1.28s/it] 44%|████▍     | 89/200 [01:56<02:21,  1.28s/it] 45%|████▌     | 90/200 [01:58<02:20,  1.28s/it] 46%|████▌     | 91/200 [01:59<02:19,  1.28s/it] 46%|████▌     | 92/200 [02:00<02:17,  1.27s/it] 46%|████▋     | 93/200 [02:01<02:16,  1.27s/it] 47%|████▋     | 94/200 [02:03<02:14,  1.27s/it] 48%|████▊     | 95/200 [02:04<02:14,  1.28s/it] 48%|████▊     | 96/200 [02:05<02:12,  1.28s/it] 48%|████▊     | 97/200 [02:07<02:11,  1.28s/it] 49%|████▉     | 98/200 [02:08<02:10,  1.28s/it] 50%|████▉     | 99/200 [02:09<02:08,  1.27s/it] 50%|█████     | 100/200 [02:10<02:07,  1.27s/it] 50%|█████     | 101/200 [02:12<02:07,  1.29s/it] 51%|█████     | 102/200 [02:13<02:05,  1.28s/it] 52%|█████▏    | 103/200 [02:14<02:04,  1.28s/it] 52%|█████▏    | 104/200 [02:15<02:02,  1.28s/it] 52%|█████▎    | 105/200 [02:17<02:01,  1.28s/it] 53%|█████▎    | 106/200 [02:18<01:59,  1.27s/it] 54%|█████▎    | 107/200 [02:19<01:58,  1.28s/it] 54%|█████▍    | 108/200 [02:21<01:56,  1.26s/it] 55%|█████▍    | 109/200 [02:22<01:55,  1.26s/it] 55%|█████▌    | 110/200 [02:23<01:53,  1.27s/it] 56%|█████▌    | 111/200 [02:24<01:52,  1.27s/it] 56%|█████▌    | 112/200 [02:26<01:51,  1.27s/it] 56%|█████▋    | 113/200 [02:27<01:50,  1.27s/it] 57%|█████▋    | 114/200 [02:28<01:49,  1.27s/it] 57%|█████▊    | 115/200 [02:29<01:48,  1.28s/it] 58%|█████▊    | 116/200 [02:31<01:47,  1.28s/it] 58%|█████▊    | 117/200 [02:32<01:45,  1.28s/it] 59%|█████▉    | 118/200 [02:33<01:44,  1.27s/it] 60%|█████▉    | 119/200 [02:35<01:43,  1.27s/it] 60%|██████    | 120/200 [02:36<01:41,  1.27s/it] 60%|██████    | 121/200 [02:37<01:40,  1.27s/it] 61%|██████    | 122/200 [02:38<01:39,  1.28s/it] 62%|██████▏   | 123/200 [02:40<01:38,  1.28s/it] 62%|██████▏   | 124/200 [02:41<01:37,  1.28s/it] 62%|██████▎   | 125/200 [02:42<01:35,  1.28s/it] 63%|██████▎   | 126/200 [02:43<01:34,  1.28s/it] 64%|██████▎   | 127/200 [02:45<01:33,  1.27s/it] 64%|██████▍   | 128/200 [02:46<01:31,  1.28s/it] 64%|██████▍   | 129/200 [02:47<01:30,  1.28s/it] 65%|██████▌   | 130/200 [02:49<01:29,  1.27s/it] 66%|██████▌   | 131/200 [02:50<01:28,  1.29s/it] 66%|██████▌   | 132/200 [02:51<01:27,  1.28s/it] 66%|██████▋   | 133/200 [02:52<01:25,  1.28s/it] 67%|██████▋   | 134/200 [02:54<01:24,  1.28s/it] 68%|██████▊   | 135/200 [02:55<01:22,  1.28s/it] 68%|██████▊   | 136/200 [02:56<01:21,  1.27s/it] 68%|██████▊   | 137/200 [02:58<01:20,  1.28s/it] 69%|██████▉   | 138/200 [02:59<01:19,  1.28s/it] 70%|██████▉   | 139/200 [03:00<01:17,  1.28s/it] 70%|███████   | 140/200 [03:01<01:16,  1.27s/it] 70%|███████   | 141/200 [03:03<01:15,  1.27s/it] 71%|███████   | 142/200 [03:04<01:13,  1.27s/it] 72%|███████▏  | 143/200 [03:05<01:12,  1.27s/it] 72%|███████▏  | 144/200 [03:06<01:10,  1.26s/it] 72%|███████▎  | 145/200 [03:08<01:07,  1.23s/it] 73%|███████▎  | 146/200 [03:09<01:07,  1.25s/it] 74%|███████▎  | 147/200 [03:10<01:06,  1.25s/it] 74%|███████▍  | 148/200 [03:11<01:05,  1.26s/it] 74%|███████▍  | 149/200 [03:13<01:04,  1.26s/it] 75%|███████▌  | 150/200 [03:14<01:03,  1.27s/it] 76%|███████▌  | 151/200 [03:15<01:02,  1.28s/it] 76%|███████▌  | 152/200 [03:17<01:01,  1.28s/it] 76%|███████▋  | 153/200 [03:18<01:00,  1.28s/it] 77%|███████▋  | 154/200 [03:19<00:58,  1.28s/it] 78%|███████▊  | 155/200 [03:20<00:57,  1.27s/it] 78%|███████▊  | 156/200 [03:22<00:56,  1.27s/it] 78%|███████▊  | 157/200 [03:23<00:54,  1.27s/it] 79%|███████▉  | 158/200 [03:24<00:53,  1.27s/it] 80%|███████▉  | 159/200 [03:25<00:52,  1.29s/it] 80%|████████  | 160/200 [03:27<00:51,  1.28s/it] 80%|████████  | 161/200 [03:28<00:49,  1.28s/it] 81%|████████  | 162/200 [03:29<00:48,  1.28s/it] 82%|████████▏ | 163/200 [03:31<00:47,  1.27s/it] 82%|████████▏ | 164/200 [03:32<00:45,  1.27s/it] 82%|████████▎ | 165/200 [03:33<00:44,  1.27s/it] 83%|████████▎ | 166/200 [03:34<00:43,  1.27s/it] 84%|████████▎ | 167/200 [03:36<00:42,  1.28s/it] 84%|████████▍ | 168/200 [03:37<00:40,  1.28s/it] 84%|████████▍ | 169/200 [03:38<00:39,  1.28s/it] 85%|████████▌ | 170/200 [03:39<00:38,  1.27s/it] 86%|████████▌ | 171/200 [03:41<00:36,  1.27s/it] 86%|████████▌ | 172/200 [03:42<00:35,  1.27s/it] 86%|████████▋ | 173/200 [03:43<00:34,  1.28s/it] 87%|████████▋ | 174/200 [03:45<00:33,  1.28s/it] 88%|████████▊ | 175/200 [03:46<00:31,  1.28s/it] 88%|████████▊ | 176/200 [03:47<00:30,  1.28s/it] 88%|████████▊ | 177/200 [03:48<00:29,  1.27s/it] 89%|████████▉ | 178/200 [03:50<00:28,  1.27s/it] 90%|████████▉ | 179/200 [03:51<00:26,  1.27s/it] 90%|█████████ | 180/200 [03:52<00:25,  1.26s/it] 90%|█████████ | 181/200 [03:53<00:23,  1.25s/it] 91%|█████████ | 182/200 [03:55<00:22,  1.26s/it] 92%|█████████▏| 183/200 [03:56<00:21,  1.26s/it] 92%|█████████▏| 184/200 [03:57<00:20,  1.27s/it] 92%|█████████▎| 185/200 [03:59<00:18,  1.27s/it] 93%|█████████▎| 186/200 [04:00<00:17,  1.27s/it] 94%|█████████▎| 187/200 [04:01<00:16,  1.28s/it] 94%|█████████▍| 188/200 [04:02<00:15,  1.28s/it] 94%|█████████▍| 189/200 [04:04<00:14,  1.28s/it] 95%|█████████▌| 190/200 [04:05<00:12,  1.28s/it] 96%|█████████▌| 191/200 [04:06<00:11,  1.28s/it] 96%|█████████▌| 192/200 [04:07<00:10,  1.27s/it] 96%|█████████▋| 193/200 [04:09<00:08,  1.27s/it] 97%|█████████▋| 194/200 [04:10<00:07,  1.27s/it] 98%|█████████▊| 195/200 [04:11<00:06,  1.28s/it] 98%|█████████▊| 196/200 [04:13<00:05,  1.28s/it] 98%|█████████▊| 197/200 [04:14<00:03,  1.28s/it] 99%|█████████▉| 198/200 [04:15<00:02,  1.28s/it]100%|█████████▉| 199/200 [04:16<00:01,  1.27s/it]100%|██████████| 200/200 [04:18<00:00,  1.27s/it][INFO|trainer.py:3846] 2024-11-26 09:13:24,576 >> Saving model checkpoint to /tmp/test-clm2-gpt2-medium-lora-wikitext-2-raw-v1/checkpoint-200
[INFO|configuration_utils.py:690] 2024-11-26 09:13:24,588 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:13:24,588 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-medium",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:13:24,596 >> tokenizer config file saved in /tmp/test-clm2-gpt2-medium-lora-wikitext-2-raw-v1/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:13:24,596 >> Special tokens file saved in /tmp/test-clm2-gpt2-medium-lora-wikitext-2-raw-v1/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-26 09:13:24,654 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [04:18<00:00,  1.27s/it]100%|██████████| 200/200 [04:18<00:00,  1.29s/it]
[INFO|trainer.py:3846] 2024-11-26 09:13:24,655 >> Saving model checkpoint to /tmp/test-clm2-gpt2-medium-lora-wikitext-2-raw-v1
[INFO|configuration_utils.py:690] 2024-11-26 09:13:24,665 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:13:24,665 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-medium",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:13:24,671 >> tokenizer config file saved in /tmp/test-clm2-gpt2-medium-lora-wikitext-2-raw-v1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:13:24,672 >> Special tokens file saved in /tmp/test-clm2-gpt2-medium-lora-wikitext-2-raw-v1/special_tokens_map.json
{'train_runtime': 258.2468, 'train_samples_per_second': 12.391, 'train_steps_per_second': 0.774, 'train_loss': 2.7321148681640626, 'epoch': 1.38}
***** train metrics *****
  epoch                    =     1.3793
  total_flos               =  5546418GF
  train_loss               =     2.7321
  train_runtime            = 0:04:18.24
  train_samples            =       2318
  train_samples_per_second =     12.391
  train_steps_per_second   =      0.774
11/26/2024 09:13:24 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-26 09:13:24,726 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-26 09:13:24,726 >>   Num examples = 240
[INFO|trainer.py:4167] 2024-11-26 09:13:24,726 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:01<00:03,  1.58it/s] 38%|███▊      | 3/8 [00:02<00:04,  1.10it/s] 50%|█████     | 4/8 [00:03<00:04,  1.04s/it] 62%|██████▎   | 5/8 [00:05<00:03,  1.12s/it] 75%|███████▌  | 6/8 [00:06<00:02,  1.16s/it] 88%|████████▊ | 7/8 [00:07<00:01,  1.19s/it]100%|██████████| 8/8 [00:08<00:00,  1.05s/it]100%|██████████| 8/8 [00:08<00:00,  1.10s/it]
***** eval metrics *****
  epoch                   =     1.3793
  eval_accuracy           =      0.455
  eval_loss               =     2.7799
  eval_runtime            = 0:00:10.11
  eval_samples            =        240
  eval_samples_per_second =     23.735
  eval_steps_per_second   =      0.791
  perplexity              =     16.118
------------------------------------
LagEmbed: Training openai-community/gpt2-medium on wikitext (wikitext-2-raw-v1) with LagEmbed (in_channels=1024, n_components=2, dof=8)
11/26/2024 09:13:40 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/26/2024 09:13:40 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1/runs/Nov26_09-13-40_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
11/26/2024 09:13:40 - WARNING - datasets.load - Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 09:26:37 2024).
11/26/2024 09:13:40 - WARNING - datasets.packaged_modules.cache.cache - Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 09:26:37 2024).
Using custom data configuration wikitext-2-raw-v1
11/26/2024 09:13:40 - INFO - datasets.builder - Using custom data configuration wikitext-2-raw-v1
Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
11/26/2024 09:13:40 - INFO - datasets.info - Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
Overwrite dataset info from restored data version if exists.
11/26/2024 09:13:40 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/26/2024 09:13:40 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-26 09:13:40,618 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:13:40,620 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:13:40,639 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:13:40,639 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:13:40,639 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:13:40,640 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:13:40,640 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:13:40,640 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-26 09:13:40,739 >> loading weights file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-26 09:13:40,745 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-26 09:13:40,808 >> All model checkpoint weights were used when initializing LagGPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-26 09:13:40,808 >> All the weights of LagGPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LagGPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-26 09:13:40,810 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-26 09:13:40,810 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-0f2c01d7be648209.arrow
11/26/2024 09:13:40 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-0f2c01d7be648209.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-8c40282a74da6fa9.arrow
11/26/2024 09:13:40 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-8c40282a74da6fa9.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-0e495039892a0181.arrow
11/26/2024 09:13:40 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-0e495039892a0181.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-d3bc33d1e020fd56.arrow
11/26/2024 09:13:40 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-d3bc33d1e020fd56.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-1f63afd6607f6b8b.arrow
11/26/2024 09:13:40 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-1f63afd6607f6b8b.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-a37ce5b11bb20b8e.arrow
11/26/2024 09:13:40 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-a37ce5b11bb20b8e.arrow
11/26/2024 09:13:42 - WARNING - evaluate.loading - Using the latest cached version of the module from /home/cs/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Nov 18 09:05:53 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
[WARNING|trainer.py:664] 2024-11-26 09:13:43,983 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-26 09:13:44,102 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-26 09:13:44,102 >>   Num examples = 2,318
[INFO|trainer.py:2337] 2024-11-26 09:13:44,102 >>   Num Epochs = 3
[INFO|trainer.py:2338] 2024-11-26 09:13:44,102 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:2340] 2024-11-26 09:13:44,102 >>   Training with DataParallel so batch size has been adjusted to: 32
[INFO|trainer.py:2341] 2024-11-26 09:13:44,102 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2342] 2024-11-26 09:13:44,103 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-26 09:13:44,103 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-26 09:13:44,103 >>   Number of trainable parameters = 806,162
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:05<19:27,  5.87s/it]  1%|          | 2/200 [00:07<10:13,  3.10s/it]  2%|▏         | 3/200 [00:08<07:15,  2.21s/it]  2%|▏         | 4/200 [00:09<05:51,  1.79s/it]  2%|▎         | 5/200 [00:10<05:04,  1.56s/it]  3%|▎         | 6/200 [00:11<04:36,  1.43s/it]  4%|▎         | 7/200 [00:12<04:18,  1.34s/it]  4%|▍         | 8/200 [00:13<04:05,  1.28s/it]  4%|▍         | 9/200 [00:15<03:56,  1.24s/it]  5%|▌         | 10/200 [00:16<03:50,  1.21s/it]  6%|▌         | 11/200 [00:17<03:46,  1.20s/it]  6%|▌         | 12/200 [00:18<03:42,  1.18s/it]  6%|▋         | 13/200 [00:19<03:39,  1.17s/it]  7%|▋         | 14/200 [00:20<03:37,  1.17s/it]  8%|▊         | 15/200 [00:22<03:35,  1.17s/it]  8%|▊         | 16/200 [00:23<03:39,  1.19s/it]  8%|▊         | 17/200 [00:24<03:36,  1.18s/it]  9%|▉         | 18/200 [00:25<03:33,  1.17s/it] 10%|▉         | 19/200 [00:26<03:31,  1.17s/it] 10%|█         | 20/200 [00:27<03:29,  1.17s/it] 10%|█         | 21/200 [00:29<03:28,  1.16s/it] 11%|█         | 22/200 [00:30<03:26,  1.16s/it] 12%|█▏        | 23/200 [00:31<03:25,  1.16s/it] 12%|█▏        | 24/200 [00:32<03:23,  1.16s/it] 12%|█▎        | 25/200 [00:33<03:22,  1.16s/it] 13%|█▎        | 26/200 [00:34<03:21,  1.16s/it] 14%|█▎        | 27/200 [00:36<03:21,  1.17s/it] 14%|█▍        | 28/200 [00:37<03:21,  1.17s/it] 14%|█▍        | 29/200 [00:38<03:19,  1.17s/it] 15%|█▌        | 30/200 [00:39<03:18,  1.17s/it] 16%|█▌        | 31/200 [00:40<03:21,  1.19s/it] 16%|█▌        | 32/200 [00:41<03:18,  1.18s/it] 16%|█▋        | 33/200 [00:43<03:15,  1.17s/it] 17%|█▋        | 34/200 [00:44<03:14,  1.17s/it] 18%|█▊        | 35/200 [00:45<03:12,  1.17s/it] 18%|█▊        | 36/200 [00:46<03:10,  1.16s/it] 18%|█▊        | 37/200 [00:47<03:09,  1.16s/it] 19%|█▉        | 38/200 [00:48<03:08,  1.16s/it] 20%|█▉        | 39/200 [00:50<03:07,  1.16s/it] 20%|██        | 40/200 [00:51<03:05,  1.16s/it] 20%|██        | 41/200 [00:52<03:04,  1.16s/it] 21%|██        | 42/200 [00:53<03:03,  1.16s/it] 22%|██▏       | 43/200 [00:54<03:02,  1.16s/it] 22%|██▏       | 44/200 [00:55<03:00,  1.16s/it] 22%|██▎       | 45/200 [00:57<02:59,  1.16s/it] 23%|██▎       | 46/200 [00:58<03:02,  1.19s/it] 24%|██▎       | 47/200 [00:59<03:00,  1.18s/it] 24%|██▍       | 48/200 [01:00<02:58,  1.17s/it] 24%|██▍       | 49/200 [01:01<02:56,  1.17s/it] 25%|██▌       | 50/200 [01:02<02:54,  1.17s/it] 26%|██▌       | 51/200 [01:04<02:53,  1.16s/it] 26%|██▌       | 52/200 [01:05<02:52,  1.16s/it] 26%|██▋       | 53/200 [01:06<02:51,  1.16s/it] 27%|██▋       | 54/200 [01:07<02:49,  1.16s/it] 28%|██▊       | 55/200 [01:08<02:48,  1.16s/it] 28%|██▊       | 56/200 [01:09<02:46,  1.16s/it] 28%|██▊       | 57/200 [01:11<02:45,  1.16s/it] 29%|██▉       | 58/200 [01:12<02:47,  1.18s/it] 30%|██▉       | 59/200 [01:13<02:45,  1.17s/it] 30%|███       | 60/200 [01:14<02:43,  1.17s/it] 30%|███       | 61/200 [01:15<02:42,  1.17s/it] 31%|███       | 62/200 [01:16<02:40,  1.16s/it] 32%|███▏      | 63/200 [01:18<02:39,  1.16s/it] 32%|███▏      | 64/200 [01:19<02:38,  1.16s/it] 32%|███▎      | 65/200 [01:20<02:36,  1.16s/it] 33%|███▎      | 66/200 [01:21<02:35,  1.16s/it] 34%|███▎      | 67/200 [01:22<02:34,  1.16s/it] 34%|███▍      | 68/200 [01:23<02:33,  1.16s/it] 34%|███▍      | 69/200 [01:25<02:32,  1.16s/it] 35%|███▌      | 70/200 [01:26<02:30,  1.16s/it] 36%|███▌      | 71/200 [01:27<02:29,  1.16s/it] 36%|███▌      | 72/200 [01:28<02:27,  1.16s/it] 36%|███▋      | 73/200 [01:29<02:07,  1.00s/it] 37%|███▋      | 74/200 [01:30<02:16,  1.08s/it] 38%|███▊      | 75/200 [01:31<02:18,  1.11s/it] 38%|███▊      | 76/200 [01:32<02:19,  1.12s/it] 38%|███▊      | 77/200 [01:33<02:19,  1.13s/it] 39%|███▉      | 78/200 [01:35<02:19,  1.14s/it] 40%|███▉      | 79/200 [01:36<02:18,  1.15s/it] 40%|████      | 80/200 [01:37<02:18,  1.15s/it] 40%|████      | 81/200 [01:38<02:17,  1.15s/it] 41%|████      | 82/200 [01:39<02:16,  1.16s/it] 42%|████▏     | 83/200 [01:40<02:15,  1.16s/it] 42%|████▏     | 84/200 [01:41<02:14,  1.16s/it] 42%|████▎     | 85/200 [01:43<02:13,  1.16s/it] 43%|████▎     | 86/200 [01:44<02:12,  1.16s/it] 44%|████▎     | 87/200 [01:45<02:10,  1.16s/it] 44%|████▍     | 88/200 [01:46<02:09,  1.16s/it] 44%|████▍     | 89/200 [01:47<02:11,  1.19s/it] 45%|████▌     | 90/200 [01:49<02:09,  1.18s/it] 46%|████▌     | 91/200 [01:50<02:07,  1.17s/it] 46%|████▌     | 92/200 [01:51<02:06,  1.17s/it] 46%|████▋     | 93/200 [01:52<02:04,  1.16s/it] 47%|████▋     | 94/200 [01:53<02:03,  1.16s/it] 48%|████▊     | 95/200 [01:54<02:02,  1.16s/it] 48%|████▊     | 96/200 [01:55<02:00,  1.16s/it] 48%|████▊     | 97/200 [01:57<01:59,  1.16s/it] 49%|████▉     | 98/200 [01:58<01:58,  1.16s/it] 50%|████▉     | 99/200 [01:59<01:57,  1.16s/it] 50%|█████     | 100/200 [02:00<01:55,  1.16s/it] 50%|█████     | 101/200 [02:01<01:54,  1.16s/it] 51%|█████     | 102/200 [02:02<01:53,  1.16s/it] 52%|█████▏    | 103/200 [02:04<01:52,  1.16s/it] 52%|█████▏    | 104/200 [02:05<01:54,  1.19s/it] 52%|█████▎    | 105/200 [02:06<01:52,  1.18s/it] 53%|█████▎    | 106/200 [02:07<01:50,  1.17s/it] 54%|█████▎    | 107/200 [02:08<01:48,  1.17s/it] 54%|█████▍    | 108/200 [02:10<01:47,  1.17s/it] 55%|█████▍    | 109/200 [02:11<01:45,  1.16s/it] 55%|█████▌    | 110/200 [02:12<01:44,  1.16s/it] 56%|█████▌    | 111/200 [02:13<01:43,  1.16s/it] 56%|█████▌    | 112/200 [02:14<01:42,  1.16s/it] 56%|█████▋    | 113/200 [02:15<01:40,  1.16s/it] 57%|█████▋    | 114/200 [02:16<01:39,  1.16s/it] 57%|█████▊    | 115/200 [02:18<01:38,  1.16s/it] 58%|█████▊    | 116/200 [02:19<01:39,  1.18s/it] 58%|█████▊    | 117/200 [02:20<01:37,  1.18s/it] 59%|█████▉    | 118/200 [02:21<01:36,  1.17s/it] 60%|█████▉    | 119/200 [02:22<01:34,  1.17s/it] 60%|██████    | 120/200 [02:23<01:33,  1.16s/it] 60%|██████    | 121/200 [02:25<01:31,  1.16s/it] 61%|██████    | 122/200 [02:26<01:30,  1.16s/it] 62%|██████▏   | 123/200 [02:27<01:29,  1.16s/it] 62%|██████▏   | 124/200 [02:28<01:28,  1.16s/it] 62%|██████▎   | 125/200 [02:29<01:26,  1.16s/it] 63%|██████▎   | 126/200 [02:30<01:25,  1.16s/it] 64%|██████▎   | 127/200 [02:32<01:24,  1.16s/it] 64%|██████▍   | 128/200 [02:33<01:23,  1.16s/it] 64%|██████▍   | 129/200 [02:34<01:22,  1.16s/it] 65%|██████▌   | 130/200 [02:35<01:21,  1.16s/it] 66%|██████▌   | 131/200 [02:36<01:20,  1.16s/it] 66%|██████▌   | 132/200 [02:38<01:20,  1.19s/it] 66%|██████▋   | 133/200 [02:39<01:19,  1.18s/it] 67%|██████▋   | 134/200 [02:40<01:17,  1.17s/it] 68%|██████▊   | 135/200 [02:41<01:16,  1.17s/it] 68%|██████▊   | 136/200 [02:42<01:14,  1.17s/it] 68%|██████▊   | 137/200 [02:43<01:13,  1.16s/it] 69%|██████▉   | 138/200 [02:44<01:12,  1.16s/it] 70%|██████▉   | 139/200 [02:46<01:10,  1.16s/it] 70%|███████   | 140/200 [02:47<01:09,  1.16s/it] 70%|███████   | 141/200 [02:48<01:08,  1.16s/it] 71%|███████   | 142/200 [02:49<01:07,  1.16s/it] 72%|███████▏  | 143/200 [02:50<01:06,  1.16s/it] 72%|███████▏  | 144/200 [02:51<01:04,  1.16s/it] 72%|███████▎  | 145/200 [02:53<01:03,  1.16s/it] 73%|███████▎  | 146/200 [02:53<00:54,  1.00s/it] 74%|███████▎  | 147/200 [02:54<00:57,  1.08s/it] 74%|███████▍  | 148/200 [02:56<00:57,  1.11s/it] 74%|███████▍  | 149/200 [02:57<00:57,  1.12s/it] 75%|███████▌  | 150/200 [02:58<00:56,  1.13s/it] 76%|███████▌  | 151/200 [02:59<00:55,  1.14s/it] 76%|███████▌  | 152/200 [03:00<00:55,  1.15s/it] 76%|███████▋  | 153/200 [03:01<00:54,  1.15s/it] 77%|███████▋  | 154/200 [03:03<00:53,  1.15s/it] 78%|███████▊  | 155/200 [03:04<00:51,  1.16s/it] 78%|███████▊  | 156/200 [03:05<00:50,  1.16s/it] 78%|███████▊  | 157/200 [03:06<00:49,  1.16s/it] 79%|███████▉  | 158/200 [03:07<00:48,  1.16s/it] 80%|███████▉  | 159/200 [03:08<00:47,  1.16s/it] 80%|████████  | 160/200 [03:10<00:46,  1.16s/it] 80%|████████  | 161/200 [03:11<00:45,  1.16s/it] 81%|████████  | 162/200 [03:12<00:45,  1.19s/it] 82%|████████▏ | 163/200 [03:13<00:43,  1.18s/it] 82%|████████▏ | 164/200 [03:14<00:42,  1.17s/it] 82%|████████▎ | 165/200 [03:15<00:40,  1.17s/it] 83%|████████▎ | 166/200 [03:17<00:39,  1.17s/it] 84%|████████▎ | 167/200 [03:18<00:38,  1.17s/it] 84%|████████▍ | 168/200 [03:19<00:37,  1.16s/it] 84%|████████▍ | 169/200 [03:20<00:36,  1.16s/it] 85%|████████▌ | 170/200 [03:21<00:34,  1.16s/it] 86%|████████▌ | 171/200 [03:22<00:33,  1.16s/it] 86%|████████▌ | 172/200 [03:24<00:32,  1.16s/it] 86%|████████▋ | 173/200 [03:25<00:31,  1.16s/it] 87%|████████▋ | 174/200 [03:26<00:30,  1.18s/it] 88%|████████▊ | 175/200 [03:27<00:29,  1.18s/it] 88%|████████▊ | 176/200 [03:28<00:28,  1.17s/it] 88%|████████▊ | 177/200 [03:29<00:26,  1.17s/it] 89%|████████▉ | 178/200 [03:31<00:25,  1.17s/it] 90%|████████▉ | 179/200 [03:32<00:24,  1.17s/it] 90%|█████████ | 180/200 [03:33<00:23,  1.16s/it] 90%|█████████ | 181/200 [03:34<00:22,  1.16s/it] 91%|█████████ | 182/200 [03:35<00:20,  1.16s/it] 92%|█████████▏| 183/200 [03:36<00:19,  1.16s/it] 92%|█████████▏| 184/200 [03:38<00:18,  1.16s/it] 92%|█████████▎| 185/200 [03:39<00:17,  1.16s/it] 93%|█████████▎| 186/200 [03:40<00:16,  1.16s/it] 94%|█████████▎| 187/200 [03:41<00:15,  1.16s/it] 94%|█████████▍| 188/200 [03:42<00:13,  1.16s/it] 94%|█████████▍| 189/200 [03:43<00:12,  1.16s/it] 95%|█████████▌| 190/200 [03:45<00:11,  1.19s/it] 96%|█████████▌| 191/200 [03:46<00:10,  1.18s/it] 96%|█████████▌| 192/200 [03:47<00:09,  1.17s/it] 96%|█████████▋| 193/200 [03:48<00:08,  1.17s/it] 97%|█████████▋| 194/200 [03:49<00:06,  1.17s/it] 98%|█████████▊| 195/200 [03:50<00:05,  1.16s/it] 98%|█████████▊| 196/200 [03:52<00:04,  1.16s/it] 98%|█████████▊| 197/200 [03:53<00:03,  1.16s/it] 99%|█████████▉| 198/200 [03:54<00:02,  1.16s/it]100%|█████████▉| 199/200 [03:55<00:01,  1.16s/it]100%|██████████| 200/200 [03:56<00:00,  1.16s/it][INFO|trainer.py:3846] 2024-11-26 09:17:40,838 >> Saving model checkpoint to /tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1/checkpoint-200
[INFO|configuration_utils.py:416] 2024-11-26 09:17:40,839 >> Configuration saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1/checkpoint-200/config.json
[INFO|configuration_utils.py:873] 2024-11-26 09:17:40,840 >> Configuration saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-26 09:17:42,592 >> Model weights saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1/checkpoint-200/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:17:42,593 >> tokenizer config file saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:17:42,593 >> Special tokens file saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-26 09:17:42,662 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [03:58<00:00,  1.16s/it]100%|██████████| 200/200 [03:58<00:00,  1.19s/it]
[INFO|trainer.py:3846] 2024-11-26 09:17:42,663 >> Saving model checkpoint to /tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1
[INFO|configuration_utils.py:416] 2024-11-26 09:17:42,664 >> Configuration saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:873] 2024-11-26 09:17:42,664 >> Configuration saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-26 09:17:44,347 >> Model weights saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:17:44,348 >> tokenizer config file saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:17:44,348 >> Special tokens file saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-2-raw-v1/special_tokens_map.json
{'train_runtime': 238.5587, 'train_samples_per_second': 26.828, 'train_steps_per_second': 0.838, 'train_loss': 2.6461810302734374, 'epoch': 2.74}
***** train metrics *****
  epoch                    =     2.7397
  total_flos               = 11038057GF
  train_loss               =     2.6462
  train_runtime            = 0:03:58.55
  train_samples            =       2318
  train_samples_per_second =     26.828
  train_steps_per_second   =      0.838
11/26/2024 09:17:44 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-26 09:17:44,385 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-26 09:17:44,385 >>   Num examples = 240
[INFO|trainer.py:4167] 2024-11-26 09:17:44,385 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:01<00:03,  1.63it/s] 38%|███▊      | 3/8 [00:02<00:04,  1.14it/s] 50%|█████     | 4/8 [00:03<00:04,  1.01s/it] 62%|██████▎   | 5/8 [00:04<00:03,  1.09s/it] 75%|███████▌  | 6/8 [00:06<00:02,  1.14s/it] 88%|████████▊ | 7/8 [00:07<00:01,  1.16s/it]100%|██████████| 8/8 [00:08<00:00,  1.02s/it]100%|██████████| 8/8 [00:08<00:00,  1.07s/it]
***** eval metrics *****
  epoch                   =     2.7397
  eval_accuracy           =     0.4553
  eval_loss               =     2.7794
  eval_runtime            = 0:00:09.74
  eval_samples            =        240
  eval_samples_per_second =     24.627
  eval_steps_per_second   =      0.821
  perplexity              =    16.1086
------------------------------------
LoRA: Fine-tuning openai-community/gpt2-medium on wikitext (wikitext-103-raw-v1) using baseline model
11/26/2024 09:17:59 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/26/2024 09:17:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm2-gpt2-medium-lora-wikitext-103-raw-v1/runs/Nov26_09-17-59_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm2-gpt2-medium-lora-wikitext-103-raw-v1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm2-gpt2-medium-lora-wikitext-103-raw-v1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
11/26/2024 09:17:59 - WARNING - datasets.load - Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-103-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 10:47:57 2024).
11/26/2024 09:17:59 - WARNING - datasets.packaged_modules.cache.cache - Found the latest cached dataset configuration 'wikitext-103-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 10:47:57 2024).
Using custom data configuration wikitext-103-raw-v1
11/26/2024 09:17:59 - INFO - datasets.builder - Using custom data configuration wikitext-103-raw-v1
Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
11/26/2024 09:17:59 - INFO - datasets.info - Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
Overwrite dataset info from restored data version if exists.
11/26/2024 09:17:59 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/26/2024 09:17:59 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-26 09:17:59,671 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:17:59,674 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-medium-base-wikitext-103-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:17:59,696 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:17:59,696 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:17:59,696 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:17:59,696 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:17:59,696 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:17:59,696 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-26 09:17:59,822 >> loading weights file /tmp/test-clm-gpt2-medium-base-wikitext-103-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-26 09:17:59,828 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-26 09:17:59,892 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-26 09:17:59,892 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-medium-base-wikitext-103-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-26 09:17:59,894 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-103-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-26 09:17:59,894 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-89a71f4dbd84c73b.arrow
11/26/2024 09:17:59 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-89a71f4dbd84c73b.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-97c797bd44a505ec.arrow
11/26/2024 09:18:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-97c797bd44a505ec.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-15734f0af1f1363b.arrow
11/26/2024 09:18:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-15734f0af1f1363b.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-b03524853584bcf6.arrow
11/26/2024 09:18:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-b03524853584bcf6.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-73d70dbe93ddb60c.arrow
11/26/2024 09:18:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-73d70dbe93ddb60c.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-83d6f185e4988c25.arrow
11/26/2024 09:18:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-83d6f185e4988c25.arrow
11/26/2024 09:18:02 - WARNING - evaluate.loading - Using the latest cached version of the module from /home/cs/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Nov 18 09:05:53 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
[WARNING|trainer.py:664] 2024-11-26 09:18:04,403 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-26 09:18:04,531 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-26 09:18:04,531 >>   Num examples = 114,248
[INFO|trainer.py:2337] 2024-11-26 09:18:04,531 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-26 09:18:04,531 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-26 09:18:04,531 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-26 09:18:04,531 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-26 09:18:04,531 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-26 09:18:04,531 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-26 09:18:04,532 >>   Number of trainable parameters = 786,432
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:06<20:40,  6.24s/it]  1%|          | 2/200 [00:07<10:27,  3.17s/it]  2%|▏         | 3/200 [00:08<07:11,  2.19s/it]  2%|▏         | 4/200 [00:09<05:38,  1.73s/it]  2%|▎         | 5/200 [00:10<04:47,  1.47s/it]  3%|▎         | 6/200 [00:11<04:15,  1.32s/it]  4%|▎         | 7/200 [00:12<03:55,  1.22s/it]  4%|▍         | 8/200 [00:13<04:00,  1.25s/it]  4%|▍         | 9/200 [00:14<04:00,  1.26s/it]  5%|▌         | 10/200 [00:16<04:00,  1.26s/it]  6%|▌         | 11/200 [00:17<04:00,  1.27s/it]  6%|▌         | 12/200 [00:18<03:59,  1.28s/it]  6%|▋         | 13/200 [00:20<03:58,  1.28s/it]  7%|▋         | 14/200 [00:21<03:57,  1.28s/it]  8%|▊         | 15/200 [00:22<03:58,  1.29s/it]  8%|▊         | 16/200 [00:23<03:56,  1.29s/it]  8%|▊         | 17/200 [00:25<03:54,  1.28s/it]  9%|▉         | 18/200 [00:26<03:53,  1.28s/it] 10%|▉         | 19/200 [00:27<03:51,  1.28s/it] 10%|█         | 20/200 [00:29<03:49,  1.28s/it] 10%|█         | 21/200 [00:30<03:48,  1.28s/it] 11%|█         | 22/200 [00:31<03:47,  1.28s/it] 12%|█▏        | 23/200 [00:32<03:48,  1.29s/it] 12%|█▏        | 24/200 [00:34<03:46,  1.29s/it] 12%|█▎        | 25/200 [00:35<03:45,  1.29s/it] 13%|█▎        | 26/200 [00:36<03:43,  1.28s/it] 14%|█▎        | 27/200 [00:38<03:41,  1.28s/it] 14%|█▍        | 28/200 [00:39<03:39,  1.28s/it] 14%|█▍        | 29/200 [00:40<03:38,  1.28s/it] 15%|█▌        | 30/200 [00:41<03:37,  1.28s/it] 16%|█▌        | 31/200 [00:43<03:37,  1.29s/it] 16%|█▌        | 32/200 [00:44<03:36,  1.29s/it] 16%|█▋        | 33/200 [00:45<03:34,  1.28s/it] 17%|█▋        | 34/200 [00:47<03:33,  1.28s/it] 18%|█▊        | 35/200 [00:48<03:32,  1.29s/it] 18%|█▊        | 36/200 [00:49<03:31,  1.29s/it] 18%|█▊        | 37/200 [00:50<03:31,  1.30s/it] 19%|█▉        | 38/200 [00:52<03:30,  1.30s/it] 20%|█▉        | 39/200 [00:53<03:27,  1.29s/it] 20%|██        | 40/200 [00:54<03:25,  1.29s/it] 20%|██        | 41/200 [00:56<03:23,  1.28s/it] 21%|██        | 42/200 [00:57<03:22,  1.28s/it] 22%|██▏       | 43/200 [00:58<03:21,  1.28s/it] 22%|██▏       | 44/200 [00:59<03:19,  1.28s/it] 22%|██▎       | 45/200 [01:01<03:24,  1.32s/it] 23%|██▎       | 46/200 [01:02<03:21,  1.31s/it] 24%|██▎       | 47/200 [01:03<03:18,  1.30s/it] 24%|██▍       | 48/200 [01:05<03:16,  1.29s/it] 24%|██▍       | 49/200 [01:06<03:15,  1.29s/it] 25%|██▌       | 50/200 [01:07<03:13,  1.29s/it] 26%|██▌       | 51/200 [01:09<03:11,  1.28s/it] 26%|██▌       | 52/200 [01:10<03:09,  1.28s/it] 26%|██▋       | 53/200 [01:11<03:10,  1.29s/it] 27%|██▋       | 54/200 [01:12<03:08,  1.29s/it] 28%|██▊       | 55/200 [01:14<03:06,  1.29s/it] 28%|██▊       | 56/200 [01:15<03:05,  1.29s/it] 28%|██▊       | 57/200 [01:16<03:03,  1.28s/it] 29%|██▉       | 58/200 [01:18<03:01,  1.28s/it] 30%|██▉       | 59/200 [01:19<03:00,  1.28s/it] 30%|███       | 60/200 [01:20<03:00,  1.29s/it] 30%|███       | 61/200 [01:21<02:58,  1.29s/it] 31%|███       | 62/200 [01:23<02:57,  1.29s/it] 32%|███▏      | 63/200 [01:24<02:56,  1.29s/it] 32%|███▏      | 64/200 [01:25<02:54,  1.29s/it] 32%|███▎      | 65/200 [01:27<02:53,  1.28s/it] 33%|███▎      | 66/200 [01:28<02:51,  1.28s/it] 34%|███▎      | 67/200 [01:29<02:50,  1.28s/it] 34%|███▍      | 68/200 [01:30<02:54,  1.32s/it] 34%|███▍      | 69/200 [01:32<02:51,  1.31s/it] 35%|███▌      | 70/200 [01:33<02:49,  1.30s/it] 36%|███▌      | 71/200 [01:34<02:46,  1.29s/it] 36%|███▌      | 72/200 [01:36<02:45,  1.29s/it] 36%|███▋      | 73/200 [01:37<02:43,  1.29s/it] 37%|███▋      | 74/200 [01:38<02:42,  1.29s/it] 38%|███▊      | 75/200 [01:39<02:42,  1.30s/it] 38%|███▊      | 76/200 [01:41<02:40,  1.29s/it] 38%|███▊      | 77/200 [01:42<02:38,  1.29s/it] 39%|███▉      | 78/200 [01:43<02:37,  1.29s/it] 40%|███▉      | 79/200 [01:45<02:35,  1.29s/it] 40%|████      | 80/200 [01:46<02:34,  1.28s/it] 40%|████      | 81/200 [01:47<02:32,  1.28s/it] 41%|████      | 82/200 [01:48<02:31,  1.28s/it] 42%|████▏     | 83/200 [01:50<02:31,  1.29s/it] 42%|████▏     | 84/200 [01:51<02:29,  1.29s/it] 42%|████▎     | 85/200 [01:52<02:27,  1.29s/it] 43%|████▎     | 86/200 [01:54<02:26,  1.29s/it] 44%|████▎     | 87/200 [01:55<02:24,  1.28s/it] 44%|████▍     | 88/200 [01:56<02:23,  1.29s/it] 44%|████▍     | 89/200 [01:57<02:22,  1.28s/it] 45%|████▌     | 90/200 [01:59<02:20,  1.28s/it] 46%|████▌     | 91/200 [02:00<02:20,  1.29s/it] 46%|████▌     | 92/200 [02:01<02:19,  1.29s/it] 46%|████▋     | 93/200 [02:03<02:17,  1.28s/it] 47%|████▋     | 94/200 [02:04<02:16,  1.28s/it] 48%|████▊     | 95/200 [02:05<02:14,  1.28s/it] 48%|████▊     | 96/200 [02:06<02:13,  1.28s/it] 48%|████▊     | 97/200 [02:08<02:12,  1.28s/it] 49%|████▉     | 98/200 [02:09<02:11,  1.29s/it] 50%|████▉     | 99/200 [02:10<02:10,  1.29s/it] 50%|█████     | 100/200 [02:12<02:08,  1.29s/it] 50%|█████     | 101/200 [02:13<02:07,  1.28s/it] 51%|█████     | 102/200 [02:14<02:05,  1.28s/it] 52%|█████▏    | 103/200 [02:15<02:04,  1.28s/it] 52%|█████▏    | 104/200 [02:17<02:03,  1.28s/it] 52%|█████▎    | 105/200 [02:18<02:05,  1.32s/it] 53%|█████▎    | 106/200 [02:19<02:02,  1.31s/it] 54%|█████▎    | 107/200 [02:21<02:00,  1.30s/it] 54%|█████▍    | 108/200 [02:22<01:59,  1.30s/it] 55%|█████▍    | 109/200 [02:23<01:57,  1.29s/it] 55%|█████▌    | 110/200 [02:25<01:55,  1.29s/it] 56%|█████▌    | 111/200 [02:26<01:54,  1.29s/it] 56%|█████▌    | 112/200 [02:27<01:55,  1.31s/it] 56%|█████▋    | 113/200 [02:28<01:53,  1.30s/it] 57%|█████▋    | 114/200 [02:30<01:52,  1.30s/it] 57%|█████▊    | 115/200 [02:31<01:50,  1.30s/it] 58%|█████▊    | 116/200 [02:32<01:48,  1.29s/it] 58%|█████▊    | 117/200 [02:34<01:47,  1.29s/it] 59%|█████▉    | 118/200 [02:35<01:45,  1.29s/it] 60%|█████▉    | 119/200 [02:36<01:44,  1.29s/it] 60%|██████    | 120/200 [02:38<01:43,  1.30s/it] 60%|██████    | 121/200 [02:39<01:42,  1.29s/it] 61%|██████    | 122/200 [02:40<01:40,  1.29s/it] 62%|██████▏   | 123/200 [02:41<01:39,  1.29s/it] 62%|██████▏   | 124/200 [02:43<01:38,  1.29s/it] 62%|██████▎   | 125/200 [02:44<01:36,  1.29s/it] 63%|██████▎   | 126/200 [02:45<01:35,  1.29s/it] 64%|██████▎   | 127/200 [02:47<01:33,  1.28s/it] 64%|██████▍   | 128/200 [02:48<01:33,  1.30s/it] 64%|██████▍   | 129/200 [02:49<01:31,  1.29s/it] 65%|██████▌   | 130/200 [02:50<01:30,  1.29s/it] 66%|██████▌   | 131/200 [02:52<01:28,  1.29s/it] 66%|██████▌   | 132/200 [02:53<01:27,  1.29s/it] 66%|██████▋   | 133/200 [02:54<01:25,  1.28s/it] 67%|██████▋   | 134/200 [02:56<01:24,  1.28s/it] 68%|██████▊   | 135/200 [02:57<01:24,  1.30s/it] 68%|██████▊   | 136/200 [02:58<01:22,  1.29s/it] 68%|██████▊   | 137/200 [02:59<01:20,  1.28s/it] 69%|██████▉   | 138/200 [03:01<01:19,  1.29s/it] 70%|██████▉   | 139/200 [03:02<01:18,  1.28s/it] 70%|███████   | 140/200 [03:03<01:17,  1.28s/it] 70%|███████   | 141/200 [03:05<01:15,  1.28s/it] 71%|███████   | 142/200 [03:06<01:16,  1.32s/it] 72%|███████▏  | 143/200 [03:07<01:14,  1.31s/it] 72%|███████▏  | 144/200 [03:09<01:12,  1.30s/it] 72%|███████▎  | 145/200 [03:10<01:11,  1.29s/it] 73%|███████▎  | 146/200 [03:11<01:09,  1.29s/it] 74%|███████▎  | 147/200 [03:12<01:08,  1.29s/it] 74%|███████▍  | 148/200 [03:14<01:06,  1.29s/it] 74%|███████▍  | 149/200 [03:15<01:06,  1.30s/it] 75%|███████▌  | 150/200 [03:16<01:04,  1.30s/it] 76%|███████▌  | 151/200 [03:18<01:03,  1.29s/it] 76%|███████▌  | 152/200 [03:19<01:01,  1.29s/it] 76%|███████▋  | 153/200 [03:20<01:00,  1.29s/it] 77%|███████▋  | 154/200 [03:21<00:59,  1.29s/it] 78%|███████▊  | 155/200 [03:23<00:57,  1.29s/it] 78%|███████▊  | 156/200 [03:24<00:56,  1.29s/it] 78%|███████▊  | 157/200 [03:25<00:55,  1.30s/it] 79%|███████▉  | 158/200 [03:27<00:54,  1.29s/it] 80%|███████▉  | 159/200 [03:28<00:52,  1.29s/it] 80%|████████  | 160/200 [03:29<00:51,  1.28s/it] 80%|████████  | 161/200 [03:30<00:50,  1.28s/it] 81%|████████  | 162/200 [03:32<00:48,  1.28s/it] 82%|████████▏ | 163/200 [03:33<00:47,  1.28s/it] 82%|████████▏ | 164/200 [03:34<00:46,  1.28s/it] 82%|████████▎ | 165/200 [03:36<00:45,  1.30s/it] 83%|████████▎ | 166/200 [03:37<00:43,  1.29s/it] 84%|████████▎ | 167/200 [03:38<00:42,  1.29s/it] 84%|████████▍ | 168/200 [03:39<00:41,  1.29s/it] 84%|████████▍ | 169/200 [03:41<00:39,  1.29s/it] 85%|████████▌ | 170/200 [03:42<00:38,  1.28s/it] 86%|████████▌ | 171/200 [03:43<00:37,  1.28s/it] 86%|████████▌ | 172/200 [03:45<00:36,  1.30s/it] 86%|████████▋ | 173/200 [03:46<00:35,  1.30s/it] 87%|████████▋ | 174/200 [03:47<00:33,  1.29s/it] 88%|████████▊ | 175/200 [03:48<00:32,  1.29s/it] 88%|████████▊ | 176/200 [03:50<00:30,  1.28s/it] 88%|████████▊ | 177/200 [03:51<00:29,  1.28s/it] 89%|████████▉ | 178/200 [03:52<00:28,  1.28s/it] 90%|████████▉ | 179/200 [03:54<00:27,  1.31s/it] 90%|█████████ | 180/200 [03:55<00:26,  1.30s/it] 90%|█████████ | 181/200 [03:56<00:24,  1.30s/it] 91%|█████████ | 182/200 [03:58<00:23,  1.29s/it] 92%|█████████▏| 183/200 [03:59<00:21,  1.29s/it] 92%|█████████▏| 184/200 [04:00<00:20,  1.28s/it] 92%|█████████▎| 185/200 [04:01<00:19,  1.28s/it] 93%|█████████▎| 186/200 [04:03<00:18,  1.30s/it] 94%|█████████▎| 187/200 [04:04<00:16,  1.29s/it] 94%|█████████▍| 188/200 [04:05<00:15,  1.29s/it] 94%|█████████▍| 189/200 [04:07<00:14,  1.28s/it] 95%|█████████▌| 190/200 [04:08<00:12,  1.29s/it] 96%|█████████▌| 191/200 [04:09<00:11,  1.28s/it] 96%|█████████▌| 192/200 [04:10<00:10,  1.28s/it] 96%|█████████▋| 193/200 [04:12<00:08,  1.28s/it] 97%|█████████▋| 194/200 [04:13<00:07,  1.29s/it] 98%|█████████▊| 195/200 [04:14<00:06,  1.29s/it] 98%|█████████▊| 196/200 [04:16<00:05,  1.28s/it] 98%|█████████▊| 197/200 [04:17<00:03,  1.28s/it] 99%|█████████▉| 198/200 [04:18<00:02,  1.29s/it]100%|█████████▉| 199/200 [04:19<00:01,  1.28s/it]100%|██████████| 200/200 [04:21<00:00,  1.28s/it][INFO|trainer.py:3846] 2024-11-26 09:22:25,675 >> Saving model checkpoint to /tmp/test-clm2-gpt2-medium-lora-wikitext-103-raw-v1/checkpoint-200
[INFO|configuration_utils.py:690] 2024-11-26 09:22:25,687 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:22:25,687 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-medium",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:22:25,695 >> tokenizer config file saved in /tmp/test-clm2-gpt2-medium-lora-wikitext-103-raw-v1/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:22:25,695 >> Special tokens file saved in /tmp/test-clm2-gpt2-medium-lora-wikitext-103-raw-v1/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-26 09:22:25,753 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [04:21<00:00,  1.28s/it]100%|██████████| 200/200 [04:21<00:00,  1.31s/it]
[INFO|trainer.py:3846] 2024-11-26 09:22:25,755 >> Saving model checkpoint to /tmp/test-clm2-gpt2-medium-lora-wikitext-103-raw-v1
[INFO|configuration_utils.py:690] 2024-11-26 09:22:25,765 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:22:25,765 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-medium",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:22:25,772 >> tokenizer config file saved in /tmp/test-clm2-gpt2-medium-lora-wikitext-103-raw-v1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:22:25,772 >> Special tokens file saved in /tmp/test-clm2-gpt2-medium-lora-wikitext-103-raw-v1/special_tokens_map.json
{'train_runtime': 261.2212, 'train_samples_per_second': 12.25, 'train_steps_per_second': 0.766, 'train_loss': 2.783411865234375, 'epoch': 0.03}
***** train metrics *****
  epoch                    =      0.028
  total_flos               =  5549887GF
  train_loss               =     2.7834
  train_runtime            = 0:04:21.22
  train_samples            =     114248
  train_samples_per_second =      12.25
  train_steps_per_second   =      0.766
11/26/2024 09:22:25 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-26 09:22:25,819 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-26 09:22:25,819 >>   Num examples = 240
[INFO|trainer.py:4167] 2024-11-26 09:22:25,819 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:01<00:03,  1.53it/s] 38%|███▊      | 3/8 [00:02<00:04,  1.11it/s] 50%|█████     | 4/8 [00:03<00:04,  1.04s/it] 62%|██████▎   | 5/8 [00:05<00:03,  1.11s/it] 75%|███████▌  | 6/8 [00:06<00:02,  1.16s/it] 88%|████████▊ | 7/8 [00:07<00:01,  1.19s/it]100%|██████████| 8/8 [00:08<00:00,  1.05s/it]100%|██████████| 8/8 [00:08<00:00,  1.10s/it]
***** eval metrics *****
  epoch                   =      0.028
  eval_accuracy           =     0.4571
  eval_loss               =     2.7655
  eval_runtime            = 0:00:10.08
  eval_samples            =        240
  eval_samples_per_second =      23.79
  eval_steps_per_second   =      0.793
  perplexity              =    15.8873
------------------------------------
LagEmbed: Training openai-community/gpt2-medium on wikitext (wikitext-103-raw-v1) with LagEmbed (in_channels=1024, n_components=2, dof=8)
11/26/2024 09:22:41 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/26/2024 09:22:41 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1/runs/Nov26_09-22-41_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
11/26/2024 09:22:41 - WARNING - datasets.load - Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-103-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 10:47:57 2024).
11/26/2024 09:22:41 - WARNING - datasets.packaged_modules.cache.cache - Found the latest cached dataset configuration 'wikitext-103-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 10:47:57 2024).
Using custom data configuration wikitext-103-raw-v1
11/26/2024 09:22:41 - INFO - datasets.builder - Using custom data configuration wikitext-103-raw-v1
Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
11/26/2024 09:22:41 - INFO - datasets.info - Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
Overwrite dataset info from restored data version if exists.
11/26/2024 09:22:41 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/26/2024 09:22:41 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-26 09:22:41,454 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:22:41,455 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-medium-base-wikitext-103-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:22:41,469 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:22:41,469 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:22:41,469 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:22:41,469 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:22:41,469 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:22:41,469 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-26 09:22:41,581 >> loading weights file /tmp/test-clm-gpt2-medium-base-wikitext-103-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-26 09:22:41,587 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-26 09:22:41,646 >> All model checkpoint weights were used when initializing LagGPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-26 09:22:41,646 >> All the weights of LagGPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-medium-base-wikitext-103-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LagGPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-26 09:22:41,648 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-103-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-26 09:22:41,648 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-89a71f4dbd84c73b.arrow
11/26/2024 09:22:41 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-89a71f4dbd84c73b.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-97c797bd44a505ec.arrow
11/26/2024 09:22:41 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-97c797bd44a505ec.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-15734f0af1f1363b.arrow
11/26/2024 09:22:41 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-15734f0af1f1363b.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-b03524853584bcf6.arrow
11/26/2024 09:22:41 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-b03524853584bcf6.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-73d70dbe93ddb60c.arrow
11/26/2024 09:22:41 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-73d70dbe93ddb60c.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-83d6f185e4988c25.arrow
11/26/2024 09:22:41 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-83d6f185e4988c25.arrow
11/26/2024 09:22:43 - WARNING - evaluate.loading - Using the latest cached version of the module from /home/cs/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Nov 18 09:05:53 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
[WARNING|trainer.py:664] 2024-11-26 09:22:44,944 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-26 09:22:45,073 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-26 09:22:45,073 >>   Num examples = 114,248
[INFO|trainer.py:2337] 2024-11-26 09:22:45,073 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-26 09:22:45,073 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:2340] 2024-11-26 09:22:45,073 >>   Training with DataParallel so batch size has been adjusted to: 32
[INFO|trainer.py:2341] 2024-11-26 09:22:45,073 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2342] 2024-11-26 09:22:45,073 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-26 09:22:45,073 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-26 09:22:45,074 >>   Number of trainable parameters = 806,162
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:06<20:05,  6.06s/it]  1%|          | 2/200 [00:07<10:29,  3.18s/it]  2%|▏         | 3/200 [00:08<07:24,  2.26s/it]  2%|▏         | 4/200 [00:09<05:57,  1.82s/it]  2%|▎         | 5/200 [00:10<05:09,  1.59s/it]  3%|▎         | 6/200 [00:11<04:39,  1.44s/it]  4%|▎         | 7/200 [00:13<04:20,  1.35s/it]  4%|▍         | 8/200 [00:14<04:07,  1.29s/it]  4%|▍         | 9/200 [00:15<03:58,  1.25s/it]  5%|▌         | 10/200 [00:16<03:52,  1.22s/it]  6%|▌         | 11/200 [00:17<03:47,  1.20s/it]  6%|▌         | 12/200 [00:18<03:43,  1.19s/it]  6%|▋         | 13/200 [00:20<03:41,  1.18s/it]  7%|▋         | 14/200 [00:21<03:38,  1.18s/it]  8%|▊         | 15/200 [00:22<03:36,  1.17s/it]  8%|▊         | 16/200 [00:23<03:34,  1.17s/it]  8%|▊         | 17/200 [00:24<03:38,  1.19s/it]  9%|▉         | 18/200 [00:25<03:35,  1.18s/it] 10%|▉         | 19/200 [00:27<03:33,  1.18s/it] 10%|█         | 20/200 [00:28<03:30,  1.17s/it] 10%|█         | 21/200 [00:29<03:29,  1.17s/it] 11%|█         | 22/200 [00:30<03:27,  1.17s/it] 12%|█▏        | 23/200 [00:31<03:26,  1.17s/it] 12%|█▏        | 24/200 [00:32<03:25,  1.16s/it] 12%|█▎        | 25/200 [00:34<03:23,  1.16s/it] 13%|█▎        | 26/200 [00:35<03:22,  1.16s/it] 14%|█▎        | 27/200 [00:36<03:21,  1.16s/it] 14%|█▍        | 28/200 [00:37<03:20,  1.16s/it] 14%|█▍        | 29/200 [00:38<03:18,  1.16s/it] 15%|█▌        | 30/200 [00:39<03:17,  1.16s/it] 16%|█▌        | 31/200 [00:41<03:21,  1.19s/it] 16%|█▌        | 32/200 [00:42<03:18,  1.18s/it] 16%|█▋        | 33/200 [00:43<03:16,  1.18s/it] 17%|█▋        | 34/200 [00:44<03:14,  1.17s/it] 18%|█▊        | 35/200 [00:45<03:12,  1.17s/it] 18%|█▊        | 36/200 [00:46<03:11,  1.17s/it] 18%|█▊        | 37/200 [00:48<03:09,  1.17s/it] 19%|█▉        | 38/200 [00:49<03:08,  1.16s/it] 20%|█▉        | 39/200 [00:50<03:07,  1.16s/it] 20%|██        | 40/200 [00:51<03:06,  1.16s/it] 20%|██        | 41/200 [00:52<03:05,  1.16s/it] 21%|██        | 42/200 [00:53<03:03,  1.16s/it] 22%|██▏       | 43/200 [00:55<03:02,  1.16s/it] 22%|██▏       | 44/200 [00:56<03:01,  1.16s/it] 22%|██▎       | 45/200 [00:57<03:04,  1.19s/it] 23%|██▎       | 46/200 [00:58<03:02,  1.18s/it] 24%|██▎       | 47/200 [00:59<03:00,  1.18s/it] 24%|██▍       | 48/200 [01:00<02:58,  1.17s/it] 24%|██▍       | 49/200 [01:02<02:56,  1.17s/it] 25%|██▌       | 50/200 [01:03<02:55,  1.17s/it] 26%|██▌       | 51/200 [01:04<02:53,  1.17s/it] 26%|██▌       | 52/200 [01:05<02:52,  1.17s/it] 26%|██▋       | 53/200 [01:06<02:51,  1.16s/it] 27%|██▋       | 54/200 [01:07<02:49,  1.16s/it] 28%|██▊       | 55/200 [01:09<02:48,  1.16s/it] 28%|██▊       | 56/200 [01:10<02:47,  1.16s/it] 28%|██▊       | 57/200 [01:11<02:46,  1.16s/it] 29%|██▉       | 58/200 [01:12<02:49,  1.19s/it] 30%|██▉       | 59/200 [01:13<02:46,  1.18s/it] 30%|███       | 60/200 [01:15<02:44,  1.18s/it] 30%|███       | 61/200 [01:16<02:42,  1.17s/it] 31%|███       | 62/200 [01:17<02:41,  1.17s/it] 32%|███▏      | 63/200 [01:18<02:40,  1.17s/it] 32%|███▏      | 64/200 [01:19<02:38,  1.17s/it] 32%|███▎      | 65/200 [01:20<02:37,  1.17s/it] 33%|███▎      | 66/200 [01:21<02:36,  1.16s/it] 34%|███▎      | 67/200 [01:23<02:34,  1.16s/it] 34%|███▍      | 68/200 [01:24<02:33,  1.17s/it] 34%|███▍      | 69/200 [01:25<02:32,  1.17s/it] 35%|███▌      | 70/200 [01:26<02:31,  1.16s/it] 36%|███▌      | 71/200 [01:27<02:30,  1.17s/it] 36%|███▌      | 72/200 [01:28<02:29,  1.16s/it] 36%|███▋      | 73/200 [01:30<02:27,  1.17s/it] 37%|███▋      | 74/200 [01:31<02:26,  1.17s/it] 38%|███▊      | 75/200 [01:32<02:25,  1.16s/it] 38%|███▊      | 76/200 [01:33<02:28,  1.20s/it] 38%|███▊      | 77/200 [01:34<02:25,  1.19s/it] 39%|███▉      | 78/200 [01:36<02:23,  1.18s/it] 40%|███▉      | 79/200 [01:37<02:22,  1.18s/it] 40%|████      | 80/200 [01:38<02:20,  1.17s/it] 40%|████      | 81/200 [01:39<02:19,  1.17s/it] 41%|████      | 82/200 [01:40<02:17,  1.17s/it] 42%|████▏     | 83/200 [01:41<02:16,  1.17s/it] 42%|████▏     | 84/200 [01:43<02:15,  1.17s/it] 42%|████▎     | 85/200 [01:44<02:14,  1.17s/it] 43%|████▎     | 86/200 [01:45<02:12,  1.16s/it] 44%|████▎     | 87/200 [01:46<02:11,  1.16s/it] 44%|████▍     | 88/200 [01:47<02:10,  1.16s/it] 44%|████▍     | 89/200 [01:48<02:09,  1.16s/it] 45%|████▌     | 90/200 [01:50<02:08,  1.16s/it] 46%|████▌     | 91/200 [01:51<02:09,  1.19s/it] 46%|████▌     | 92/200 [01:52<02:07,  1.18s/it] 46%|████▋     | 93/200 [01:53<02:06,  1.18s/it] 47%|████▋     | 94/200 [01:54<02:04,  1.17s/it] 48%|████▊     | 95/200 [01:55<02:03,  1.17s/it] 48%|████▊     | 96/200 [01:57<02:01,  1.17s/it] 48%|████▊     | 97/200 [01:58<02:00,  1.17s/it] 49%|████▉     | 98/200 [01:59<01:58,  1.17s/it] 50%|████▉     | 99/200 [02:00<01:57,  1.17s/it] 50%|█████     | 100/200 [02:01<01:56,  1.17s/it] 50%|█████     | 101/200 [02:02<01:55,  1.17s/it] 51%|█████     | 102/200 [02:04<01:54,  1.16s/it] 52%|█████▏    | 103/200 [02:05<01:53,  1.17s/it] 52%|█████▏    | 104/200 [02:06<01:51,  1.17s/it] 52%|█████▎    | 105/200 [02:07<01:50,  1.17s/it] 53%|█████▎    | 106/200 [02:08<01:52,  1.19s/it] 54%|█████▎    | 107/200 [02:10<01:49,  1.18s/it] 54%|█████▍    | 108/200 [02:11<01:48,  1.18s/it] 55%|█████▍    | 109/200 [02:12<01:46,  1.17s/it] 55%|█████▌    | 110/200 [02:13<01:45,  1.17s/it] 56%|█████▌    | 111/200 [02:14<01:44,  1.17s/it] 56%|█████▌    | 112/200 [02:15<01:42,  1.17s/it] 56%|█████▋    | 113/200 [02:17<01:41,  1.17s/it] 57%|█████▋    | 114/200 [02:18<01:40,  1.17s/it] 57%|█████▊    | 115/200 [02:19<01:39,  1.17s/it] 58%|█████▊    | 116/200 [02:20<01:37,  1.17s/it] 58%|█████▊    | 117/200 [02:21<01:36,  1.17s/it] 59%|█████▉    | 118/200 [02:22<01:35,  1.17s/it] 60%|█████▉    | 119/200 [02:24<01:36,  1.19s/it] 60%|██████    | 120/200 [02:25<01:34,  1.19s/it] 60%|██████    | 121/200 [02:26<01:33,  1.18s/it] 61%|██████    | 122/200 [02:27<01:31,  1.17s/it] 62%|██████▏   | 123/200 [02:28<01:30,  1.17s/it] 62%|██████▏   | 124/200 [02:29<01:28,  1.17s/it] 62%|██████▎   | 125/200 [02:31<01:27,  1.17s/it] 63%|██████▎   | 126/200 [02:32<01:26,  1.17s/it] 64%|██████▎   | 127/200 [02:33<01:25,  1.17s/it] 64%|██████▍   | 128/200 [02:34<01:23,  1.17s/it] 64%|██████▍   | 129/200 [02:35<01:22,  1.17s/it] 65%|██████▌   | 130/200 [02:36<01:21,  1.17s/it] 66%|██████▌   | 131/200 [02:38<01:20,  1.17s/it] 66%|██████▌   | 132/200 [02:39<01:19,  1.17s/it] 66%|██████▋   | 133/200 [02:40<01:18,  1.17s/it] 67%|██████▋   | 134/200 [02:41<01:17,  1.17s/it] 68%|██████▊   | 135/200 [02:42<01:15,  1.17s/it] 68%|██████▊   | 136/200 [02:43<01:14,  1.17s/it] 68%|██████▊   | 137/200 [02:45<01:15,  1.20s/it] 69%|██████▉   | 138/200 [02:46<01:13,  1.19s/it] 70%|██████▉   | 139/200 [02:47<01:12,  1.18s/it] 70%|███████   | 140/200 [02:48<01:10,  1.18s/it] 70%|███████   | 141/200 [02:49<01:09,  1.17s/it] 71%|███████   | 142/200 [02:51<01:07,  1.17s/it] 72%|███████▏  | 143/200 [02:52<01:06,  1.17s/it] 72%|███████▏  | 144/200 [02:53<01:05,  1.17s/it] 72%|███████▎  | 145/200 [02:54<01:04,  1.17s/it] 73%|███████▎  | 146/200 [02:55<01:02,  1.17s/it] 74%|███████▎  | 147/200 [02:56<01:01,  1.17s/it] 74%|███████▍  | 148/200 [02:57<01:00,  1.17s/it] 74%|███████▍  | 149/200 [02:59<00:59,  1.16s/it] 75%|███████▌  | 150/200 [03:00<00:58,  1.17s/it] 76%|███████▌  | 151/200 [03:01<00:57,  1.17s/it] 76%|███████▌  | 152/200 [03:02<00:57,  1.20s/it] 76%|███████▋  | 153/200 [03:03<00:55,  1.19s/it] 77%|███████▋  | 154/200 [03:05<00:54,  1.18s/it] 78%|███████▊  | 155/200 [03:06<00:52,  1.18s/it] 78%|███████▊  | 156/200 [03:07<00:51,  1.17s/it] 78%|███████▊  | 157/200 [03:08<00:50,  1.17s/it] 79%|███████▉  | 158/200 [03:09<00:49,  1.17s/it] 80%|███████▉  | 159/200 [03:10<00:47,  1.17s/it] 80%|████████  | 160/200 [03:12<00:46,  1.17s/it] 80%|████████  | 161/200 [03:13<00:45,  1.17s/it] 81%|████████  | 162/200 [03:14<00:44,  1.17s/it] 82%|████████▏ | 163/200 [03:15<00:43,  1.16s/it] 82%|████████▏ | 164/200 [03:16<00:41,  1.16s/it] 82%|████████▎ | 165/200 [03:17<00:40,  1.17s/it] 83%|████████▎ | 166/200 [03:19<00:39,  1.16s/it] 84%|████████▎ | 167/200 [03:20<00:39,  1.19s/it] 84%|████████▍ | 168/200 [03:21<00:37,  1.18s/it] 84%|████████▍ | 169/200 [03:22<00:36,  1.18s/it] 85%|████████▌ | 170/200 [03:23<00:35,  1.17s/it] 86%|████████▌ | 171/200 [03:24<00:33,  1.17s/it] 86%|████████▌ | 172/200 [03:26<00:32,  1.17s/it] 86%|████████▋ | 173/200 [03:27<00:31,  1.17s/it] 87%|████████▋ | 174/200 [03:28<00:30,  1.17s/it] 88%|████████▊ | 175/200 [03:29<00:29,  1.17s/it] 88%|████████▊ | 176/200 [03:30<00:27,  1.17s/it] 88%|████████▊ | 177/200 [03:31<00:26,  1.17s/it] 89%|████████▉ | 178/200 [03:33<00:25,  1.17s/it] 90%|████████▉ | 179/200 [03:34<00:24,  1.16s/it] 90%|█████████ | 180/200 [03:35<00:23,  1.20s/it] 90%|█████████ | 181/200 [03:36<00:22,  1.19s/it] 91%|█████████ | 182/200 [03:37<00:21,  1.18s/it] 92%|█████████▏| 183/200 [03:39<00:19,  1.18s/it] 92%|█████████▏| 184/200 [03:40<00:18,  1.17s/it] 92%|█████████▎| 185/200 [03:41<00:17,  1.17s/it] 93%|█████████▎| 186/200 [03:42<00:16,  1.17s/it] 94%|█████████▎| 187/200 [03:43<00:15,  1.17s/it] 94%|█████████▍| 188/200 [03:44<00:13,  1.17s/it] 94%|█████████▍| 189/200 [03:46<00:12,  1.17s/it] 95%|█████████▌| 190/200 [03:47<00:11,  1.17s/it] 96%|█████████▌| 191/200 [03:48<00:10,  1.17s/it] 96%|█████████▌| 192/200 [03:49<00:09,  1.16s/it] 96%|█████████▋| 193/200 [03:50<00:08,  1.16s/it] 97%|█████████▋| 194/200 [03:51<00:06,  1.16s/it] 98%|█████████▊| 195/200 [03:53<00:05,  1.16s/it] 98%|█████████▊| 196/200 [03:54<00:04,  1.16s/it] 98%|█████████▊| 197/200 [03:55<00:03,  1.16s/it] 99%|█████████▉| 198/200 [03:56<00:02,  1.19s/it]100%|█████████▉| 199/200 [03:57<00:01,  1.19s/it]100%|██████████| 200/200 [03:58<00:00,  1.18s/it][INFO|trainer.py:3846] 2024-11-26 09:26:44,034 >> Saving model checkpoint to /tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1/checkpoint-200
[INFO|configuration_utils.py:416] 2024-11-26 09:26:44,035 >> Configuration saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1/checkpoint-200/config.json
[INFO|configuration_utils.py:873] 2024-11-26 09:26:44,035 >> Configuration saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-26 09:26:45,857 >> Model weights saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1/checkpoint-200/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:26:45,858 >> tokenizer config file saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:26:45,858 >> Special tokens file saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-26 09:26:45,904 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [04:00<00:00,  1.18s/it]100%|██████████| 200/200 [04:00<00:00,  1.20s/it]
[INFO|trainer.py:3846] 2024-11-26 09:26:45,906 >> Saving model checkpoint to /tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1
[INFO|configuration_utils.py:416] 2024-11-26 09:26:45,907 >> Configuration saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:873] 2024-11-26 09:26:45,907 >> Configuration saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-26 09:26:47,759 >> Model weights saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:26:47,760 >> tokenizer config file saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:26:47,760 >> Special tokens file saved in /tmp/test-clm2-gpt2-medium-lagembed-wikitext-103-raw-v1/special_tokens_map.json
{'train_runtime': 240.8305, 'train_samples_per_second': 26.575, 'train_steps_per_second': 0.83, 'train_loss': 2.7181851196289064, 'epoch': 0.06}
***** train metrics *****
  epoch                    =      0.056
  total_flos               = 11100497GF
  train_loss               =     2.7182
  train_runtime            = 0:04:00.83
  train_samples            =     114248
  train_samples_per_second =     26.575
  train_steps_per_second   =       0.83
11/26/2024 09:26:47 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-26 09:26:47,797 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-26 09:26:47,797 >>   Num examples = 240
[INFO|trainer.py:4167] 2024-11-26 09:26:47,797 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:01<00:03,  1.92it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.36it/s] 50%|█████     | 4/8 [00:03<00:03,  1.17it/s] 62%|██████▎   | 5/8 [00:04<00:02,  1.09it/s] 75%|███████▌  | 6/8 [00:05<00:01,  1.04it/s] 88%|████████▊ | 7/8 [00:06<00:00,  1.02it/s]100%|██████████| 8/8 [00:06<00:00,  1.17it/s]100%|██████████| 8/8 [00:07<00:00,  1.10it/s]
***** eval metrics *****
  epoch                   =      0.056
  eval_accuracy           =     0.4574
  eval_loss               =     2.7643
  eval_runtime            = 0:00:08.36
  eval_samples            =        240
  eval_samples_per_second =     28.696
  eval_steps_per_second   =      0.957
  perplexity              =    15.8676
------------------------------------
LoRA: Fine-tuning openai-community/gpt2-large on wikitext (wikitext-2-raw-v1) using baseline model
11/26/2024 09:27:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/26/2024 09:27:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm2-gpt2-large-lora-wikitext-2-raw-v1/runs/Nov26_09-27-00_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm2-gpt2-large-lora-wikitext-2-raw-v1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm2-gpt2-large-lora-wikitext-2-raw-v1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
11/26/2024 09:27:00 - WARNING - datasets.load - Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 09:26:37 2024).
11/26/2024 09:27:00 - WARNING - datasets.packaged_modules.cache.cache - Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 09:26:37 2024).
Using custom data configuration wikitext-2-raw-v1
11/26/2024 09:27:00 - INFO - datasets.builder - Using custom data configuration wikitext-2-raw-v1
Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
11/26/2024 09:27:00 - INFO - datasets.info - Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
Overwrite dataset info from restored data version if exists.
11/26/2024 09:27:00 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/26/2024 09:27:00 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-26 09:27:00,870 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:27:00,872 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:27:00,890 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:27:00,890 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:27:00,890 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:27:00,890 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:27:00,890 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:27:00,890 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-26 09:27:00,994 >> loading weights file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-26 09:27:01,001 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-26 09:27:01,096 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-26 09:27:01,096 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-26 09:27:01,098 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-26 09:27:01,098 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-dd506bc349a7577a.arrow
11/26/2024 09:27:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-dd506bc349a7577a.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-f42e72b3caeab712.arrow
11/26/2024 09:27:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-f42e72b3caeab712.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-f41d22461690d257.arrow
11/26/2024 09:27:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-f41d22461690d257.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5d0834e9f140e5c7.arrow
11/26/2024 09:27:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5d0834e9f140e5c7.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-404d790b642c04e2.arrow
11/26/2024 09:27:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-404d790b642c04e2.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-bf9729f8ad2b0490.arrow
11/26/2024 09:27:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-bf9729f8ad2b0490.arrow
11/26/2024 09:27:02 - WARNING - evaluate.loading - Using the latest cached version of the module from /home/cs/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Nov 18 09:05:53 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
[WARNING|trainer.py:664] 2024-11-26 09:27:04,291 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-26 09:27:04,416 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-26 09:27:04,416 >>   Num examples = 2,318
[INFO|trainer.py:2337] 2024-11-26 09:27:04,416 >>   Num Epochs = 2
[INFO|trainer.py:2338] 2024-11-26 09:27:04,416 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-26 09:27:04,416 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-26 09:27:04,416 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-26 09:27:04,416 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-26 09:27:04,416 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-26 09:27:04,417 >>   Number of trainable parameters = 1,474,560
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:06<21:29,  6.48s/it]  1%|          | 2/200 [00:08<12:30,  3.79s/it]  2%|▏         | 3/200 [00:10<09:34,  2.92s/it]  2%|▏         | 4/200 [00:12<08:19,  2.55s/it]  2%|▎         | 5/200 [00:14<07:30,  2.31s/it]  3%|▎         | 6/200 [00:16<07:00,  2.17s/it]  4%|▎         | 7/200 [00:17<06:41,  2.08s/it]  4%|▍         | 8/200 [00:19<06:28,  2.02s/it]  4%|▍         | 9/200 [00:21<06:23,  2.01s/it]  5%|▌         | 10/200 [00:23<06:15,  1.98s/it]  6%|▌         | 11/200 [00:25<06:09,  1.96s/it]  6%|▌         | 12/200 [00:27<06:04,  1.94s/it]  6%|▋         | 13/200 [00:29<06:00,  1.93s/it]  7%|▋         | 14/200 [00:31<06:01,  1.95s/it]  8%|▊         | 15/200 [00:33<05:57,  1.93s/it]  8%|▊         | 16/200 [00:35<05:54,  1.93s/it]  8%|▊         | 17/200 [00:37<05:51,  1.92s/it]  9%|▉         | 18/200 [00:39<05:52,  1.94s/it] 10%|▉         | 19/200 [00:41<05:49,  1.93s/it] 10%|█         | 20/200 [00:43<06:00,  2.00s/it] 10%|█         | 21/200 [00:45<06:17,  2.11s/it] 11%|█         | 22/200 [00:47<06:30,  2.19s/it] 12%|█▏        | 23/200 [00:50<06:35,  2.24s/it] 12%|█▏        | 24/200 [00:52<06:41,  2.28s/it] 12%|█▎        | 25/200 [00:55<06:45,  2.32s/it] 13%|█▎        | 26/200 [00:57<06:45,  2.33s/it] 14%|█▎        | 27/200 [00:59<06:47,  2.36s/it] 14%|█▍        | 28/200 [01:02<06:43,  2.34s/it] 14%|█▍        | 29/200 [01:04<06:43,  2.36s/it] 15%|█▌        | 30/200 [01:06<06:42,  2.37s/it] 16%|█▌        | 31/200 [01:09<06:40,  2.37s/it] 16%|█▌        | 32/200 [01:11<06:35,  2.36s/it] 16%|█▋        | 33/200 [01:14<06:35,  2.37s/it] 17%|█▋        | 34/200 [01:16<06:41,  2.42s/it] 18%|█▊        | 35/200 [01:19<06:44,  2.45s/it] 18%|█▊        | 36/200 [01:21<06:53,  2.52s/it] 18%|█▊        | 37/200 [01:24<06:47,  2.50s/it] 19%|█▉        | 38/200 [01:26<06:42,  2.48s/it] 20%|█▉        | 39/200 [01:29<06:38,  2.48s/it] 20%|██        | 40/200 [01:31<06:34,  2.47s/it] 20%|██        | 41/200 [01:33<06:28,  2.44s/it] 21%|██        | 42/200 [01:36<06:26,  2.44s/it] 22%|██▏       | 43/200 [01:38<06:29,  2.48s/it] 22%|██▏       | 44/200 [01:41<06:23,  2.46s/it] 22%|██▎       | 45/200 [01:43<06:22,  2.47s/it] 23%|██▎       | 46/200 [01:46<06:23,  2.49s/it] 24%|██▎       | 47/200 [01:48<06:23,  2.51s/it] 24%|██▍       | 48/200 [01:51<06:18,  2.49s/it] 24%|██▍       | 49/200 [01:53<06:14,  2.48s/it] 25%|██▌       | 50/200 [01:56<06:07,  2.45s/it] 26%|██▌       | 51/200 [01:58<06:04,  2.45s/it] 26%|██▌       | 52/200 [02:01<06:03,  2.46s/it] 26%|██▋       | 53/200 [02:03<06:00,  2.46s/it] 27%|██▋       | 54/200 [02:06<05:58,  2.45s/it] 28%|██▊       | 55/200 [02:08<05:52,  2.43s/it] 28%|██▊       | 56/200 [02:10<05:50,  2.43s/it] 28%|██▊       | 57/200 [02:13<05:34,  2.34s/it] 29%|██▉       | 58/200 [02:14<05:16,  2.23s/it] 30%|██▉       | 59/200 [02:17<05:05,  2.16s/it] 30%|███       | 60/200 [02:18<04:54,  2.10s/it] 30%|███       | 61/200 [02:20<04:45,  2.06s/it] 31%|███       | 62/200 [02:22<04:39,  2.03s/it] 32%|███▏      | 63/200 [02:24<04:34,  2.00s/it] 32%|███▏      | 64/200 [02:27<04:46,  2.10s/it] 32%|███▎      | 65/200 [02:29<04:58,  2.21s/it] 33%|███▎      | 66/200 [02:32<05:05,  2.28s/it] 34%|███▎      | 67/200 [02:34<05:08,  2.32s/it] 34%|███▍      | 68/200 [02:36<05:11,  2.36s/it] 34%|███▍      | 69/200 [02:39<05:12,  2.39s/it] 35%|███▌      | 70/200 [02:41<05:13,  2.41s/it] 36%|███▌      | 71/200 [02:44<05:12,  2.42s/it] 36%|███▌      | 72/200 [02:46<05:11,  2.43s/it] 36%|███▋      | 73/200 [02:49<05:06,  2.42s/it] 37%|███▋      | 74/200 [02:51<05:05,  2.43s/it] 38%|███▊      | 75/200 [02:54<05:04,  2.43s/it] 38%|███▊      | 76/200 [02:56<05:02,  2.44s/it] 38%|███▊      | 77/200 [02:58<04:57,  2.42s/it] 39%|███▉      | 78/200 [03:01<04:56,  2.43s/it] 40%|███▉      | 79/200 [03:03<04:54,  2.44s/it] 40%|████      | 80/200 [03:06<04:53,  2.44s/it] 40%|████      | 81/200 [03:08<04:51,  2.45s/it] 41%|████      | 82/200 [03:11<04:48,  2.45s/it] 42%|████▏     | 83/200 [03:13<04:46,  2.45s/it] 42%|████▏     | 84/200 [03:16<04:43,  2.45s/it] 42%|████▎     | 85/200 [03:18<04:41,  2.45s/it] 43%|████▎     | 86/200 [03:20<04:36,  2.42s/it] 44%|████▎     | 87/200 [03:23<04:35,  2.43s/it] 44%|████▍     | 88/200 [03:25<04:33,  2.44s/it] 44%|████▍     | 89/200 [03:28<04:31,  2.44s/it] 45%|████▌     | 90/200 [03:30<04:26,  2.43s/it] 46%|████▌     | 91/200 [03:33<04:24,  2.43s/it] 46%|████▌     | 92/200 [03:35<04:23,  2.44s/it] 46%|████▋     | 93/200 [03:37<04:21,  2.44s/it] 47%|████▋     | 94/200 [03:40<04:16,  2.42s/it] 48%|████▊     | 95/200 [03:42<04:14,  2.43s/it] 48%|████▊     | 96/200 [03:45<04:13,  2.44s/it] 48%|████▊     | 97/200 [03:47<04:11,  2.44s/it] 49%|████▉     | 98/200 [03:50<04:09,  2.45s/it] 50%|████▉     | 99/200 [03:52<04:04,  2.42s/it] 50%|█████     | 100/200 [03:54<04:03,  2.43s/it] 50%|█████     | 101/200 [03:57<04:01,  2.44s/it] 51%|█████     | 102/200 [03:59<03:59,  2.44s/it] 52%|█████▏    | 103/200 [04:02<03:54,  2.42s/it] 52%|█████▏    | 104/200 [04:04<03:53,  2.43s/it] 52%|█████▎    | 105/200 [04:07<03:51,  2.44s/it] 53%|█████▎    | 106/200 [04:09<03:49,  2.44s/it] 54%|█████▎    | 107/200 [04:12<03:47,  2.44s/it] 54%|█████▍    | 108/200 [04:14<03:42,  2.42s/it] 55%|█████▍    | 109/200 [04:16<03:41,  2.43s/it] 55%|█████▌    | 110/200 [04:19<03:39,  2.44s/it] 56%|█████▌    | 111/200 [04:21<03:36,  2.44s/it] 56%|█████▌    | 112/200 [04:24<03:32,  2.42s/it] 56%|█████▋    | 113/200 [04:26<03:31,  2.43s/it] 57%|█████▋    | 114/200 [04:29<03:29,  2.44s/it] 57%|█████▊    | 115/200 [04:31<03:27,  2.44s/it] 58%|█████▊    | 116/200 [04:33<03:24,  2.43s/it] 58%|█████▊    | 117/200 [04:36<03:20,  2.41s/it] 59%|█████▉    | 118/200 [04:38<03:18,  2.42s/it] 60%|█████▉    | 119/200 [04:41<03:16,  2.43s/it] 60%|██████    | 120/200 [04:43<03:14,  2.43s/it] 60%|██████    | 121/200 [04:46<03:12,  2.44s/it] 61%|██████    | 122/200 [04:48<03:09,  2.42s/it] 62%|██████▏   | 123/200 [04:50<03:07,  2.43s/it] 62%|██████▏   | 124/200 [04:53<03:05,  2.44s/it] 62%|██████▎   | 125/200 [04:55<03:03,  2.44s/it] 63%|██████▎   | 126/200 [04:58<02:59,  2.42s/it] 64%|██████▎   | 127/200 [05:00<02:48,  2.31s/it] 64%|██████▍   | 128/200 [05:02<02:38,  2.21s/it] 64%|██████▍   | 129/200 [05:04<02:31,  2.13s/it] 65%|██████▌   | 130/200 [05:06<02:34,  2.20s/it] 66%|██████▌   | 131/200 [05:08<02:37,  2.28s/it] 66%|██████▌   | 132/200 [05:11<02:38,  2.33s/it] 66%|██████▋   | 133/200 [05:13<02:38,  2.36s/it] 67%|██████▋   | 134/200 [05:16<02:37,  2.39s/it] 68%|██████▊   | 135/200 [05:18<02:34,  2.38s/it] 68%|██████▊   | 136/200 [05:21<02:33,  2.40s/it] 68%|██████▊   | 137/200 [05:23<02:32,  2.42s/it] 69%|██████▉   | 138/200 [05:26<02:30,  2.43s/it] 70%|██████▉   | 139/200 [05:28<02:28,  2.44s/it] 70%|███████   | 140/200 [05:30<02:25,  2.42s/it] 70%|███████   | 141/200 [05:33<02:22,  2.42s/it] 71%|███████   | 142/200 [05:35<02:21,  2.43s/it] 72%|███████▏  | 143/200 [05:38<02:19,  2.44s/it] 72%|███████▏  | 144/200 [05:40<02:15,  2.42s/it] 72%|███████▎  | 145/200 [05:42<02:12,  2.41s/it] 73%|███████▎  | 146/200 [05:45<02:10,  2.42s/it] 74%|███████▎  | 147/200 [05:47<02:08,  2.43s/it] 74%|███████▍  | 148/200 [05:50<02:06,  2.44s/it] 74%|███████▍  | 149/200 [05:52<02:03,  2.42s/it] 75%|███████▌  | 150/200 [05:55<02:01,  2.43s/it] 76%|███████▌  | 151/200 [05:57<01:53,  2.31s/it] 76%|███████▌  | 152/200 [05:59<01:45,  2.20s/it] 76%|███████▋  | 153/200 [06:01<01:40,  2.15s/it] 77%|███████▋  | 154/200 [06:03<01:36,  2.09s/it] 78%|███████▊  | 155/200 [06:05<01:36,  2.15s/it] 78%|███████▊  | 156/200 [06:07<01:38,  2.24s/it] 78%|███████▊  | 157/200 [06:10<01:38,  2.30s/it] 79%|███████▉  | 158/200 [06:12<01:37,  2.33s/it] 80%|███████▉  | 159/200 [06:15<01:36,  2.37s/it] 80%|████████  | 160/200 [06:17<01:35,  2.39s/it] 80%|████████  | 161/200 [06:20<01:33,  2.41s/it] 81%|████████  | 162/200 [06:22<01:32,  2.42s/it] 82%|████████▏ | 163/200 [06:24<01:29,  2.43s/it] 82%|████████▏ | 164/200 [06:27<01:27,  2.44s/it] 82%|████████▎ | 165/200 [06:29<01:25,  2.44s/it] 83%|████████▎ | 166/200 [06:32<01:23,  2.44s/it] 84%|████████▎ | 167/200 [06:34<01:19,  2.42s/it] 84%|████████▍ | 168/200 [06:37<01:17,  2.43s/it] 84%|████████▍ | 169/200 [06:39<01:15,  2.43s/it] 85%|████████▌ | 170/200 [06:41<01:13,  2.44s/it] 86%|████████▌ | 171/200 [06:44<01:10,  2.42s/it] 86%|████████▌ | 172/200 [06:46<01:08,  2.43s/it] 86%|████████▋ | 173/200 [06:49<01:05,  2.43s/it] 87%|████████▋ | 174/200 [06:51<01:03,  2.44s/it] 88%|████████▊ | 175/200 [06:54<01:01,  2.44s/it] 88%|████████▊ | 176/200 [06:56<00:58,  2.45s/it] 88%|████████▊ | 177/200 [06:59<00:56,  2.45s/it] 89%|████████▉ | 178/200 [07:01<00:53,  2.44s/it] 90%|████████▉ | 179/200 [07:03<00:51,  2.44s/it] 90%|█████████ | 180/200 [07:06<00:48,  2.42s/it] 90%|█████████ | 181/200 [07:08<00:46,  2.43s/it] 91%|█████████ | 182/200 [07:11<00:43,  2.44s/it] 92%|█████████▏| 183/200 [07:13<00:41,  2.44s/it] 92%|█████████▏| 184/200 [07:16<00:38,  2.42s/it] 92%|█████████▎| 185/200 [07:18<00:36,  2.43s/it] 93%|█████████▎| 186/200 [07:20<00:34,  2.43s/it] 94%|█████████▎| 187/200 [07:23<00:31,  2.44s/it] 94%|█████████▍| 188/200 [07:25<00:29,  2.42s/it] 94%|█████████▍| 189/200 [07:28<00:26,  2.43s/it] 95%|█████████▌| 190/200 [07:30<00:24,  2.44s/it] 96%|█████████▌| 191/200 [07:33<00:21,  2.44s/it] 96%|█████████▌| 192/200 [07:35<00:19,  2.44s/it] 96%|█████████▋| 193/200 [07:37<00:16,  2.42s/it] 97%|█████████▋| 194/200 [07:40<00:14,  2.42s/it] 98%|█████████▊| 195/200 [07:42<00:12,  2.44s/it] 98%|█████████▊| 196/200 [07:45<00:09,  2.44s/it] 98%|█████████▊| 197/200 [07:47<00:07,  2.42s/it] 99%|█████████▉| 198/200 [07:50<00:04,  2.43s/it]100%|█████████▉| 199/200 [07:52<00:02,  2.43s/it]100%|██████████| 200/200 [07:54<00:00,  2.44s/it][INFO|trainer.py:3846] 2024-11-26 09:34:59,405 >> Saving model checkpoint to /tmp/test-clm2-gpt2-large-lora-wikitext-2-raw-v1/checkpoint-200
[INFO|configuration_utils.py:690] 2024-11-26 09:34:59,419 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:34:59,419 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-large",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:34:59,433 >> tokenizer config file saved in /tmp/test-clm2-gpt2-large-lora-wikitext-2-raw-v1/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:34:59,433 >> Special tokens file saved in /tmp/test-clm2-gpt2-large-lora-wikitext-2-raw-v1/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-26 09:34:59,504 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [07:55<00:00,  2.44s/it]100%|██████████| 200/200 [07:55<00:00,  2.38s/it]
[INFO|trainer.py:3846] 2024-11-26 09:34:59,505 >> Saving model checkpoint to /tmp/test-clm2-gpt2-large-lora-wikitext-2-raw-v1
[INFO|configuration_utils.py:690] 2024-11-26 09:34:59,517 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:34:59,518 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-large",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:34:59,529 >> tokenizer config file saved in /tmp/test-clm2-gpt2-large-lora-wikitext-2-raw-v1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:34:59,529 >> Special tokens file saved in /tmp/test-clm2-gpt2-large-lora-wikitext-2-raw-v1/special_tokens_map.json
{'train_runtime': 475.0867, 'train_samples_per_second': 6.736, 'train_steps_per_second': 0.421, 'train_loss': 2.4977099609375, 'epoch': 1.38}
***** train metrics *****
  epoch                    =     1.3793
  total_flos               = 12989891GF
  train_loss               =     2.4977
  train_runtime            = 0:07:55.08
  train_samples            =       2318
  train_samples_per_second =      6.736
  train_steps_per_second   =      0.421
11/26/2024 09:34:59 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-26 09:34:59,579 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-26 09:34:59,579 >>   Num examples = 240
[INFO|trainer.py:4167] 2024-11-26 09:34:59,579 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:02<00:06,  1.13s/it] 38%|███▊      | 3/8 [00:04<00:08,  1.61s/it] 50%|█████     | 4/8 [00:06<00:07,  1.87s/it] 62%|██████▎   | 5/8 [00:09<00:06,  2.01s/it] 75%|███████▌  | 6/8 [00:11<00:04,  2.07s/it] 88%|████████▊ | 7/8 [00:13<00:02,  2.13s/it]100%|██████████| 8/8 [00:14<00:00,  1.90s/it]100%|██████████| 8/8 [00:15<00:00,  1.93s/it]
***** eval metrics *****
  epoch                   =     1.3793
  eval_accuracy           =      0.472
  eval_loss               =     2.6388
  eval_runtime            = 0:00:18.22
  eval_samples            =        240
  eval_samples_per_second =     13.172
  eval_steps_per_second   =      0.439
  perplexity              =    13.9966
------------------------------------
LagEmbed: Training openai-community/gpt2-large on wikitext (wikitext-2-raw-v1) with LagEmbed (in_channels=1280, n_components=3, dof=8)
11/26/2024 09:35:23 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/26/2024 09:35:23 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1/runs/Nov26_09-35-22_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
11/26/2024 09:35:23 - WARNING - datasets.load - Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 09:26:37 2024).
11/26/2024 09:35:23 - WARNING - datasets.packaged_modules.cache.cache - Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 09:26:37 2024).
Using custom data configuration wikitext-2-raw-v1
11/26/2024 09:35:23 - INFO - datasets.builder - Using custom data configuration wikitext-2-raw-v1
Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
11/26/2024 09:35:23 - INFO - datasets.info - Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
Overwrite dataset info from restored data version if exists.
11/26/2024 09:35:23 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/26/2024 09:35:23 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-26 09:35:23,138 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:35:23,138 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:35:23,145 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:35:23,145 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:35:23,145 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:35:23,145 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:35:23,145 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:35:23,145 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-26 09:35:23,243 >> loading weights file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-26 09:35:23,251 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-26 09:35:23,349 >> All model checkpoint weights were used when initializing LagGPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-26 09:35:23,349 >> All the weights of LagGPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LagGPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-26 09:35:23,351 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-26 09:35:23,351 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-dd506bc349a7577a.arrow
11/26/2024 09:35:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-dd506bc349a7577a.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-f42e72b3caeab712.arrow
11/26/2024 09:35:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-f42e72b3caeab712.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-f41d22461690d257.arrow
11/26/2024 09:35:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-f41d22461690d257.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5d0834e9f140e5c7.arrow
11/26/2024 09:35:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5d0834e9f140e5c7.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-404d790b642c04e2.arrow
11/26/2024 09:35:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-404d790b642c04e2.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-bf9729f8ad2b0490.arrow
11/26/2024 09:35:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-bf9729f8ad2b0490.arrow
11/26/2024 09:35:24 - WARNING - evaluate.loading - Using the latest cached version of the module from /home/cs/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Nov 18 09:05:53 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
[WARNING|trainer.py:664] 2024-11-26 09:35:26,601 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-26 09:35:26,732 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-26 09:35:26,732 >>   Num examples = 2,318
[INFO|trainer.py:2337] 2024-11-26 09:35:26,732 >>   Num Epochs = 2
[INFO|trainer.py:2338] 2024-11-26 09:35:26,732 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-26 09:35:26,732 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-26 09:35:26,732 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-26 09:35:26,732 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-26 09:35:26,732 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-26 09:35:26,733 >>   Number of trainable parameters = 1,210,011
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:05<19:43,  5.94s/it]  1%|          | 2/200 [00:07<10:32,  3.19s/it]  2%|▏         | 3/200 [00:08<07:35,  2.31s/it]  2%|▏         | 4/200 [00:09<06:12,  1.90s/it]  2%|▎         | 5/200 [00:11<05:25,  1.67s/it]  3%|▎         | 6/200 [00:12<04:57,  1.53s/it]  4%|▎         | 7/200 [00:13<04:38,  1.44s/it]  4%|▍         | 8/200 [00:14<04:26,  1.39s/it]  4%|▍         | 9/200 [00:16<04:20,  1.37s/it]  5%|▌         | 10/200 [00:17<04:14,  1.34s/it]  6%|▌         | 11/200 [00:18<04:08,  1.31s/it]  6%|▌         | 12/200 [00:19<04:04,  1.30s/it]  6%|▋         | 13/200 [00:21<04:01,  1.29s/it]  7%|▋         | 14/200 [00:22<03:58,  1.28s/it]  8%|▊         | 15/200 [00:23<03:56,  1.28s/it]  8%|▊         | 16/200 [00:24<03:54,  1.27s/it]  8%|▊         | 17/200 [00:26<03:52,  1.27s/it]  9%|▉         | 18/200 [00:27<03:54,  1.29s/it] 10%|▉         | 19/200 [00:28<03:52,  1.28s/it] 10%|█         | 20/200 [00:30<03:49,  1.28s/it] 10%|█         | 21/200 [00:31<03:47,  1.27s/it] 11%|█         | 22/200 [00:32<03:46,  1.27s/it] 12%|█▏        | 23/200 [00:33<03:45,  1.27s/it] 12%|█▏        | 24/200 [00:35<03:43,  1.27s/it] 12%|█▎        | 25/200 [00:36<03:42,  1.27s/it] 13%|█▎        | 26/200 [00:37<03:43,  1.29s/it] 14%|█▎        | 27/200 [00:39<03:41,  1.28s/it] 14%|█▍        | 28/200 [00:40<03:39,  1.28s/it] 14%|█▍        | 29/200 [00:41<03:37,  1.27s/it] 15%|█▌        | 30/200 [00:42<03:36,  1.27s/it] 16%|█▌        | 31/200 [00:44<03:34,  1.27s/it] 16%|█▌        | 32/200 [00:45<03:33,  1.27s/it] 16%|█▋        | 33/200 [00:46<03:31,  1.27s/it] 17%|█▋        | 34/200 [00:48<03:35,  1.30s/it] 18%|█▊        | 35/200 [00:49<03:32,  1.29s/it] 18%|█▊        | 36/200 [00:50<03:29,  1.28s/it] 18%|█▊        | 37/200 [00:51<03:27,  1.28s/it] 19%|█▉        | 38/200 [00:53<03:26,  1.27s/it] 20%|█▉        | 39/200 [00:54<03:24,  1.27s/it] 20%|██        | 40/200 [00:55<03:23,  1.27s/it] 20%|██        | 41/200 [00:56<03:22,  1.27s/it] 21%|██        | 42/200 [00:58<03:20,  1.27s/it] 22%|██▏       | 43/200 [00:59<03:22,  1.29s/it] 22%|██▏       | 44/200 [01:00<03:20,  1.28s/it] 22%|██▎       | 45/200 [01:02<03:17,  1.28s/it] 23%|██▎       | 46/200 [01:03<03:16,  1.28s/it] 24%|██▎       | 47/200 [01:04<03:14,  1.27s/it] 24%|██▍       | 48/200 [01:05<03:13,  1.27s/it] 24%|██▍       | 49/200 [01:07<03:11,  1.27s/it] 25%|██▌       | 50/200 [01:08<03:10,  1.27s/it] 26%|██▌       | 51/200 [01:09<03:08,  1.27s/it] 26%|██▌       | 52/200 [01:10<03:10,  1.28s/it] 26%|██▋       | 53/200 [01:12<03:08,  1.28s/it] 27%|██▋       | 54/200 [01:13<03:06,  1.28s/it] 28%|██▊       | 55/200 [01:14<03:04,  1.27s/it] 28%|██▊       | 56/200 [01:16<03:03,  1.27s/it] 28%|██▊       | 57/200 [01:17<03:01,  1.27s/it] 29%|██▉       | 58/200 [01:18<03:00,  1.27s/it] 30%|██▉       | 59/200 [01:19<02:58,  1.27s/it] 30%|███       | 60/200 [01:21<02:59,  1.28s/it] 30%|███       | 61/200 [01:22<02:57,  1.28s/it] 31%|███       | 62/200 [01:23<02:56,  1.28s/it] 32%|███▏      | 63/200 [01:24<02:54,  1.27s/it] 32%|███▏      | 64/200 [01:26<02:53,  1.27s/it] 32%|███▎      | 65/200 [01:27<02:51,  1.27s/it] 33%|███▎      | 66/200 [01:28<02:50,  1.27s/it] 34%|███▎      | 67/200 [01:30<02:48,  1.27s/it] 34%|███▍      | 68/200 [01:31<02:47,  1.27s/it] 34%|███▍      | 69/200 [01:32<02:48,  1.29s/it] 35%|███▌      | 70/200 [01:33<02:46,  1.28s/it] 36%|███▌      | 71/200 [01:35<02:44,  1.28s/it] 36%|███▌      | 72/200 [01:36<02:43,  1.28s/it] 36%|███▋      | 73/200 [01:37<02:41,  1.27s/it] 37%|███▋      | 74/200 [01:38<02:40,  1.27s/it] 38%|███▊      | 75/200 [01:40<02:39,  1.27s/it] 38%|███▊      | 76/200 [01:41<02:37,  1.27s/it] 38%|███▊      | 77/200 [01:42<02:36,  1.27s/it] 39%|███▉      | 78/200 [01:44<02:37,  1.29s/it] 40%|███▉      | 79/200 [01:45<02:35,  1.28s/it] 40%|████      | 80/200 [01:46<02:33,  1.28s/it] 40%|████      | 81/200 [01:47<02:32,  1.28s/it] 41%|████      | 82/200 [01:49<02:30,  1.28s/it] 42%|████▏     | 83/200 [01:50<02:29,  1.27s/it] 42%|████▏     | 84/200 [01:51<02:27,  1.27s/it] 42%|████▎     | 85/200 [01:52<02:26,  1.27s/it] 43%|████▎     | 86/200 [01:54<02:26,  1.29s/it] 44%|████▎     | 87/200 [01:55<02:24,  1.28s/it] 44%|████▍     | 88/200 [01:56<02:23,  1.28s/it] 44%|████▍     | 89/200 [01:58<02:21,  1.28s/it] 45%|████▌     | 90/200 [01:59<02:20,  1.27s/it] 46%|████▌     | 91/200 [02:00<02:18,  1.27s/it] 46%|████▌     | 92/200 [02:01<02:17,  1.27s/it] 46%|████▋     | 93/200 [02:03<02:15,  1.27s/it] 47%|████▋     | 94/200 [02:04<02:17,  1.30s/it] 48%|████▊     | 95/200 [02:05<02:15,  1.29s/it] 48%|████▊     | 96/200 [02:07<02:13,  1.28s/it] 48%|████▊     | 97/200 [02:08<02:11,  1.28s/it] 49%|████▉     | 98/200 [02:09<02:10,  1.28s/it] 50%|████▉     | 99/200 [02:10<02:08,  1.27s/it] 50%|█████     | 100/200 [02:12<02:07,  1.27s/it] 50%|█████     | 101/200 [02:13<02:06,  1.27s/it] 51%|█████     | 102/200 [02:14<02:04,  1.27s/it] 52%|█████▏    | 103/200 [02:16<02:05,  1.29s/it] 52%|█████▏    | 104/200 [02:17<02:03,  1.28s/it] 52%|█████▎    | 105/200 [02:18<02:01,  1.28s/it] 53%|█████▎    | 106/200 [02:19<02:00,  1.28s/it] 54%|█████▎    | 107/200 [02:21<01:58,  1.27s/it] 54%|█████▍    | 108/200 [02:22<01:57,  1.27s/it] 55%|█████▍    | 109/200 [02:23<01:55,  1.27s/it] 55%|█████▌    | 110/200 [02:24<01:54,  1.27s/it] 56%|█████▌    | 111/200 [02:26<01:53,  1.27s/it] 56%|█████▌    | 112/200 [02:27<01:53,  1.29s/it] 56%|█████▋    | 113/200 [02:28<01:51,  1.28s/it] 57%|█████▋    | 114/200 [02:30<01:49,  1.28s/it] 57%|█████▊    | 115/200 [02:31<01:48,  1.27s/it] 58%|█████▊    | 116/200 [02:32<01:46,  1.27s/it] 58%|█████▊    | 117/200 [02:33<01:45,  1.27s/it] 59%|█████▉    | 118/200 [02:35<01:44,  1.27s/it] 60%|█████▉    | 119/200 [02:36<01:42,  1.27s/it] 60%|██████    | 120/200 [02:37<01:42,  1.29s/it] 60%|██████    | 121/200 [02:39<01:41,  1.28s/it] 61%|██████    | 122/200 [02:40<01:39,  1.28s/it] 62%|██████▏   | 123/200 [02:41<01:38,  1.28s/it] 62%|██████▏   | 124/200 [02:42<01:36,  1.27s/it] 62%|██████▎   | 125/200 [02:44<01:35,  1.27s/it] 63%|██████▎   | 126/200 [02:45<01:34,  1.27s/it] 64%|██████▎   | 127/200 [02:46<01:32,  1.27s/it] 64%|██████▍   | 128/200 [02:47<01:31,  1.27s/it] 64%|██████▍   | 129/200 [02:49<01:31,  1.29s/it] 65%|██████▌   | 130/200 [02:50<01:29,  1.28s/it] 66%|██████▌   | 131/200 [02:51<01:28,  1.28s/it] 66%|██████▌   | 132/200 [02:53<01:26,  1.27s/it] 66%|██████▋   | 133/200 [02:54<01:25,  1.27s/it] 67%|██████▋   | 134/200 [02:55<01:23,  1.27s/it] 68%|██████▊   | 135/200 [02:56<01:22,  1.27s/it] 68%|██████▊   | 136/200 [02:58<01:21,  1.27s/it] 68%|██████▊   | 137/200 [02:59<01:19,  1.27s/it] 69%|██████▉   | 138/200 [03:00<01:19,  1.29s/it] 70%|██████▉   | 139/200 [03:01<01:18,  1.28s/it] 70%|███████   | 140/200 [03:03<01:16,  1.28s/it] 70%|███████   | 141/200 [03:04<01:15,  1.27s/it] 71%|███████   | 142/200 [03:05<01:13,  1.27s/it] 72%|███████▏  | 143/200 [03:07<01:12,  1.27s/it] 72%|███████▏  | 144/200 [03:08<01:11,  1.27s/it] 72%|███████▎  | 145/200 [03:09<01:08,  1.25s/it] 73%|███████▎  | 146/200 [03:10<01:09,  1.28s/it] 74%|███████▎  | 147/200 [03:12<01:07,  1.28s/it] 74%|███████▍  | 148/200 [03:13<01:06,  1.27s/it] 74%|███████▍  | 149/200 [03:14<01:04,  1.27s/it] 75%|███████▌  | 150/200 [03:15<01:03,  1.27s/it] 76%|███████▌  | 151/200 [03:17<01:02,  1.27s/it] 76%|███████▌  | 152/200 [03:18<01:01,  1.27s/it] 76%|███████▋  | 153/200 [03:19<00:59,  1.27s/it] 77%|███████▋  | 154/200 [03:21<00:59,  1.30s/it] 78%|███████▊  | 155/200 [03:22<00:58,  1.29s/it] 78%|███████▊  | 156/200 [03:23<00:56,  1.29s/it] 78%|███████▊  | 157/200 [03:24<00:55,  1.28s/it] 79%|███████▉  | 158/200 [03:26<00:53,  1.28s/it] 80%|███████▉  | 159/200 [03:27<00:52,  1.28s/it] 80%|████████  | 160/200 [03:28<00:50,  1.27s/it] 80%|████████  | 161/200 [03:30<00:49,  1.27s/it] 81%|████████  | 162/200 [03:31<00:48,  1.27s/it] 82%|████████▏ | 163/200 [03:32<00:47,  1.29s/it] 82%|████████▏ | 164/200 [03:33<00:46,  1.28s/it] 82%|████████▎ | 165/200 [03:35<00:44,  1.28s/it] 83%|████████▎ | 166/200 [03:36<00:43,  1.28s/it] 84%|████████▎ | 167/200 [03:37<00:42,  1.27s/it] 84%|████████▍ | 168/200 [03:38<00:40,  1.27s/it] 84%|████████▍ | 169/200 [03:40<00:39,  1.27s/it] 85%|████████▌ | 170/200 [03:41<00:38,  1.27s/it] 86%|████████▌ | 171/200 [03:42<00:36,  1.27s/it] 86%|████████▌ | 172/200 [03:44<00:36,  1.29s/it] 86%|████████▋ | 173/200 [03:45<00:34,  1.28s/it] 87%|████████▋ | 174/200 [03:46<00:33,  1.28s/it] 88%|████████▊ | 175/200 [03:47<00:31,  1.28s/it] 88%|████████▊ | 176/200 [03:49<00:30,  1.27s/it] 88%|████████▊ | 177/200 [03:50<00:29,  1.27s/it] 89%|████████▉ | 178/200 [03:51<00:27,  1.27s/it] 90%|████████▉ | 179/200 [03:52<00:26,  1.27s/it] 90%|█████████ | 180/200 [03:54<00:25,  1.29s/it] 90%|█████████ | 181/200 [03:55<00:24,  1.28s/it] 91%|█████████ | 182/200 [03:56<00:22,  1.28s/it] 92%|█████████▏| 183/200 [03:58<00:21,  1.27s/it] 92%|█████████▏| 184/200 [03:59<00:20,  1.27s/it] 92%|█████████▎| 185/200 [04:00<00:19,  1.27s/it] 93%|█████████▎| 186/200 [04:01<00:17,  1.27s/it] 94%|█████████▎| 187/200 [04:03<00:16,  1.27s/it] 94%|█████████▍| 188/200 [04:04<00:15,  1.27s/it] 94%|█████████▍| 189/200 [04:05<00:14,  1.29s/it] 95%|█████████▌| 190/200 [04:07<00:12,  1.28s/it] 96%|█████████▌| 191/200 [04:08<00:11,  1.28s/it] 96%|█████████▌| 192/200 [04:09<00:10,  1.27s/it] 96%|█████████▋| 193/200 [04:10<00:08,  1.27s/it] 97%|█████████▋| 194/200 [04:12<00:07,  1.27s/it] 98%|█████████▊| 195/200 [04:13<00:06,  1.27s/it] 98%|█████████▊| 196/200 [04:14<00:05,  1.27s/it] 98%|█████████▊| 197/200 [04:15<00:03,  1.27s/it] 99%|█████████▉| 198/200 [04:17<00:02,  1.29s/it]100%|█████████▉| 199/200 [04:18<00:01,  1.28s/it]100%|██████████| 200/200 [04:19<00:00,  1.28s/it][INFO|trainer.py:3846] 2024-11-26 09:39:46,542 >> Saving model checkpoint to /tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1/checkpoint-200
[INFO|configuration_utils.py:416] 2024-11-26 09:39:46,543 >> Configuration saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1/checkpoint-200/config.json
[INFO|configuration_utils.py:873] 2024-11-26 09:39:46,543 >> Configuration saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-26 09:39:50,253 >> Model weights saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1/checkpoint-200/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:39:50,254 >> tokenizer config file saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:39:50,254 >> Special tokens file saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-26 09:39:50,302 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [04:23<00:00,  1.28s/it]100%|██████████| 200/200 [04:23<00:00,  1.32s/it]
[INFO|trainer.py:3846] 2024-11-26 09:39:50,303 >> Saving model checkpoint to /tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1
[INFO|configuration_utils.py:416] 2024-11-26 09:39:50,304 >> Configuration saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:873] 2024-11-26 09:39:50,304 >> Configuration saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-26 09:39:54,109 >> Model weights saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:39:54,110 >> tokenizer config file saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:39:54,110 >> Special tokens file saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-2-raw-v1/special_tokens_map.json
{'train_runtime': 263.5689, 'train_samples_per_second': 12.141, 'train_steps_per_second': 0.759, 'train_loss': 2.4185643005371094, 'epoch': 1.38}
***** train metrics *****
  epoch                    =     1.3793
  total_flos               = 12985050GF
  train_loss               =     2.4186
  train_runtime            = 0:04:23.56
  train_samples            =       2318
  train_samples_per_second =     12.141
  train_steps_per_second   =      0.759
11/26/2024 09:39:54 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-26 09:39:54,145 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-26 09:39:54,145 >>   Num examples = 240
[INFO|trainer.py:4167] 2024-11-26 09:39:54,145 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:01<00:05,  1.08it/s] 38%|███▊      | 3/8 [00:03<00:06,  1.31s/it] 50%|█████     | 4/8 [00:05<00:06,  1.51s/it] 62%|██████▎   | 5/8 [00:07<00:04,  1.65s/it] 75%|███████▌  | 6/8 [00:09<00:03,  1.71s/it] 88%|████████▊ | 7/8 [00:11<00:01,  1.75s/it]100%|██████████| 8/8 [00:12<00:00,  1.51s/it]100%|██████████| 8/8 [00:12<00:00,  1.58s/it]
***** eval metrics *****
  epoch                   =     1.3793
  eval_accuracy           =     0.4723
  eval_loss               =     2.6388
  eval_runtime            = 0:00:14.47
  eval_samples            =        240
  eval_samples_per_second =     16.578
  eval_steps_per_second   =      0.553
  perplexity              =    13.9964
------------------------------------
LoRA: Fine-tuning openai-community/gpt2-large on wikitext (wikitext-103-raw-v1) using baseline model
11/26/2024 09:40:13 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/26/2024 09:40:13 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm2-gpt2-large-lora-wikitext-103-raw-v1/runs/Nov26_09-40-13_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm2-gpt2-large-lora-wikitext-103-raw-v1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm2-gpt2-large-lora-wikitext-103-raw-v1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
11/26/2024 09:40:13 - WARNING - datasets.load - Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-103-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 10:47:57 2024).
11/26/2024 09:40:13 - WARNING - datasets.packaged_modules.cache.cache - Found the latest cached dataset configuration 'wikitext-103-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 10:47:57 2024).
Using custom data configuration wikitext-103-raw-v1
11/26/2024 09:40:13 - INFO - datasets.builder - Using custom data configuration wikitext-103-raw-v1
Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
11/26/2024 09:40:13 - INFO - datasets.info - Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
Overwrite dataset info from restored data version if exists.
11/26/2024 09:40:13 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/26/2024 09:40:13 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-26 09:40:13,868 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:40:13,869 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-large-base-wikitext-103-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:40:13,883 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:40:13,883 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:40:13,883 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:40:13,883 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:40:13,883 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:40:13,883 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-26 09:40:13,973 >> loading weights file /tmp/test-clm-gpt2-large-base-wikitext-103-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-26 09:40:13,980 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-26 09:40:14,073 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-26 09:40:14,073 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-large-base-wikitext-103-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-26 09:40:14,075 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-103-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-26 09:40:14,075 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-c6b4c78a9bc62b62.arrow
11/26/2024 09:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-c6b4c78a9bc62b62.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-48995eed9f595a97.arrow
11/26/2024 09:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-48995eed9f595a97.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-86d5a043031fe8e4.arrow
11/26/2024 09:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-86d5a043031fe8e4.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-144fe6d06de645a8.arrow
11/26/2024 09:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-144fe6d06de645a8.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-b103bdaaea593c99.arrow
11/26/2024 09:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-b103bdaaea593c99.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-31ee60068e45a749.arrow
11/26/2024 09:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-31ee60068e45a749.arrow
11/26/2024 09:40:15 - WARNING - evaluate.loading - Using the latest cached version of the module from /home/cs/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Nov 18 09:05:53 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
[WARNING|trainer.py:664] 2024-11-26 09:40:17,503 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-26 09:40:17,632 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-26 09:40:17,632 >>   Num examples = 114,248
[INFO|trainer.py:2337] 2024-11-26 09:40:17,632 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-26 09:40:17,632 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-26 09:40:17,632 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-26 09:40:17,632 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-26 09:40:17,632 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-26 09:40:17,632 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-26 09:40:17,634 >>   Number of trainable parameters = 1,474,560
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:06<22:03,  6.65s/it]  1%|          | 2/200 [00:08<12:46,  3.87s/it]  2%|▏         | 3/200 [00:10<09:49,  2.99s/it]  2%|▏         | 4/200 [00:12<08:24,  2.58s/it]  2%|▎         | 5/200 [00:14<07:55,  2.44s/it]  3%|▎         | 6/200 [00:17<07:54,  2.44s/it]  4%|▎         | 7/200 [00:19<07:35,  2.36s/it]  4%|▍         | 8/200 [00:21<07:37,  2.38s/it]  4%|▍         | 9/200 [00:24<07:39,  2.40s/it]  5%|▌         | 10/200 [00:26<07:34,  2.39s/it]  6%|▌         | 11/200 [00:28<07:35,  2.41s/it]  6%|▌         | 12/200 [00:31<07:35,  2.42s/it]  6%|▋         | 13/200 [00:33<07:34,  2.43s/it]  7%|▋         | 14/200 [00:36<07:28,  2.41s/it]  8%|▊         | 15/200 [00:38<07:28,  2.42s/it]  8%|▊         | 16/200 [00:41<07:25,  2.42s/it]  8%|▊         | 17/200 [00:43<07:24,  2.43s/it]  9%|▉         | 18/200 [00:46<07:23,  2.44s/it] 10%|▉         | 19/200 [00:48<07:17,  2.42s/it] 10%|█         | 20/200 [00:50<07:17,  2.43s/it] 10%|█         | 21/200 [00:53<07:15,  2.43s/it] 11%|█         | 22/200 [00:55<07:13,  2.44s/it] 12%|█▏        | 23/200 [00:58<07:09,  2.43s/it] 12%|█▏        | 24/200 [01:00<07:04,  2.41s/it] 12%|█▎        | 25/200 [01:02<07:04,  2.42s/it] 13%|█▎        | 26/200 [01:05<07:03,  2.43s/it] 14%|█▎        | 27/200 [01:07<07:01,  2.44s/it] 14%|█▍        | 28/200 [01:10<06:55,  2.42s/it] 14%|█▍        | 29/200 [01:12<06:54,  2.42s/it] 15%|█▌        | 30/200 [01:15<06:53,  2.43s/it] 16%|█▌        | 31/200 [01:17<06:51,  2.44s/it] 16%|█▌        | 32/200 [01:20<06:49,  2.44s/it] 16%|█▋        | 33/200 [01:22<06:43,  2.42s/it] 17%|█▋        | 34/200 [01:24<06:43,  2.43s/it] 18%|█▊        | 35/200 [01:27<06:40,  2.43s/it] 18%|█▊        | 36/200 [01:29<06:39,  2.44s/it] 18%|█▊        | 37/200 [01:32<06:37,  2.44s/it] 19%|█▉        | 38/200 [01:34<06:31,  2.42s/it] 20%|█▉        | 39/200 [01:37<06:31,  2.43s/it] 20%|██        | 40/200 [01:39<06:30,  2.44s/it] 20%|██        | 41/200 [01:41<06:27,  2.43s/it] 21%|██        | 42/200 [01:44<06:21,  2.41s/it] 22%|██▏       | 43/200 [01:46<06:20,  2.42s/it] 22%|██▏       | 44/200 [01:49<06:19,  2.43s/it] 22%|██▎       | 45/200 [01:51<06:17,  2.44s/it] 23%|██▎       | 46/200 [01:54<06:16,  2.44s/it] 24%|██▎       | 47/200 [01:56<06:10,  2.42s/it] 24%|██▍       | 48/200 [01:58<06:09,  2.43s/it] 24%|██▍       | 49/200 [02:01<06:07,  2.44s/it] 25%|██▌       | 50/200 [02:03<06:06,  2.44s/it] 26%|██▌       | 51/200 [02:06<06:04,  2.44s/it] 26%|██▌       | 52/200 [02:08<05:58,  2.42s/it] 26%|██▋       | 53/200 [02:11<05:57,  2.43s/it] 27%|██▋       | 54/200 [02:13<05:56,  2.44s/it] 28%|██▊       | 55/200 [02:15<05:52,  2.43s/it] 28%|██▊       | 56/200 [02:18<05:47,  2.42s/it] 28%|██▊       | 57/200 [02:20<05:47,  2.43s/it] 29%|██▉       | 58/200 [02:23<05:45,  2.43s/it] 30%|██▉       | 59/200 [02:25<05:43,  2.43s/it] 30%|███       | 60/200 [02:28<05:50,  2.50s/it] 30%|███       | 61/200 [02:30<05:45,  2.49s/it] 31%|███       | 62/200 [02:33<05:41,  2.48s/it] 32%|███▏      | 63/200 [02:35<05:38,  2.47s/it] 32%|███▏      | 64/200 [02:38<05:36,  2.47s/it] 32%|███▎      | 65/200 [02:40<05:29,  2.44s/it] 33%|███▎      | 66/200 [02:42<05:27,  2.45s/it] 34%|███▎      | 67/200 [02:45<05:25,  2.45s/it] 34%|███▍      | 68/200 [02:47<05:23,  2.45s/it] 34%|███▍      | 69/200 [02:50<05:21,  2.45s/it] 35%|███▌      | 70/200 [02:52<05:15,  2.43s/it] 36%|███▌      | 71/200 [02:55<05:14,  2.43s/it] 36%|███▌      | 72/200 [02:57<05:12,  2.44s/it] 36%|███▋      | 73/200 [03:00<05:10,  2.45s/it] 37%|███▋      | 74/200 [03:02<05:05,  2.43s/it] 38%|███▊      | 75/200 [03:04<05:04,  2.43s/it] 38%|███▊      | 76/200 [03:07<05:02,  2.44s/it] 38%|███▊      | 77/200 [03:09<05:00,  2.44s/it] 39%|███▉      | 78/200 [03:12<04:58,  2.44s/it] 40%|███▉      | 79/200 [03:14<04:56,  2.45s/it] 40%|████      | 80/200 [03:17<04:54,  2.45s/it] 40%|████      | 81/200 [03:19<04:51,  2.45s/it] 41%|████      | 82/200 [03:22<04:49,  2.45s/it] 42%|████▏     | 83/200 [03:24<04:46,  2.45s/it] 42%|████▏     | 84/200 [03:26<04:41,  2.43s/it] 42%|████▎     | 85/200 [03:29<04:40,  2.44s/it] 43%|████▎     | 86/200 [03:31<04:38,  2.44s/it] 44%|████▎     | 87/200 [03:34<04:36,  2.44s/it] 44%|████▍     | 88/200 [03:36<04:33,  2.45s/it] 44%|████▍     | 89/200 [03:39<04:29,  2.43s/it] 45%|████▌     | 90/200 [03:41<04:27,  2.44s/it] 46%|████▌     | 91/200 [03:44<04:25,  2.44s/it] 46%|████▌     | 92/200 [03:46<04:23,  2.44s/it] 46%|████▋     | 93/200 [03:48<04:23,  2.47s/it] 47%|████▋     | 94/200 [03:51<04:20,  2.46s/it] 48%|████▊     | 95/200 [03:53<04:17,  2.46s/it] 48%|████▊     | 96/200 [03:56<04:15,  2.45s/it] 48%|████▊     | 97/200 [03:58<04:12,  2.45s/it] 49%|████▉     | 98/200 [04:01<04:07,  2.43s/it] 50%|████▉     | 99/200 [04:03<04:05,  2.43s/it] 50%|█████     | 100/200 [04:06<04:04,  2.44s/it] 50%|█████     | 101/200 [04:08<04:01,  2.44s/it] 51%|█████     | 102/200 [04:10<03:59,  2.44s/it] 52%|█████▏    | 103/200 [04:13<03:54,  2.42s/it] 52%|█████▏    | 104/200 [04:15<03:53,  2.43s/it] 52%|█████▎    | 105/200 [04:18<03:51,  2.43s/it] 53%|█████▎    | 106/200 [04:20<03:49,  2.44s/it] 54%|█████▎    | 107/200 [04:23<03:51,  2.49s/it] 54%|█████▍    | 108/200 [04:25<03:47,  2.48s/it] 55%|█████▍    | 109/200 [04:28<03:44,  2.47s/it] 55%|█████▌    | 110/200 [04:30<03:41,  2.47s/it] 56%|█████▌    | 111/200 [04:33<03:38,  2.46s/it] 56%|█████▌    | 112/200 [04:35<03:33,  2.43s/it] 56%|█████▋    | 113/200 [04:37<03:33,  2.45s/it] 57%|█████▋    | 114/200 [04:40<03:29,  2.44s/it] 57%|█████▊    | 115/200 [04:42<03:25,  2.42s/it] 58%|█████▊    | 116/200 [04:45<03:23,  2.42s/it] 58%|█████▊    | 117/200 [04:47<03:18,  2.39s/it] 59%|█████▉    | 118/200 [04:49<03:15,  2.39s/it] 60%|█████▉    | 119/200 [04:52<03:13,  2.39s/it] 60%|██████    | 120/200 [04:54<03:10,  2.39s/it] 60%|██████    | 121/200 [04:56<03:06,  2.36s/it] 61%|██████    | 122/200 [04:59<03:04,  2.37s/it] 62%|██████▏   | 123/200 [05:01<03:03,  2.38s/it] 62%|██████▏   | 124/200 [05:04<03:00,  2.38s/it] 62%|██████▎   | 125/200 [05:06<02:58,  2.38s/it] 63%|██████▎   | 126/200 [05:08<02:56,  2.38s/it] 64%|██████▎   | 127/200 [05:11<02:53,  2.38s/it] 64%|██████▍   | 128/200 [05:13<02:51,  2.39s/it] 64%|██████▍   | 129/200 [05:16<02:49,  2.39s/it] 65%|██████▌   | 130/200 [05:18<02:46,  2.38s/it] 66%|██████▌   | 131/200 [05:20<02:36,  2.27s/it] 66%|██████▌   | 132/200 [05:22<02:27,  2.16s/it] 66%|██████▋   | 133/200 [05:24<02:19,  2.08s/it] 67%|██████▋   | 134/200 [05:26<02:13,  2.03s/it] 68%|██████▊   | 135/200 [05:28<02:09,  1.99s/it] 68%|██████▊   | 136/200 [05:30<02:08,  2.00s/it] 68%|██████▊   | 137/200 [05:32<02:06,  2.01s/it] 69%|██████▉   | 138/200 [05:34<02:03,  2.00s/it] 70%|██████▉   | 139/200 [05:35<02:01,  1.98s/it] 70%|███████   | 140/200 [05:37<01:58,  1.98s/it] 70%|███████   | 141/200 [05:39<01:56,  1.97s/it] 71%|███████   | 142/200 [05:41<01:53,  1.96s/it] 72%|███████▏  | 143/200 [05:43<01:51,  1.96s/it] 72%|███████▏  | 144/200 [05:45<01:49,  1.95s/it] 72%|███████▎  | 145/200 [05:47<01:48,  1.97s/it] 73%|███████▎  | 146/200 [05:49<01:46,  1.97s/it] 74%|███████▎  | 147/200 [05:51<01:44,  1.96s/it] 74%|███████▍  | 148/200 [05:53<01:41,  1.96s/it] 74%|███████▍  | 149/200 [05:55<01:39,  1.96s/it] 75%|███████▌  | 150/200 [05:57<01:38,  1.98s/it] 76%|███████▌  | 151/200 [05:59<01:36,  1.97s/it] 76%|███████▌  | 152/200 [06:01<01:34,  1.97s/it] 76%|███████▋  | 153/200 [06:03<01:32,  1.96s/it] 77%|███████▋  | 154/200 [06:05<01:31,  1.99s/it] 78%|███████▊  | 155/200 [06:07<01:29,  1.98s/it] 78%|███████▊  | 156/200 [06:09<01:26,  1.97s/it] 78%|███████▊  | 157/200 [06:11<01:24,  1.97s/it] 79%|███████▉  | 158/200 [06:13<01:22,  1.96s/it] 80%|███████▉  | 159/200 [06:15<01:21,  1.98s/it] 80%|████████  | 160/200 [06:17<01:19,  1.98s/it] 80%|████████  | 161/200 [06:19<01:16,  1.97s/it] 81%|████████  | 162/200 [06:21<01:16,  2.02s/it] 82%|████████▏ | 163/200 [06:23<01:19,  2.15s/it] 82%|████████▏ | 164/200 [06:26<01:19,  2.22s/it] 82%|████████▎ | 165/200 [06:28<01:20,  2.29s/it] 83%|████████▎ | 166/200 [06:31<01:19,  2.34s/it] 84%|████████▎ | 167/200 [06:33<01:18,  2.38s/it] 84%|████████▍ | 168/200 [06:35<01:15,  2.37s/it] 84%|████████▍ | 169/200 [06:38<01:14,  2.40s/it] 85%|████████▌ | 170/200 [06:40<01:12,  2.42s/it] 86%|████████▌ | 171/200 [06:43<01:10,  2.43s/it] 86%|████████▌ | 172/200 [06:45<01:08,  2.44s/it] 86%|████████▋ | 173/200 [06:48<01:05,  2.44s/it] 87%|████████▋ | 174/200 [06:50<01:03,  2.45s/it] 88%|████████▊ | 175/200 [06:53<01:01,  2.45s/it] 88%|████████▊ | 176/200 [06:55<00:58,  2.46s/it] 88%|████████▊ | 177/200 [06:58<00:56,  2.45s/it] 89%|████████▉ | 178/200 [07:00<00:53,  2.44s/it] 90%|████████▉ | 179/200 [07:02<00:51,  2.44s/it] 90%|█████████ | 180/200 [07:05<00:48,  2.45s/it] 90%|█████████ | 181/200 [07:07<00:46,  2.45s/it] 91%|█████████ | 182/200 [07:10<00:44,  2.45s/it] 92%|█████████▏| 183/200 [07:12<00:41,  2.43s/it] 92%|█████████▏| 184/200 [07:15<00:38,  2.44s/it] 92%|█████████▎| 185/200 [07:17<00:36,  2.44s/it] 93%|█████████▎| 186/200 [07:20<00:34,  2.44s/it] 94%|█████████▎| 187/200 [07:22<00:31,  2.46s/it] 94%|█████████▍| 188/200 [07:24<00:29,  2.45s/it] 94%|█████████▍| 189/200 [07:27<00:26,  2.45s/it] 95%|█████████▌| 190/200 [07:29<00:24,  2.45s/it] 96%|█████████▌| 191/200 [07:32<00:22,  2.45s/it] 96%|█████████▌| 192/200 [07:34<00:19,  2.43s/it] 96%|█████████▋| 193/200 [07:37<00:17,  2.44s/it] 97%|█████████▋| 194/200 [07:39<00:14,  2.44s/it] 98%|█████████▊| 195/200 [07:42<00:12,  2.44s/it] 98%|█████████▊| 196/200 [07:44<00:09,  2.45s/it] 98%|█████████▊| 197/200 [07:46<00:07,  2.43s/it] 99%|█████████▉| 198/200 [07:49<00:04,  2.44s/it]100%|█████████▉| 199/200 [07:51<00:02,  2.44s/it]100%|██████████| 200/200 [07:54<00:00,  2.45s/it][INFO|trainer.py:3846] 2024-11-26 09:48:11,908 >> Saving model checkpoint to /tmp/test-clm2-gpt2-large-lora-wikitext-103-raw-v1/checkpoint-200
[INFO|configuration_utils.py:690] 2024-11-26 09:48:11,921 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:48:11,922 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-large",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:48:11,935 >> tokenizer config file saved in /tmp/test-clm2-gpt2-large-lora-wikitext-103-raw-v1/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:48:11,935 >> Special tokens file saved in /tmp/test-clm2-gpt2-large-lora-wikitext-103-raw-v1/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-26 09:48:12,003 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [07:54<00:00,  2.45s/it]100%|██████████| 200/200 [07:54<00:00,  2.37s/it]
[INFO|trainer.py:3846] 2024-11-26 09:48:12,005 >> Saving model checkpoint to /tmp/test-clm2-gpt2-large-lora-wikitext-103-raw-v1
[INFO|configuration_utils.py:690] 2024-11-26 09:48:12,017 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:48:12,018 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-large",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:48:12,028 >> tokenizer config file saved in /tmp/test-clm2-gpt2-large-lora-wikitext-103-raw-v1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:48:12,028 >> Special tokens file saved in /tmp/test-clm2-gpt2-large-lora-wikitext-103-raw-v1/special_tokens_map.json
{'train_runtime': 474.3695, 'train_samples_per_second': 6.746, 'train_steps_per_second': 0.422, 'train_loss': 2.566797790527344, 'epoch': 0.03}
***** train metrics *****
  epoch                    =      0.028
  total_flos               = 12998015GF
  train_loss               =     2.5668
  train_runtime            = 0:07:54.36
  train_samples            =     114248
  train_samples_per_second =      6.746
  train_steps_per_second   =      0.422
11/26/2024 09:48:12 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-26 09:48:12,081 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-26 09:48:12,081 >>   Num examples = 240
[INFO|trainer.py:4167] 2024-11-26 09:48:12,081 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:02<00:06,  1.08s/it] 38%|███▊      | 3/8 [00:04<00:07,  1.56s/it] 50%|█████     | 4/8 [00:06<00:07,  1.82s/it] 62%|██████▎   | 5/8 [00:08<00:05,  1.98s/it] 75%|███████▌  | 6/8 [00:11<00:04,  2.08s/it] 88%|████████▊ | 7/8 [00:13<00:02,  2.10s/it]100%|██████████| 8/8 [00:14<00:00,  1.88s/it]100%|██████████| 8/8 [00:15<00:00,  1.90s/it]
***** eval metrics *****
  epoch                   =      0.028
  eval_accuracy           =     0.4733
  eval_loss               =     2.6282
  eval_runtime            = 0:00:17.96
  eval_samples            =        240
  eval_samples_per_second =     13.361
  eval_steps_per_second   =      0.445
  perplexity              =    13.8482
------------------------------------
LagEmbed: Training openai-community/gpt2-large on wikitext (wikitext-103-raw-v1) with LagEmbed (in_channels=1280, n_components=3, dof=8)
11/26/2024 09:48:35 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/26/2024 09:48:35 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1/runs/Nov26_09-48-35_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
11/26/2024 09:48:35 - WARNING - datasets.load - Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-103-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 10:47:57 2024).
11/26/2024 09:48:35 - WARNING - datasets.packaged_modules.cache.cache - Found the latest cached dataset configuration 'wikitext-103-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Nov 21 10:47:57 2024).
Using custom data configuration wikitext-103-raw-v1
11/26/2024 09:48:35 - INFO - datasets.builder - Using custom data configuration wikitext-103-raw-v1
Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
11/26/2024 09:48:35 - INFO - datasets.info - Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
Overwrite dataset info from restored data version if exists.
11/26/2024 09:48:35 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/26/2024 09:48:35 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-26 09:48:35,539 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-26 09:48:35,541 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-large-base-wikitext-103-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:48:35,562 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:48:35,562 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:48:35,563 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:48:35,563 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:48:35,563 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-26 09:48:35,563 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-26 09:48:35,670 >> loading weights file /tmp/test-clm-gpt2-large-base-wikitext-103-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-26 09:48:35,678 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-26 09:48:35,773 >> All model checkpoint weights were used when initializing LagGPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-26 09:48:35,773 >> All the weights of LagGPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-large-base-wikitext-103-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LagGPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-26 09:48:35,775 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-103-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-26 09:48:35,775 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-c6b4c78a9bc62b62.arrow
11/26/2024 09:48:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-c6b4c78a9bc62b62.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-48995eed9f595a97.arrow
11/26/2024 09:48:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-48995eed9f595a97.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-86d5a043031fe8e4.arrow
11/26/2024 09:48:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-86d5a043031fe8e4.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-144fe6d06de645a8.arrow
11/26/2024 09:48:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-144fe6d06de645a8.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-b103bdaaea593c99.arrow
11/26/2024 09:48:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-b103bdaaea593c99.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-31ee60068e45a749.arrow
11/26/2024 09:48:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-31ee60068e45a749.arrow
11/26/2024 09:48:37 - WARNING - evaluate.loading - Using the latest cached version of the module from /home/cs/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Nov 18 09:05:53 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
[WARNING|trainer.py:664] 2024-11-26 09:48:39,350 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-26 09:48:39,477 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-26 09:48:39,477 >>   Num examples = 114,248
[INFO|trainer.py:2337] 2024-11-26 09:48:39,477 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-26 09:48:39,477 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-26 09:48:39,477 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-26 09:48:39,478 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-26 09:48:39,478 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-26 09:48:39,478 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-26 09:48:39,478 >>   Number of trainable parameters = 1,210,011
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:07<25:30,  7.69s/it]  1%|          | 2/200 [00:08<12:53,  3.91s/it]  2%|▏         | 3/200 [00:10<08:52,  2.70s/it]  2%|▏         | 4/200 [00:11<06:58,  2.14s/it]  2%|▎         | 5/200 [00:12<05:55,  1.82s/it]  3%|▎         | 6/200 [00:14<05:17,  1.64s/it]  4%|▎         | 7/200 [00:15<04:52,  1.52s/it]  4%|▍         | 8/200 [00:16<04:36,  1.44s/it]  4%|▍         | 9/200 [00:17<04:28,  1.41s/it]  5%|▌         | 10/200 [00:19<04:19,  1.36s/it]  6%|▌         | 11/200 [00:20<04:12,  1.34s/it]  6%|▌         | 12/200 [00:21<04:07,  1.32s/it]  6%|▋         | 13/200 [00:22<04:03,  1.30s/it]  7%|▋         | 14/200 [00:24<04:00,  1.29s/it]  8%|▊         | 15/200 [00:25<03:57,  1.29s/it]  8%|▊         | 16/200 [00:26<03:59,  1.30s/it]  8%|▊         | 17/200 [00:28<03:56,  1.29s/it]  9%|▉         | 18/200 [00:29<03:53,  1.28s/it] 10%|▉         | 19/200 [00:30<03:51,  1.28s/it] 10%|█         | 20/200 [00:31<03:49,  1.28s/it] 10%|█         | 21/200 [00:33<03:48,  1.27s/it] 11%|█         | 22/200 [00:34<03:46,  1.27s/it] 12%|█▏        | 23/200 [00:35<03:45,  1.27s/it] 12%|█▏        | 24/200 [00:37<03:43,  1.27s/it] 12%|█▎        | 25/200 [00:38<03:45,  1.29s/it] 13%|█▎        | 26/200 [00:39<03:43,  1.28s/it] 14%|█▎        | 27/200 [00:40<03:41,  1.28s/it] 14%|█▍        | 28/200 [00:42<03:39,  1.28s/it] 14%|█▍        | 29/200 [00:43<03:38,  1.27s/it] 15%|█▌        | 30/200 [00:44<03:36,  1.27s/it] 16%|█▌        | 31/200 [00:45<03:35,  1.27s/it] 16%|█▌        | 32/200 [00:47<03:33,  1.27s/it] 16%|█▋        | 33/200 [00:48<03:32,  1.27s/it] 17%|█▋        | 34/200 [00:49<03:33,  1.29s/it] 18%|█▊        | 35/200 [00:51<03:31,  1.28s/it] 18%|█▊        | 36/200 [00:52<03:29,  1.28s/it] 18%|█▊        | 37/200 [00:53<03:28,  1.28s/it] 19%|█▉        | 38/200 [00:54<03:26,  1.28s/it] 20%|█▉        | 39/200 [00:56<03:25,  1.27s/it] 20%|██        | 40/200 [00:57<03:23,  1.27s/it] 20%|██        | 41/200 [00:58<03:22,  1.27s/it] 21%|██        | 42/200 [01:00<03:20,  1.27s/it] 22%|██▏       | 43/200 [01:01<03:22,  1.29s/it] 22%|██▏       | 44/200 [01:02<03:20,  1.28s/it] 22%|██▎       | 45/200 [01:03<03:18,  1.28s/it] 23%|██▎       | 46/200 [01:05<03:16,  1.28s/it] 24%|██▎       | 47/200 [01:06<03:14,  1.27s/it] 24%|██▍       | 48/200 [01:07<03:13,  1.27s/it] 24%|██▍       | 49/200 [01:08<03:12,  1.27s/it] 25%|██▌       | 50/200 [01:10<03:10,  1.27s/it] 26%|██▌       | 51/200 [01:11<03:14,  1.30s/it] 26%|██▌       | 52/200 [01:12<03:11,  1.29s/it] 26%|██▋       | 53/200 [01:14<03:08,  1.28s/it] 27%|██▋       | 54/200 [01:15<03:06,  1.28s/it] 28%|██▊       | 55/200 [01:16<03:05,  1.28s/it] 28%|██▊       | 56/200 [01:17<03:03,  1.28s/it] 28%|██▊       | 57/200 [01:19<03:01,  1.27s/it] 29%|██▉       | 58/200 [01:20<03:00,  1.27s/it] 30%|██▉       | 59/200 [01:21<02:59,  1.27s/it] 30%|███       | 60/200 [01:23<02:57,  1.27s/it] 30%|███       | 61/200 [01:24<02:59,  1.29s/it] 31%|███       | 62/200 [01:25<02:57,  1.28s/it] 32%|███▏      | 63/200 [01:26<02:55,  1.28s/it] 32%|███▏      | 64/200 [01:28<02:53,  1.28s/it] 32%|███▎      | 65/200 [01:29<02:51,  1.27s/it] 33%|███▎      | 66/200 [01:30<02:50,  1.27s/it] 34%|███▎      | 67/200 [01:31<02:49,  1.27s/it] 34%|███▍      | 68/200 [01:33<02:48,  1.28s/it] 34%|███▍      | 69/200 [01:34<02:49,  1.29s/it] 35%|███▌      | 70/200 [01:35<02:47,  1.29s/it] 36%|███▌      | 71/200 [01:37<02:45,  1.28s/it] 36%|███▌      | 72/200 [01:38<02:43,  1.28s/it] 36%|███▋      | 73/200 [01:39<02:42,  1.28s/it] 37%|███▋      | 74/200 [01:40<02:40,  1.27s/it] 38%|███▊      | 75/200 [01:42<02:39,  1.27s/it] 38%|███▊      | 76/200 [01:43<02:38,  1.27s/it] 38%|███▊      | 77/200 [01:44<02:36,  1.27s/it] 39%|███▉      | 78/200 [01:46<02:37,  1.29s/it] 40%|███▉      | 79/200 [01:47<02:35,  1.28s/it] 40%|████      | 80/200 [01:48<02:33,  1.28s/it] 40%|████      | 81/200 [01:49<02:32,  1.28s/it] 41%|████      | 82/200 [01:51<02:30,  1.28s/it] 42%|████▏     | 83/200 [01:52<02:28,  1.27s/it] 42%|████▏     | 84/200 [01:53<02:27,  1.27s/it] 42%|████▎     | 85/200 [01:54<02:26,  1.27s/it] 43%|████▎     | 86/200 [01:56<02:26,  1.29s/it] 44%|████▎     | 87/200 [01:57<02:24,  1.28s/it] 44%|████▍     | 88/200 [01:58<02:23,  1.28s/it] 44%|████▍     | 89/200 [02:00<02:21,  1.28s/it] 45%|████▌     | 90/200 [02:01<02:20,  1.28s/it] 46%|████▌     | 91/200 [02:02<02:18,  1.27s/it] 46%|████▌     | 92/200 [02:03<02:17,  1.27s/it] 46%|████▋     | 93/200 [02:05<02:16,  1.27s/it] 47%|████▋     | 94/200 [02:06<02:14,  1.27s/it] 48%|████▊     | 95/200 [02:07<02:13,  1.27s/it] 48%|████▊     | 96/200 [02:09<02:14,  1.29s/it] 48%|████▊     | 97/200 [02:10<02:12,  1.29s/it] 49%|████▉     | 98/200 [02:11<02:10,  1.28s/it] 50%|████▉     | 99/200 [02:12<02:09,  1.28s/it] 50%|█████     | 100/200 [02:14<02:07,  1.28s/it] 50%|█████     | 101/200 [02:15<02:06,  1.28s/it] 51%|█████     | 102/200 [02:16<02:04,  1.27s/it] 52%|█████▏    | 103/200 [02:18<02:03,  1.27s/it] 52%|█████▏    | 104/200 [02:19<02:03,  1.29s/it] 52%|█████▎    | 105/200 [02:20<02:01,  1.28s/it] 53%|█████▎    | 106/200 [02:21<02:00,  1.28s/it] 54%|█████▎    | 107/200 [02:23<01:58,  1.28s/it] 54%|█████▍    | 108/200 [02:24<01:57,  1.28s/it] 55%|█████▍    | 109/200 [02:25<01:55,  1.27s/it] 55%|█████▌    | 110/200 [02:26<01:54,  1.27s/it] 56%|█████▌    | 111/200 [02:28<01:53,  1.27s/it] 56%|█████▌    | 112/200 [02:29<01:52,  1.27s/it] 56%|█████▋    | 113/200 [02:30<01:52,  1.29s/it] 57%|█████▋    | 114/200 [02:32<01:59,  1.39s/it] 57%|█████▊    | 115/200 [02:34<02:07,  1.50s/it] 58%|█████▊    | 116/200 [02:36<02:13,  1.59s/it] 58%|█████▊    | 117/200 [02:37<02:16,  1.64s/it] 59%|█████▉    | 118/200 [02:39<02:17,  1.68s/it] 60%|█████▉    | 119/200 [02:41<02:18,  1.71s/it] 60%|██████    | 120/200 [02:43<02:18,  1.73s/it] 60%|██████    | 121/200 [02:44<02:20,  1.77s/it] 61%|██████    | 122/200 [02:46<02:18,  1.77s/it] 62%|██████▏   | 123/200 [02:48<02:16,  1.77s/it] 62%|██████▏   | 124/200 [02:50<02:14,  1.77s/it] 62%|██████▎   | 125/200 [02:52<02:12,  1.77s/it] 63%|██████▎   | 126/200 [02:53<02:11,  1.77s/it] 64%|██████▎   | 127/200 [02:55<02:09,  1.77s/it] 64%|██████▍   | 128/200 [02:57<02:07,  1.77s/it] 64%|██████▍   | 129/200 [02:59<02:06,  1.78s/it] 65%|██████▌   | 130/200 [03:00<02:04,  1.77s/it] 66%|██████▌   | 131/200 [03:02<02:00,  1.74s/it] 66%|██████▌   | 132/200 [03:04<01:59,  1.75s/it] 66%|██████▋   | 133/200 [03:06<01:57,  1.76s/it] 67%|██████▋   | 134/200 [03:07<01:56,  1.77s/it] 68%|██████▊   | 135/200 [03:09<01:54,  1.77s/it] 68%|██████▊   | 136/200 [03:11<01:53,  1.77s/it] 68%|██████▊   | 137/200 [03:13<01:51,  1.77s/it] 69%|██████▉   | 138/200 [03:15<01:50,  1.78s/it] 70%|██████▉   | 139/200 [03:16<01:46,  1.74s/it] 70%|███████   | 140/200 [03:18<01:45,  1.75s/it] 70%|███████   | 141/200 [03:20<01:43,  1.76s/it] 71%|███████   | 142/200 [03:22<01:42,  1.76s/it] 72%|███████▏  | 143/200 [03:23<01:40,  1.77s/it] 72%|███████▏  | 144/200 [03:25<01:39,  1.77s/it] 72%|███████▎  | 145/200 [03:27<01:37,  1.77s/it] 73%|███████▎  | 146/200 [03:29<01:35,  1.77s/it] 74%|███████▎  | 147/200 [03:30<01:34,  1.77s/it] 74%|███████▍  | 148/200 [03:32<01:30,  1.74s/it] 74%|███████▍  | 149/200 [03:34<01:29,  1.75s/it] 75%|███████▌  | 150/200 [03:36<01:27,  1.76s/it] 76%|███████▌  | 151/200 [03:37<01:26,  1.76s/it] 76%|███████▌  | 152/200 [03:39<01:24,  1.77s/it] 76%|███████▋  | 153/200 [03:41<01:23,  1.77s/it] 77%|███████▋  | 154/200 [03:43<01:21,  1.77s/it] 78%|███████▊  | 155/200 [03:45<01:19,  1.77s/it] 78%|███████▊  | 156/200 [03:46<01:18,  1.79s/it] 78%|███████▊  | 157/200 [03:48<01:17,  1.79s/it] 79%|███████▉  | 158/200 [03:50<01:15,  1.79s/it] 80%|███████▉  | 159/200 [03:52<01:13,  1.78s/it] 80%|████████  | 160/200 [03:53<01:11,  1.78s/it] 80%|████████  | 161/200 [03:55<01:09,  1.78s/it] 81%|████████  | 162/200 [03:57<01:07,  1.78s/it] 82%|████████▏ | 163/200 [03:59<01:05,  1.77s/it] 82%|████████▏ | 164/200 [04:01<01:03,  1.78s/it] 82%|████████▎ | 165/200 [04:02<01:02,  1.78s/it] 83%|████████▎ | 166/200 [04:04<00:59,  1.74s/it] 84%|████████▎ | 167/200 [04:06<00:57,  1.75s/it] 84%|████████▍ | 168/200 [04:08<00:56,  1.76s/it] 84%|████████▍ | 169/200 [04:09<00:54,  1.76s/it] 85%|████████▌ | 170/200 [04:11<00:53,  1.77s/it] 86%|████████▌ | 171/200 [04:13<00:51,  1.77s/it] 86%|████████▌ | 172/200 [04:15<00:49,  1.77s/it] 86%|████████▋ | 173/200 [04:16<00:47,  1.77s/it] 87%|████████▋ | 174/200 [04:18<00:45,  1.74s/it] 88%|████████▊ | 175/200 [04:20<00:43,  1.75s/it] 88%|████████▊ | 176/200 [04:22<00:42,  1.76s/it] 88%|████████▊ | 177/200 [04:23<00:40,  1.76s/it] 89%|████████▉ | 178/200 [04:25<00:38,  1.77s/it] 90%|████████▉ | 179/200 [04:27<00:37,  1.77s/it] 90%|█████████ | 180/200 [04:29<00:35,  1.77s/it] 90%|█████████ | 181/200 [04:31<00:33,  1.78s/it] 91%|█████████ | 182/200 [04:32<00:31,  1.77s/it] 92%|█████████▏| 183/200 [04:34<00:29,  1.74s/it] 92%|█████████▏| 184/200 [04:36<00:28,  1.75s/it] 92%|█████████▎| 185/200 [04:38<00:26,  1.76s/it] 93%|█████████▎| 186/200 [04:39<00:24,  1.77s/it] 94%|█████████▎| 187/200 [04:41<00:23,  1.77s/it] 94%|█████████▍| 188/200 [04:43<00:21,  1.77s/it] 94%|█████████▍| 189/200 [04:45<00:19,  1.77s/it] 95%|█████████▌| 190/200 [04:46<00:17,  1.77s/it] 96%|█████████▌| 191/200 [04:48<00:16,  1.79s/it] 96%|█████████▌| 192/200 [04:50<00:14,  1.79s/it] 96%|█████████▋| 193/200 [04:52<00:12,  1.78s/it] 97%|█████████▋| 194/200 [04:54<00:10,  1.78s/it] 98%|█████████▊| 195/200 [04:55<00:08,  1.78s/it] 98%|█████████▊| 196/200 [04:57<00:07,  1.78s/it] 98%|█████████▊| 197/200 [04:59<00:05,  1.78s/it] 99%|█████████▉| 198/200 [05:01<00:03,  1.78s/it]100%|█████████▉| 199/200 [05:02<00:01,  1.78s/it]100%|██████████| 200/200 [05:04<00:00,  1.78s/it][INFO|trainer.py:3846] 2024-11-26 09:53:44,211 >> Saving model checkpoint to /tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1/checkpoint-200
[INFO|configuration_utils.py:416] 2024-11-26 09:53:44,213 >> Configuration saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1/checkpoint-200/config.json
[INFO|configuration_utils.py:873] 2024-11-26 09:53:44,213 >> Configuration saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-26 09:53:49,501 >> Model weights saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1/checkpoint-200/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:53:49,503 >> tokenizer config file saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:53:49,503 >> Special tokens file saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-26 09:53:49,549 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [05:10<00:00,  1.78s/it]100%|██████████| 200/200 [05:10<00:00,  1.55s/it]
[INFO|trainer.py:3846] 2024-11-26 09:53:49,551 >> Saving model checkpoint to /tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1
[INFO|configuration_utils.py:416] 2024-11-26 09:53:49,552 >> Configuration saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1/config.json
[INFO|configuration_utils.py:873] 2024-11-26 09:53:49,552 >> Configuration saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-26 09:53:53,718 >> Model weights saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-26 09:53:53,720 >> tokenizer config file saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-26 09:53:53,720 >> Special tokens file saved in /tmp/test-clm2-gpt2-large-lagembed-wikitext-103-raw-v1/special_tokens_map.json
{'train_runtime': 310.071, 'train_samples_per_second': 10.32, 'train_steps_per_second': 0.645, 'train_loss': 2.4950128173828126, 'epoch': 0.03}
***** train metrics *****
  epoch                    =      0.028
  total_flos               = 12993171GF
  train_loss               =      2.495
  train_runtime            = 0:05:10.07
  train_samples            =     114248
  train_samples_per_second =      10.32
  train_steps_per_second   =      0.645
11/26/2024 09:53:53 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-26 09:53:53,768 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-26 09:53:53,768 >>   Num examples = 240
[INFO|trainer.py:4167] 2024-11-26 09:53:53,768 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:02<00:06,  1.07s/it] 38%|███▊      | 3/8 [00:04<00:07,  1.53s/it] 50%|█████     | 4/8 [00:06<00:06,  1.73s/it] 62%|██████▎   | 5/8 [00:08<00:05,  1.76s/it] 75%|███████▌  | 6/8 [00:09<00:03,  1.77s/it] 88%|████████▊ | 7/8 [00:11<00:01,  1.77s/it]100%|██████████| 8/8 [00:12<00:00,  1.53s/it]100%|██████████| 8/8 [00:13<00:00,  1.65s/it]
***** eval metrics *****
  epoch                   =      0.028
  eval_accuracy           =     0.4733
  eval_loss               =     2.6279
  eval_runtime            = 0:00:15.28
  eval_samples            =        240
  eval_samples_per_second =     15.702
  eval_steps_per_second   =      0.523
  perplexity              =    13.8442
------------------------------------
