LoRA: Fine-tuning openai-community/gpt2 on wikitext-2-raw-v1 and wikitext-103-raw-v1 using baseline model
11/19/2024 15:23:24 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/19/2024 15:23:24 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm-transfer-gpt2-lora/runs/Nov19_15-23-24_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm-transfer-gpt2-lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm-transfer-gpt2-lora,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
11/19/2024 15:23:26 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:23:26 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:23:26 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:23:26 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Overwrite dataset info from restored data version if exists.
11/19/2024 15:23:28 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:23:28 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:23:28 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:23:28 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-19 15:23:28,456 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:23:28,457 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:23:28,468 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:23:28,468 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:23:28,468 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:23:28,468 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:23:28,468 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:23:28,468 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-19 15:23:28,583 >> loading weights file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-19 15:23:28,587 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-19 15:23:28,623 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-19 15:23:28,623 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-19 15:23:28,624 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-19 15:23:28,624 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-0f713823a10cfe5c.arrow
11/19/2024 15:23:29 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-0f713823a10cfe5c.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-2b242ceb6911396d.arrow
11/19/2024 15:23:29 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-2b242ceb6911396d.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-1eaec44ca2a0bcf1.arrow
11/19/2024 15:23:30 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-1eaec44ca2a0bcf1.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-6f312b8bea1d6878.arrow
11/19/2024 15:23:30 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-6f312b8bea1d6878.arrow
[WARNING|trainer.py:664] 2024-11-19 15:23:31,926 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-19 15:23:32,076 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-19 15:23:32,076 >>   Num examples = 116,582
[INFO|trainer.py:2337] 2024-11-19 15:23:32,076 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-19 15:23:32,076 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-19 15:23:32,076 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-19 15:23:32,076 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-19 15:23:32,076 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-19 15:23:32,076 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-19 15:23:32,076 >>   Number of trainable parameters = 294,912
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:07<24:34,  7.41s/it]  1%|          | 2/200 [00:08<11:16,  3.42s/it]  2%|▏         | 3/200 [00:08<07:00,  2.13s/it]  2%|▏         | 4/200 [00:09<04:58,  1.52s/it]  2%|▎         | 5/200 [00:09<03:50,  1.18s/it]  3%|▎         | 6/200 [00:10<03:10,  1.02it/s]  4%|▎         | 7/200 [00:10<02:45,  1.17it/s]  4%|▍         | 8/200 [00:11<02:28,  1.29it/s]  4%|▍         | 9/200 [00:12<02:18,  1.38it/s]  5%|▌         | 10/200 [00:12<02:10,  1.46it/s]  6%|▌         | 11/200 [00:13<02:06,  1.49it/s]  6%|▌         | 12/200 [00:14<02:03,  1.52it/s]  6%|▋         | 13/200 [00:14<02:00,  1.55it/s]  7%|▋         | 14/200 [00:15<01:57,  1.58it/s]  8%|▊         | 15/200 [00:15<01:55,  1.60it/s]  8%|▊         | 16/200 [00:16<01:53,  1.62it/s]  8%|▊         | 17/200 [00:17<01:52,  1.63it/s]  9%|▉         | 18/200 [00:17<01:51,  1.64it/s] 10%|▉         | 19/200 [00:18<01:49,  1.65it/s] 10%|█         | 20/200 [00:18<01:49,  1.65it/s] 10%|█         | 21/200 [00:19<01:45,  1.70it/s] 11%|█         | 22/200 [00:20<01:45,  1.69it/s] 12%|█▏        | 23/200 [00:20<01:45,  1.68it/s] 12%|█▏        | 24/200 [00:21<01:45,  1.67it/s] 12%|█▎        | 25/200 [00:21<01:44,  1.67it/s] 13%|█▎        | 26/200 [00:22<01:44,  1.67it/s] 14%|█▎        | 27/200 [00:23<01:44,  1.65it/s] 14%|█▍        | 28/200 [00:23<01:44,  1.65it/s] 14%|█▍        | 29/200 [00:24<01:42,  1.66it/s] 15%|█▌        | 30/200 [00:24<01:42,  1.66it/s] 16%|█▌        | 31/200 [00:25<01:41,  1.66it/s] 16%|█▌        | 32/200 [00:26<01:40,  1.66it/s] 16%|█▋        | 33/200 [00:26<01:40,  1.67it/s] 17%|█▋        | 34/200 [00:27<01:39,  1.66it/s] 18%|█▊        | 35/200 [00:27<01:38,  1.67it/s] 18%|█▊        | 36/200 [00:28<01:38,  1.67it/s] 18%|█▊        | 37/200 [00:29<01:44,  1.56it/s] 19%|█▉        | 38/200 [00:29<01:42,  1.58it/s] 20%|█▉        | 39/200 [00:30<01:41,  1.59it/s] 20%|██        | 40/200 [00:31<01:39,  1.61it/s] 20%|██        | 41/200 [00:31<01:37,  1.63it/s] 21%|██        | 42/200 [00:32<01:37,  1.63it/s] 22%|██▏       | 43/200 [00:32<01:37,  1.61it/s] 22%|██▏       | 44/200 [00:33<01:37,  1.59it/s] 22%|██▎       | 45/200 [00:34<01:36,  1.61it/s] 23%|██▎       | 46/200 [00:34<01:33,  1.64it/s] 24%|██▎       | 47/200 [00:35<01:33,  1.63it/s] 24%|██▍       | 48/200 [00:35<01:33,  1.62it/s] 24%|██▍       | 49/200 [00:36<01:33,  1.61it/s] 25%|██▌       | 50/200 [00:37<01:31,  1.63it/s] 26%|██▌       | 51/200 [00:37<01:30,  1.64it/s] 26%|██▌       | 52/200 [00:38<01:31,  1.62it/s] 26%|██▋       | 53/200 [00:39<01:29,  1.64it/s] 27%|██▋       | 54/200 [00:39<01:28,  1.65it/s] 28%|██▊       | 55/200 [00:40<01:28,  1.64it/s] 28%|██▊       | 56/200 [00:40<01:26,  1.66it/s] 28%|██▊       | 57/200 [00:41<01:26,  1.65it/s] 29%|██▉       | 58/200 [00:42<01:25,  1.67it/s] 30%|██▉       | 59/200 [00:42<01:22,  1.71it/s] 30%|███       | 60/200 [00:43<01:21,  1.71it/s] 30%|███       | 61/200 [00:43<01:22,  1.68it/s] 31%|███       | 62/200 [00:44<01:25,  1.62it/s] 32%|███▏      | 63/200 [00:45<01:23,  1.64it/s] 32%|███▏      | 64/200 [00:45<01:23,  1.62it/s] 32%|███▎      | 65/200 [00:46<01:23,  1.63it/s] 33%|███▎      | 66/200 [00:46<01:22,  1.63it/s] 34%|███▎      | 67/200 [00:47<01:22,  1.61it/s] 34%|███▍      | 68/200 [00:48<01:22,  1.60it/s] 34%|███▍      | 69/200 [00:48<01:22,  1.59it/s] 35%|███▌      | 70/200 [00:49<01:22,  1.58it/s] 36%|███▌      | 71/200 [00:50<01:21,  1.58it/s] 36%|███▌      | 72/200 [00:50<01:20,  1.60it/s] 36%|███▋      | 73/200 [00:51<01:19,  1.59it/s] 37%|███▋      | 74/200 [00:51<01:19,  1.59it/s] 38%|███▊      | 75/200 [00:52<01:21,  1.53it/s] 38%|███▊      | 76/200 [00:53<01:18,  1.57it/s] 38%|███▊      | 77/200 [00:53<01:16,  1.61it/s] 39%|███▉      | 78/200 [00:54<01:15,  1.62it/s] 40%|███▉      | 79/200 [00:55<01:15,  1.61it/s] 40%|████      | 80/200 [00:55<01:13,  1.63it/s] 40%|████      | 81/200 [00:56<01:12,  1.65it/s] 41%|████      | 82/200 [00:56<01:10,  1.67it/s] 42%|████▏     | 83/200 [00:57<01:11,  1.65it/s] 42%|████▏     | 84/200 [00:58<01:11,  1.63it/s] 42%|████▎     | 85/200 [00:58<01:10,  1.62it/s] 43%|████▎     | 86/200 [00:59<01:09,  1.64it/s] 44%|████▎     | 87/200 [00:59<01:08,  1.65it/s] 44%|████▍     | 88/200 [01:00<01:08,  1.62it/s] 44%|████▍     | 89/200 [01:01<01:08,  1.63it/s] 45%|████▌     | 90/200 [01:01<01:08,  1.61it/s] 46%|████▌     | 91/200 [01:02<01:08,  1.60it/s] 46%|████▌     | 92/200 [01:03<01:07,  1.61it/s] 46%|████▋     | 93/200 [01:03<01:06,  1.62it/s] 47%|████▋     | 94/200 [01:04<01:04,  1.63it/s] 48%|████▊     | 95/200 [01:04<01:03,  1.65it/s] 48%|████▊     | 96/200 [01:05<01:02,  1.67it/s] 48%|████▊     | 97/200 [01:05<01:00,  1.70it/s] 49%|████▉     | 98/200 [01:06<01:00,  1.70it/s] 50%|████▉     | 99/200 [01:07<00:59,  1.68it/s] 50%|█████     | 100/200 [01:07<00:59,  1.69it/s] 50%|█████     | 101/200 [01:08<01:00,  1.64it/s] 51%|█████     | 102/200 [01:09<01:00,  1.62it/s] 52%|█████▏    | 103/200 [01:09<01:00,  1.60it/s] 52%|█████▏    | 104/200 [01:10<01:00,  1.60it/s] 52%|█████▎    | 105/200 [01:10<00:58,  1.62it/s] 53%|█████▎    | 106/200 [01:11<00:57,  1.64it/s] 54%|█████▎    | 107/200 [01:12<00:56,  1.65it/s] 54%|█████▍    | 108/200 [01:12<00:55,  1.65it/s] 55%|█████▍    | 109/200 [01:13<00:55,  1.64it/s] 55%|█████▌    | 110/200 [01:13<00:54,  1.64it/s] 56%|█████▌    | 111/200 [01:14<00:54,  1.64it/s] 56%|█████▌    | 112/200 [01:15<00:53,  1.64it/s] 56%|█████▋    | 113/200 [01:15<00:57,  1.50it/s] 57%|█████▋    | 114/200 [01:16<00:56,  1.53it/s] 57%|█████▊    | 115/200 [01:17<00:54,  1.57it/s] 58%|█████▊    | 116/200 [01:17<00:53,  1.58it/s] 58%|█████▊    | 117/200 [01:18<00:51,  1.60it/s] 59%|█████▉    | 118/200 [01:19<00:50,  1.61it/s] 60%|█████▉    | 119/200 [01:19<00:50,  1.62it/s] 60%|██████    | 120/200 [01:20<00:49,  1.62it/s] 60%|██████    | 121/200 [01:20<00:48,  1.62it/s] 61%|██████    | 122/200 [01:21<00:47,  1.63it/s] 62%|██████▏   | 123/200 [01:22<00:46,  1.64it/s] 62%|██████▏   | 124/200 [01:22<00:45,  1.66it/s] 62%|██████▎   | 125/200 [01:23<00:44,  1.67it/s] 63%|██████▎   | 126/200 [01:23<00:44,  1.66it/s] 64%|██████▎   | 127/200 [01:24<00:44,  1.65it/s] 64%|██████▍   | 128/200 [01:25<00:43,  1.65it/s] 64%|██████▍   | 129/200 [01:25<00:43,  1.64it/s] 65%|██████▌   | 130/200 [01:26<00:42,  1.63it/s] 66%|██████▌   | 131/200 [01:26<00:42,  1.61it/s] 66%|██████▌   | 132/200 [01:27<00:41,  1.63it/s] 66%|██████▋   | 133/200 [01:28<00:40,  1.65it/s] 67%|██████▋   | 134/200 [01:28<00:40,  1.64it/s] 68%|██████▊   | 135/200 [01:29<00:38,  1.70it/s] 68%|██████▊   | 136/200 [01:29<00:37,  1.70it/s] 68%|██████▊   | 137/200 [01:30<00:37,  1.69it/s] 69%|██████▉   | 138/200 [01:31<00:36,  1.68it/s] 70%|██████▉   | 139/200 [01:31<00:35,  1.70it/s] 70%|███████   | 140/200 [01:32<00:35,  1.70it/s] 70%|███████   | 141/200 [01:32<00:34,  1.70it/s] 71%|███████   | 142/200 [01:33<00:34,  1.70it/s] 72%|███████▏  | 143/200 [01:34<00:33,  1.69it/s] 72%|███████▏  | 144/200 [01:34<00:33,  1.69it/s] 72%|███████▎  | 145/200 [01:35<00:32,  1.69it/s] 73%|███████▎  | 146/200 [01:35<00:31,  1.69it/s] 74%|███████▎  | 147/200 [01:36<00:31,  1.69it/s] 74%|███████▍  | 148/200 [01:37<00:31,  1.66it/s] 74%|███████▍  | 149/200 [01:37<00:31,  1.63it/s] 75%|███████▌  | 150/200 [01:38<00:30,  1.64it/s] 76%|███████▌  | 151/200 [01:39<00:31,  1.54it/s] 76%|███████▌  | 152/200 [01:39<00:30,  1.55it/s] 76%|███████▋  | 153/200 [01:40<00:29,  1.57it/s] 77%|███████▋  | 154/200 [01:40<00:28,  1.61it/s] 78%|███████▊  | 155/200 [01:41<00:27,  1.61it/s] 78%|███████▊  | 156/200 [01:42<00:27,  1.60it/s] 78%|███████▊  | 157/200 [01:42<00:26,  1.61it/s] 79%|███████▉  | 158/200 [01:43<00:25,  1.63it/s] 80%|███████▉  | 159/200 [01:43<00:25,  1.64it/s] 80%|████████  | 160/200 [01:44<00:24,  1.63it/s] 80%|████████  | 161/200 [01:45<00:23,  1.63it/s] 81%|████████  | 162/200 [01:45<00:23,  1.63it/s] 82%|████████▏ | 163/200 [01:46<00:22,  1.65it/s] 82%|████████▏ | 164/200 [01:46<00:21,  1.65it/s] 82%|████████▎ | 165/200 [01:47<00:21,  1.65it/s] 83%|████████▎ | 166/200 [01:48<00:20,  1.65it/s] 84%|████████▎ | 167/200 [01:48<00:19,  1.66it/s] 84%|████████▍ | 168/200 [01:49<00:19,  1.65it/s] 84%|████████▍ | 169/200 [01:49<00:18,  1.66it/s] 85%|████████▌ | 170/200 [01:50<00:18,  1.66it/s] 86%|████████▌ | 171/200 [01:51<00:17,  1.65it/s] 86%|████████▌ | 172/200 [01:51<00:16,  1.66it/s] 86%|████████▋ | 173/200 [01:52<00:15,  1.71it/s] 87%|████████▋ | 174/200 [01:52<00:15,  1.66it/s] 88%|████████▊ | 175/200 [01:53<00:15,  1.64it/s] 88%|████████▊ | 176/200 [01:54<00:14,  1.66it/s] 88%|████████▊ | 177/200 [01:54<00:13,  1.65it/s] 89%|████████▉ | 178/200 [01:55<00:13,  1.62it/s] 90%|████████▉ | 179/200 [01:56<00:13,  1.60it/s] 90%|█████████ | 180/200 [01:56<00:12,  1.62it/s] 90%|█████████ | 181/200 [01:57<00:11,  1.64it/s] 91%|█████████ | 182/200 [01:57<00:11,  1.60it/s] 92%|█████████▏| 183/200 [01:58<00:10,  1.61it/s] 92%|█████████▏| 184/200 [01:59<00:09,  1.63it/s] 92%|█████████▎| 185/200 [01:59<00:09,  1.65it/s] 93%|█████████▎| 186/200 [02:00<00:08,  1.67it/s] 94%|█████████▎| 187/200 [02:00<00:07,  1.67it/s] 94%|█████████▍| 188/200 [02:01<00:07,  1.68it/s] 94%|█████████▍| 189/200 [02:02<00:06,  1.62it/s] 95%|█████████▌| 190/200 [02:02<00:06,  1.61it/s] 96%|█████████▌| 191/200 [02:03<00:05,  1.59it/s] 96%|█████████▌| 192/200 [02:04<00:04,  1.60it/s] 96%|█████████▋| 193/200 [02:04<00:04,  1.63it/s] 97%|█████████▋| 194/200 [02:05<00:03,  1.64it/s] 98%|█████████▊| 195/200 [02:05<00:03,  1.66it/s] 98%|█████████▊| 196/200 [02:06<00:02,  1.67it/s] 98%|█████████▊| 197/200 [02:07<00:01,  1.66it/s] 99%|█████████▉| 198/200 [02:07<00:01,  1.65it/s]100%|█████████▉| 199/200 [02:08<00:00,  1.66it/s]100%|██████████| 200/200 [02:08<00:00,  1.67it/s][INFO|trainer.py:3846] 2024-11-19 15:25:40,932 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-lora/checkpoint-200
[INFO|configuration_utils.py:690] 2024-11-19 15:25:40,943 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:25:40,943 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-19 15:25:40,947 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-lora/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 15:25:40,947 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-lora/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-19 15:25:40,999 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [02:08<00:00,  1.67it/s]100%|██████████| 200/200 [02:08<00:00,  1.55it/s]
[INFO|trainer.py:3846] 2024-11-19 15:25:41,001 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-lora
[INFO|configuration_utils.py:690] 2024-11-19 15:25:41,010 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:25:41,010 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-19 15:25:41,014 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 15:25:41,014 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-lora/special_tokens_map.json
{'train_runtime': 128.9228, 'train_samples_per_second': 24.821, 'train_steps_per_second': 1.551, 'train_loss': 3.2006109619140624, 'epoch': 0.03}
***** train metrics *****
  epoch                    =     0.0274
  total_flos               =  1562821GF
  train_loss               =     3.2006
  train_runtime            = 0:02:08.92
  train_samples            =     116582
  train_samples_per_second =     24.821
  train_steps_per_second   =      1.551
11/19/2024 15:25:41 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-19 15:25:41,059 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-19 15:25:41,059 >>   Num examples = 479
[INFO|trainer.py:4167] 2024-11-19 15:25:41,059 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/15 [00:00<?, ?it/s] 13%|█▎        | 2/15 [00:00<00:04,  2.84it/s] 20%|██        | 3/15 [00:01<00:05,  2.03it/s] 27%|██▋       | 4/15 [00:02<00:06,  1.78it/s] 33%|███▎      | 5/15 [00:02<00:06,  1.64it/s] 40%|████      | 6/15 [00:03<00:05,  1.57it/s] 47%|████▋     | 7/15 [00:04<00:05,  1.53it/s] 53%|█████▎    | 8/15 [00:04<00:04,  1.52it/s] 60%|██████    | 9/15 [00:05<00:03,  1.50it/s] 67%|██████▋   | 10/15 [00:06<00:03,  1.48it/s] 73%|███████▎  | 11/15 [00:06<00:02,  1.41it/s] 80%|████████  | 12/15 [00:07<00:02,  1.43it/s] 87%|████████▋ | 13/15 [00:08<00:01,  1.43it/s] 93%|█████████▎| 14/15 [00:09<00:00,  1.43it/s]100%|██████████| 15/15 [00:09<00:00,  1.47it/s]100%|██████████| 15/15 [00:10<00:00,  1.43it/s]
[INFO|modelcard.py:449] 2024-11-19 15:25:52,271 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.42210780442311185}]}
***** eval metrics *****
  epoch                   =     0.0274
  eval_accuracy           =     0.4221
  eval_loss               =     3.0747
  eval_runtime            = 0:00:11.21
  eval_samples            =        479
  eval_samples_per_second =     42.726
  eval_steps_per_second   =      1.338
  perplexity              =    21.6431
------------------------------------
AdaLoRA: Fine-tuning openai-community/gpt2 on wikitext-2-raw-v1 and wikitext-103-raw-v1 using baseline model
11/19/2024 15:25:57 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/19/2024 15:25:57 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm-transfer-gpt2-adalora/runs/Nov19_15-25-57_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm-transfer-gpt2-adalora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm-transfer-gpt2-adalora,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
11/19/2024 15:26:00 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:26:00 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:26:00 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:26:00 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Overwrite dataset info from restored data version if exists.
11/19/2024 15:26:01 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:26:01 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:26:01 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:26:01 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-19 15:26:01,919 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:26:01,920 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:26:01,930 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:26:01,930 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:26:01,930 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:26:01,930 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:26:01,930 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:26:01,930 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-19 15:26:02,038 >> loading weights file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-19 15:26:02,042 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-19 15:26:02,077 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-19 15:26:02,077 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-19 15:26:02,078 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-19 15:26:02,078 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/peft/tuners/adalora/model.py:205: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-0f713823a10cfe5c.arrow
11/19/2024 15:26:02 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-0f713823a10cfe5c.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-2b242ceb6911396d.arrow
11/19/2024 15:26:02 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-2b242ceb6911396d.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-1eaec44ca2a0bcf1.arrow
11/19/2024 15:26:02 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-1eaec44ca2a0bcf1.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-6f312b8bea1d6878.arrow
11/19/2024 15:26:02 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-6f312b8bea1d6878.arrow
[WARNING|trainer.py:664] 2024-11-19 15:26:03,953 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-19 15:26:04,109 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-19 15:26:04,109 >>   Num examples = 116,582
[INFO|trainer.py:2337] 2024-11-19 15:26:04,109 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-19 15:26:04,109 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-19 15:26:04,109 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-19 15:26:04,109 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-19 15:26:04,109 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-19 15:26:04,109 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-19 15:26:04,109 >>   Number of trainable parameters = 295,008
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:07<24:06,  7.27s/it]  1%|          | 2/200 [00:07<11:00,  3.34s/it]  2%|▏         | 3/200 [00:08<06:53,  2.10s/it]  2%|▏         | 4/200 [00:09<04:59,  1.53s/it]  2%|▎         | 5/200 [00:09<03:54,  1.20s/it]  3%|▎         | 6/200 [00:10<03:13,  1.00it/s]  4%|▎         | 7/200 [00:10<02:49,  1.14it/s]  4%|▍         | 8/200 [00:11<02:33,  1.25it/s]  4%|▍         | 9/200 [00:12<02:20,  1.36it/s]  5%|▌         | 10/200 [00:12<02:11,  1.45it/s]  6%|▌         | 11/200 [00:13<02:05,  1.51it/s]  6%|▌         | 12/200 [00:13<02:00,  1.56it/s]  6%|▋         | 13/200 [00:14<01:57,  1.59it/s]  7%|▋         | 14/200 [00:15<01:54,  1.62it/s]  8%|▊         | 15/200 [00:15<01:52,  1.64it/s]  8%|▊         | 16/200 [00:16<01:52,  1.64it/s]  8%|▊         | 17/200 [00:17<01:52,  1.62it/s]  9%|▉         | 18/200 [00:17<01:51,  1.64it/s] 10%|▉         | 19/200 [00:18<01:50,  1.64it/s] 10%|█         | 20/200 [00:19<02:03,  1.46it/s] 10%|█         | 21/200 [00:19<01:58,  1.50it/s] 11%|█         | 22/200 [00:20<01:54,  1.56it/s] 12%|█▏        | 23/200 [00:20<01:50,  1.60it/s] 12%|█▏        | 24/200 [00:21<01:50,  1.59it/s] 12%|█▎        | 25/200 [00:22<01:49,  1.59it/s] 13%|█▎        | 26/200 [00:22<01:48,  1.61it/s] 14%|█▎        | 27/200 [00:23<01:45,  1.63it/s] 14%|█▍        | 28/200 [00:23<01:43,  1.66it/s] 14%|█▍        | 29/200 [00:24<01:42,  1.66it/s] 15%|█▌        | 30/200 [00:25<01:41,  1.67it/s] 16%|█▌        | 31/200 [00:25<01:40,  1.68it/s] 16%|█▌        | 32/200 [00:26<01:39,  1.68it/s] 16%|█▋        | 33/200 [00:26<01:38,  1.69it/s] 17%|█▋        | 34/200 [00:27<01:37,  1.70it/s] 18%|█▊        | 35/200 [00:28<01:37,  1.69it/s] 18%|█▊        | 36/200 [00:28<01:36,  1.69it/s] 18%|█▊        | 37/200 [00:29<01:35,  1.70it/s] 19%|█▉        | 38/200 [00:29<01:39,  1.63it/s] 20%|█▉        | 39/200 [00:30<01:37,  1.65it/s] 20%|██        | 40/200 [00:31<01:36,  1.66it/s] 20%|██        | 41/200 [00:31<01:35,  1.67it/s] 21%|██        | 42/200 [00:32<01:34,  1.68it/s] 22%|██▏       | 43/200 [00:32<01:33,  1.69it/s] 22%|██▏       | 44/200 [00:33<01:32,  1.68it/s] 22%|██▎       | 45/200 [00:34<01:31,  1.69it/s] 23%|██▎       | 46/200 [00:34<01:31,  1.68it/s] 24%|██▎       | 47/200 [00:35<01:31,  1.68it/s] 24%|██▍       | 48/200 [00:35<01:30,  1.68it/s] 24%|██▍       | 49/200 [00:36<01:29,  1.68it/s] 25%|██▌       | 50/200 [00:37<01:28,  1.69it/s] 26%|██▌       | 51/200 [00:37<01:28,  1.69it/s] 26%|██▌       | 52/200 [00:38<01:27,  1.69it/s] 26%|██▋       | 53/200 [00:38<01:27,  1.69it/s] 27%|██▋       | 54/200 [00:39<01:26,  1.69it/s] 28%|██▊       | 55/200 [00:39<01:25,  1.69it/s] 28%|██▊       | 56/200 [00:40<01:25,  1.69it/s] 28%|██▊       | 57/200 [00:41<01:24,  1.69it/s] 29%|██▉       | 58/200 [00:41<01:27,  1.62it/s] 30%|██▉       | 59/200 [00:42<01:25,  1.64it/s] 30%|███       | 60/200 [00:43<01:25,  1.65it/s] 30%|███       | 61/200 [00:43<01:24,  1.64it/s] 31%|███       | 62/200 [00:44<01:25,  1.62it/s] 32%|███▏      | 63/200 [00:44<01:23,  1.64it/s] 32%|███▏      | 64/200 [00:45<01:22,  1.65it/s] 32%|███▎      | 65/200 [00:46<01:23,  1.62it/s] 33%|███▎      | 66/200 [00:46<01:23,  1.61it/s] 34%|███▎      | 67/200 [00:47<01:21,  1.63it/s] 34%|███▍      | 68/200 [00:47<01:20,  1.65it/s] 34%|███▍      | 69/200 [00:48<01:19,  1.65it/s] 35%|███▌      | 70/200 [00:49<01:18,  1.65it/s] 36%|███▌      | 71/200 [00:49<01:17,  1.67it/s] 36%|███▌      | 72/200 [00:50<01:16,  1.66it/s] 36%|███▋      | 73/200 [00:50<01:17,  1.65it/s] 37%|███▋      | 74/200 [00:51<01:16,  1.65it/s] 38%|███▊      | 75/200 [00:52<01:15,  1.65it/s] 38%|███▊      | 76/200 [00:52<01:15,  1.64it/s] 38%|███▊      | 77/200 [00:53<01:18,  1.56it/s] 39%|███▉      | 78/200 [00:54<01:16,  1.60it/s] 40%|███▉      | 79/200 [00:54<01:15,  1.61it/s] 40%|████      | 80/200 [00:55<01:15,  1.59it/s] 40%|████      | 81/200 [00:55<01:14,  1.61it/s] 41%|████      | 82/200 [00:56<01:12,  1.63it/s] 42%|████▏     | 83/200 [00:57<01:11,  1.63it/s] 42%|████▏     | 84/200 [00:57<01:12,  1.61it/s] 42%|████▎     | 85/200 [00:58<01:11,  1.61it/s] 43%|████▎     | 86/200 [00:59<01:10,  1.62it/s] 44%|████▎     | 87/200 [00:59<01:09,  1.63it/s] 44%|████▍     | 88/200 [01:00<01:08,  1.63it/s] 44%|████▍     | 89/200 [01:00<01:08,  1.63it/s] 45%|████▌     | 90/200 [01:01<01:06,  1.65it/s] 46%|████▌     | 91/200 [01:02<01:06,  1.65it/s] 46%|████▌     | 92/200 [01:02<01:05,  1.65it/s] 46%|████▋     | 93/200 [01:03<01:05,  1.63it/s] 47%|████▋     | 94/200 [01:03<01:05,  1.62it/s] 48%|████▊     | 95/200 [01:04<01:04,  1.64it/s] 48%|████▊     | 96/200 [01:05<01:05,  1.58it/s] 48%|████▊     | 97/200 [01:05<01:04,  1.60it/s] 49%|████▉     | 98/200 [01:06<01:02,  1.63it/s] 50%|████▉     | 99/200 [01:06<01:01,  1.63it/s] 50%|█████     | 100/200 [01:07<01:00,  1.64it/s] 50%|█████     | 101/200 [01:08<01:00,  1.64it/s] 51%|█████     | 102/200 [01:08<01:00,  1.63it/s] 52%|█████▏    | 103/200 [01:09<00:59,  1.63it/s] 52%|█████▏    | 104/200 [01:10<00:58,  1.64it/s] 52%|█████▎    | 105/200 [01:10<00:57,  1.64it/s] 53%|█████▎    | 106/200 [01:11<00:57,  1.65it/s] 54%|█████▎    | 107/200 [01:11<00:56,  1.63it/s] 54%|█████▍    | 108/200 [01:12<00:56,  1.64it/s] 55%|█████▍    | 109/200 [01:13<00:55,  1.65it/s] 55%|█████▌    | 110/200 [01:13<00:54,  1.66it/s] 56%|█████▌    | 111/200 [01:14<00:53,  1.67it/s] 56%|█████▌    | 112/200 [01:14<00:52,  1.68it/s] 56%|█████▋    | 113/200 [01:15<00:51,  1.68it/s] 57%|█████▋    | 114/200 [01:16<00:50,  1.69it/s] 57%|█████▊    | 115/200 [01:16<00:50,  1.69it/s] 58%|█████▊    | 116/200 [01:17<00:50,  1.68it/s] 58%|█████▊    | 117/200 [01:17<00:50,  1.64it/s] 59%|█████▉    | 118/200 [01:18<00:49,  1.64it/s] 60%|█████▉    | 119/200 [01:19<00:47,  1.69it/s] 60%|██████    | 120/200 [01:19<00:47,  1.68it/s] 60%|██████    | 121/200 [01:20<00:47,  1.67it/s] 61%|██████    | 122/200 [01:20<00:46,  1.67it/s] 62%|██████▏   | 123/200 [01:21<00:45,  1.68it/s] 62%|██████▏   | 124/200 [01:22<00:45,  1.68it/s] 62%|██████▎   | 125/200 [01:22<00:45,  1.65it/s] 63%|██████▎   | 126/200 [01:23<00:44,  1.66it/s] 64%|██████▎   | 127/200 [01:23<00:43,  1.66it/s] 64%|██████▍   | 128/200 [01:24<00:43,  1.67it/s] 64%|██████▍   | 129/200 [01:25<00:42,  1.68it/s] 65%|██████▌   | 130/200 [01:25<00:41,  1.69it/s] 66%|██████▌   | 131/200 [01:26<00:40,  1.69it/s] 66%|██████▌   | 132/200 [01:26<00:40,  1.69it/s] 66%|██████▋   | 133/200 [01:27<00:39,  1.69it/s] 67%|██████▋   | 134/200 [01:27<00:39,  1.69it/s] 68%|██████▊   | 135/200 [01:28<00:41,  1.58it/s] 68%|██████▊   | 136/200 [01:29<00:40,  1.59it/s] 68%|██████▊   | 137/200 [01:29<00:39,  1.60it/s] 69%|██████▉   | 138/200 [01:30<00:38,  1.61it/s] 70%|██████▉   | 139/200 [01:31<00:37,  1.61it/s] 70%|███████   | 140/200 [01:31<00:37,  1.61it/s] 70%|███████   | 141/200 [01:32<00:36,  1.62it/s] 71%|███████   | 142/200 [01:32<00:35,  1.64it/s] 72%|███████▏  | 143/200 [01:33<00:35,  1.63it/s] 72%|███████▏  | 144/200 [01:34<00:34,  1.64it/s] 72%|███████▎  | 145/200 [01:34<00:33,  1.66it/s] 73%|███████▎  | 146/200 [01:35<00:32,  1.68it/s] 74%|███████▎  | 147/200 [01:35<00:31,  1.68it/s] 74%|███████▍  | 148/200 [01:36<00:30,  1.68it/s] 74%|███████▍  | 149/200 [01:37<00:30,  1.68it/s] 75%|███████▌  | 150/200 [01:37<00:29,  1.68it/s] 76%|███████▌  | 151/200 [01:38<00:28,  1.69it/s] 76%|███████▌  | 152/200 [01:38<00:28,  1.68it/s] 76%|███████▋  | 153/200 [01:39<00:28,  1.65it/s] 77%|███████▋  | 154/200 [01:40<00:27,  1.66it/s] 78%|███████▊  | 155/200 [01:40<00:27,  1.66it/s] 78%|███████▊  | 156/200 [01:41<00:28,  1.54it/s] 78%|███████▊  | 157/200 [01:42<00:27,  1.58it/s] 79%|███████▉  | 158/200 [01:42<00:25,  1.62it/s] 80%|███████▉  | 159/200 [01:43<00:24,  1.64it/s] 80%|████████  | 160/200 [01:43<00:24,  1.66it/s] 80%|████████  | 161/200 [01:44<00:23,  1.65it/s] 81%|████████  | 162/200 [01:45<00:23,  1.62it/s] 82%|████████▏ | 163/200 [01:45<00:22,  1.63it/s] 82%|████████▏ | 164/200 [01:46<00:21,  1.64it/s] 82%|████████▎ | 165/200 [01:46<00:21,  1.66it/s] 83%|████████▎ | 166/200 [01:47<00:20,  1.67it/s] 84%|████████▎ | 167/200 [01:48<00:19,  1.68it/s] 84%|████████▍ | 168/200 [01:48<00:19,  1.67it/s] 84%|████████▍ | 169/200 [01:49<00:18,  1.66it/s] 85%|████████▌ | 170/200 [01:49<00:18,  1.65it/s] 86%|████████▌ | 171/200 [01:50<00:17,  1.63it/s] 86%|████████▌ | 172/200 [01:51<00:17,  1.63it/s] 86%|████████▋ | 173/200 [01:51<00:16,  1.62it/s] 87%|████████▋ | 174/200 [01:52<00:16,  1.55it/s] 88%|████████▊ | 175/200 [01:53<00:15,  1.59it/s] 88%|████████▊ | 176/200 [01:53<00:15,  1.59it/s] 88%|████████▊ | 177/200 [01:54<00:14,  1.62it/s] 89%|████████▉ | 178/200 [01:54<00:13,  1.64it/s] 90%|████████▉ | 179/200 [01:55<00:12,  1.64it/s] 90%|█████████ | 180/200 [01:56<00:12,  1.64it/s] 90%|█████████ | 181/200 [01:56<00:11,  1.66it/s] 91%|█████████ | 182/200 [01:57<00:10,  1.67it/s] 92%|█████████▏| 183/200 [01:57<00:10,  1.64it/s] 92%|█████████▏| 184/200 [01:58<00:09,  1.64it/s] 92%|█████████▎| 185/200 [01:59<00:09,  1.66it/s] 93%|█████████▎| 186/200 [01:59<00:08,  1.62it/s] 94%|█████████▎| 187/200 [02:00<00:08,  1.60it/s] 94%|█████████▍| 188/200 [02:01<00:07,  1.61it/s] 94%|█████████▍| 189/200 [02:01<00:06,  1.63it/s] 95%|█████████▌| 190/200 [02:02<00:06,  1.63it/s] 96%|█████████▌| 191/200 [02:02<00:05,  1.61it/s] 96%|█████████▌| 192/200 [02:03<00:04,  1.62it/s] 96%|█████████▋| 193/200 [02:04<00:04,  1.63it/s] 97%|█████████▋| 194/200 [02:04<00:03,  1.64it/s] 98%|█████████▊| 195/200 [02:05<00:03,  1.52it/s] 98%|█████████▊| 196/200 [02:06<00:02,  1.56it/s] 98%|█████████▊| 197/200 [02:06<00:01,  1.56it/s] 99%|█████████▉| 198/200 [02:07<00:01,  1.59it/s]100%|█████████▉| 199/200 [02:07<00:00,  1.60it/s]100%|██████████| 200/200 [02:08<00:00,  1.60it/s][INFO|trainer.py:3846] 2024-11-19 15:28:12,664 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-adalora/checkpoint-200
[INFO|configuration_utils.py:690] 2024-11-19 15:28:12,676 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:28:12,676 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-19 15:28:12,680 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-adalora/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 15:28:12,680 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-adalora/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-19 15:28:12,734 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [02:08<00:00,  1.60it/s]100%|██████████| 200/200 [02:08<00:00,  1.55it/s]
[INFO|trainer.py:3846] 2024-11-19 15:28:12,736 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-adalora
[INFO|configuration_utils.py:690] 2024-11-19 15:28:12,746 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:28:12,746 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-19 15:28:12,750 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-adalora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 15:28:12,750 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-adalora/special_tokens_map.json
{'train_runtime': 128.6247, 'train_samples_per_second': 24.879, 'train_steps_per_second': 1.555, 'train_loss': 3.201134033203125, 'epoch': 0.03}
***** train metrics *****
  epoch                    =     0.0274
  total_flos               =  1562823GF
  train_loss               =     3.2011
  train_runtime            = 0:02:08.62
  train_samples            =     116582
  train_samples_per_second =     24.879
  train_steps_per_second   =      1.555
11/19/2024 15:28:12 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-19 15:28:12,796 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-19 15:28:12,796 >>   Num examples = 479
[INFO|trainer.py:4167] 2024-11-19 15:28:12,796 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/15 [00:00<?, ?it/s] 13%|█▎        | 2/15 [00:00<00:04,  2.82it/s] 20%|██        | 3/15 [00:01<00:06,  1.98it/s] 27%|██▋       | 4/15 [00:02<00:06,  1.68it/s] 33%|███▎      | 5/15 [00:02<00:06,  1.57it/s] 40%|████      | 6/15 [00:03<00:05,  1.53it/s] 47%|████▋     | 7/15 [00:04<00:05,  1.49it/s] 53%|█████▎    | 8/15 [00:04<00:04,  1.47it/s] 60%|██████    | 9/15 [00:05<00:04,  1.44it/s] 67%|██████▋   | 10/15 [00:06<00:03,  1.43it/s] 73%|███████▎  | 11/15 [00:07<00:02,  1.41it/s] 80%|████████  | 12/15 [00:07<00:02,  1.39it/s] 87%|████████▋ | 13/15 [00:08<00:01,  1.39it/s] 93%|█████████▎| 14/15 [00:09<00:00,  1.38it/s]100%|██████████| 15/15 [00:10<00:00,  1.41it/s]100%|██████████| 15/15 [00:10<00:00,  1.38it/s]
[INFO|modelcard.py:449] 2024-11-19 15:28:24,439 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.42211800815073763}]}
***** eval metrics *****
  epoch                   =     0.0274
  eval_accuracy           =     0.4221
  eval_loss               =     3.2144
  eval_runtime            = 0:00:11.64
  eval_samples            =        479
  eval_samples_per_second =     41.145
  eval_steps_per_second   =      1.288
  perplexity              =    24.8886
------------------------------------
LagEmbed: Training openai-community/gpt2 on wikitext-2-raw-v1 and wikitext-103-raw-v1 with LagEmbed (in_channels=768, n_components=2, dof=2)
11/19/2024 15:28:30 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/19/2024 15:28:30 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm-transfer-gpt2-lagembed/runs/Nov19_15-28-30_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm-transfer-gpt2-lagembed,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm-transfer-gpt2-lagembed,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
11/19/2024 15:28:32 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:28:32 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:28:32 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:28:32 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Overwrite dataset info from restored data version if exists.
11/19/2024 15:28:33 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:28:33 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:28:33 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:28:33 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-19 15:28:33,646 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:28:33,647 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:28:33,654 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:28:33,655 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:28:33,655 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:28:33,655 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:28:33,655 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:28:33,655 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-19 15:28:33,761 >> loading weights file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-19 15:28:33,765 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-19 15:28:33,814 >> All model checkpoint weights were used when initializing LagGPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-19 15:28:33,814 >> All the weights of LagGPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LagGPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-19 15:28:33,815 >> loading configuration file /tmp/test-clm-gpt2-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-19 15:28:33,815 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-0f713823a10cfe5c.arrow
11/19/2024 15:28:33 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-0f713823a10cfe5c.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-2b242ceb6911396d.arrow
11/19/2024 15:28:33 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-2b242ceb6911396d.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-1eaec44ca2a0bcf1.arrow
11/19/2024 15:28:33 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-1eaec44ca2a0bcf1.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-6f312b8bea1d6878.arrow
11/19/2024 15:28:33 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-6f312b8bea1d6878.arrow
[WARNING|trainer.py:664] 2024-11-19 15:28:35,706 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-19 15:28:35,831 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-19 15:28:35,832 >>   Num examples = 116,582
[INFO|trainer.py:2337] 2024-11-19 15:28:35,832 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-19 15:28:35,832 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:2340] 2024-11-19 15:28:35,832 >>   Training with DataParallel so batch size has been adjusted to: 32
[INFO|trainer.py:2341] 2024-11-19 15:28:35,832 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2342] 2024-11-19 15:28:35,832 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-19 15:28:35,832 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-19 15:28:35,832 >>   Number of trainable parameters = 202,566
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:07<24:06,  7.27s/it]  1%|          | 2/200 [00:08<11:24,  3.45s/it]  2%|▏         | 3/200 [00:08<07:26,  2.26s/it]  2%|▏         | 4/200 [00:09<05:31,  1.69s/it]  2%|▎         | 5/200 [00:10<04:25,  1.36s/it]  3%|▎         | 6/200 [00:11<03:45,  1.16s/it]  4%|▎         | 7/200 [00:12<03:20,  1.04s/it]  4%|▍         | 8/200 [00:12<03:05,  1.03it/s]  4%|▍         | 9/200 [00:13<02:57,  1.08it/s]  5%|▌         | 10/200 [00:14<02:50,  1.12it/s]  6%|▌         | 11/200 [00:15<02:42,  1.16it/s]  6%|▌         | 12/200 [00:16<02:38,  1.19it/s]  6%|▋         | 13/200 [00:16<02:34,  1.21it/s]  7%|▋         | 14/200 [00:17<02:33,  1.22it/s]  8%|▊         | 15/200 [00:18<02:31,  1.22it/s]  8%|▊         | 16/200 [00:19<02:29,  1.23it/s]  8%|▊         | 17/200 [00:20<02:29,  1.23it/s]  9%|▉         | 18/200 [00:20<02:28,  1.22it/s] 10%|▉         | 19/200 [00:21<02:27,  1.23it/s] 10%|█         | 20/200 [00:22<02:24,  1.24it/s] 10%|█         | 21/200 [00:23<02:22,  1.25it/s] 11%|█         | 22/200 [00:24<02:20,  1.27it/s] 12%|█▏        | 23/200 [00:24<02:19,  1.27it/s] 12%|█▏        | 24/200 [00:25<02:18,  1.27it/s] 12%|█▎        | 25/200 [00:26<02:17,  1.27it/s] 13%|█▎        | 26/200 [00:27<02:16,  1.28it/s] 14%|█▎        | 27/200 [00:28<02:15,  1.28it/s] 14%|█▍        | 28/200 [00:28<02:15,  1.27it/s] 14%|█▍        | 29/200 [00:29<02:16,  1.25it/s] 15%|█▌        | 30/200 [00:30<02:15,  1.26it/s] 16%|█▌        | 31/200 [00:31<02:15,  1.25it/s] 16%|█▌        | 32/200 [00:32<02:15,  1.24it/s] 16%|█▋        | 33/200 [00:32<02:14,  1.24it/s] 17%|█▋        | 34/200 [00:33<02:16,  1.22it/s] 18%|█▊        | 35/200 [00:34<02:14,  1.22it/s] 18%|█▊        | 36/200 [00:35<02:13,  1.23it/s] 18%|█▊        | 37/200 [00:36<02:12,  1.23it/s] 19%|█▉        | 38/200 [00:36<02:11,  1.24it/s] 20%|█▉        | 39/200 [00:37<02:09,  1.25it/s] 20%|██        | 40/200 [00:38<02:08,  1.25it/s] 20%|██        | 41/200 [00:39<02:06,  1.26it/s] 21%|██        | 42/200 [00:40<02:04,  1.26it/s] 22%|██▏       | 43/200 [00:41<02:11,  1.20it/s] 22%|██▏       | 44/200 [00:41<02:07,  1.22it/s] 22%|██▎       | 45/200 [00:42<02:07,  1.21it/s] 23%|██▎       | 46/200 [00:43<02:04,  1.24it/s] 24%|██▎       | 47/200 [00:44<02:02,  1.25it/s] 24%|██▍       | 48/200 [00:45<02:03,  1.23it/s] 24%|██▍       | 49/200 [00:45<02:02,  1.23it/s] 25%|██▌       | 50/200 [00:46<02:02,  1.23it/s] 26%|██▌       | 51/200 [00:47<02:01,  1.23it/s] 26%|██▌       | 52/200 [00:48<01:59,  1.24it/s] 26%|██▋       | 53/200 [00:49<01:58,  1.24it/s] 27%|██▋       | 54/200 [00:49<01:56,  1.26it/s] 28%|██▊       | 55/200 [00:50<01:55,  1.26it/s] 28%|██▊       | 56/200 [00:51<01:54,  1.26it/s] 28%|██▊       | 57/200 [00:52<01:53,  1.26it/s] 29%|██▉       | 58/200 [00:53<01:53,  1.25it/s] 30%|██▉       | 59/200 [00:53<01:53,  1.24it/s] 30%|███       | 60/200 [00:54<01:51,  1.26it/s] 30%|███       | 61/200 [00:55<01:50,  1.26it/s] 31%|███       | 62/200 [00:56<01:48,  1.27it/s] 32%|███▏      | 63/200 [00:56<01:47,  1.27it/s] 32%|███▏      | 64/200 [00:57<01:46,  1.28it/s] 32%|███▎      | 65/200 [00:58<01:46,  1.26it/s] 33%|███▎      | 66/200 [00:59<01:46,  1.26it/s] 34%|███▎      | 67/200 [01:00<01:45,  1.27it/s] 34%|███▍      | 68/200 [01:00<01:44,  1.26it/s] 34%|███▍      | 69/200 [01:01<01:45,  1.24it/s] 35%|███▌      | 70/200 [01:02<01:44,  1.24it/s] 36%|███▌      | 71/200 [01:03<01:43,  1.24it/s] 36%|███▌      | 72/200 [01:04<01:42,  1.25it/s] 36%|███▋      | 73/200 [01:04<01:40,  1.26it/s] 37%|███▋      | 74/200 [01:05<01:41,  1.24it/s] 38%|███▊      | 75/200 [01:06<01:39,  1.25it/s] 38%|███▊      | 76/200 [01:07<01:38,  1.25it/s] 38%|███▊      | 77/200 [01:08<01:39,  1.24it/s] 39%|███▉      | 78/200 [01:08<01:38,  1.24it/s] 40%|███▉      | 79/200 [01:09<01:36,  1.25it/s] 40%|████      | 80/200 [01:10<01:35,  1.26it/s] 40%|████      | 81/200 [01:11<01:42,  1.16it/s] 41%|████      | 82/200 [01:12<01:39,  1.19it/s] 42%|████▏     | 83/200 [01:13<01:37,  1.20it/s] 42%|████▏     | 84/200 [01:13<01:35,  1.22it/s] 42%|████▎     | 85/200 [01:14<01:32,  1.24it/s] 43%|████▎     | 86/200 [01:15<01:32,  1.23it/s] 44%|████▎     | 87/200 [01:16<01:31,  1.24it/s] 44%|████▍     | 88/200 [01:17<01:30,  1.24it/s] 44%|████▍     | 89/200 [01:18<01:30,  1.23it/s] 45%|████▌     | 90/200 [01:18<01:29,  1.23it/s] 46%|████▌     | 91/200 [01:19<01:28,  1.23it/s] 46%|████▌     | 92/200 [01:20<01:27,  1.24it/s] 46%|████▋     | 93/200 [01:21<01:25,  1.25it/s] 47%|████▋     | 94/200 [01:21<01:24,  1.26it/s] 48%|████▊     | 95/200 [01:22<01:22,  1.27it/s] 48%|████▊     | 96/200 [01:23<01:21,  1.28it/s] 48%|████▊     | 97/200 [01:24<01:20,  1.28it/s] 49%|████▉     | 98/200 [01:25<01:19,  1.29it/s] 50%|████▉     | 99/200 [01:25<01:18,  1.29it/s] 50%|█████     | 100/200 [01:26<01:18,  1.28it/s] 50%|█████     | 101/200 [01:27<01:17,  1.28it/s] 51%|█████     | 102/200 [01:28<01:16,  1.27it/s] 52%|█████▏    | 103/200 [01:28<01:15,  1.28it/s] 52%|█████▏    | 104/200 [01:29<01:17,  1.23it/s] 52%|█████▎    | 105/200 [01:30<01:17,  1.23it/s] 53%|█████▎    | 106/200 [01:31<01:17,  1.22it/s] 54%|█████▎    | 107/200 [01:32<01:15,  1.23it/s] 54%|█████▍    | 108/200 [01:33<01:14,  1.24it/s] 55%|█████▍    | 109/200 [01:33<01:14,  1.22it/s] 55%|█████▌    | 110/200 [01:34<01:13,  1.22it/s] 56%|█████▌    | 111/200 [01:35<01:12,  1.23it/s] 56%|█████▌    | 112/200 [01:36<01:11,  1.23it/s] 56%|█████▋    | 113/200 [01:37<01:09,  1.25it/s] 57%|█████▋    | 114/200 [01:37<01:09,  1.24it/s] 57%|█████▊    | 115/200 [01:38<01:08,  1.23it/s] 58%|█████▊    | 116/200 [01:39<01:08,  1.22it/s] 58%|█████▊    | 117/200 [01:40<01:08,  1.21it/s] 59%|█████▉    | 118/200 [01:41<01:07,  1.22it/s] 60%|█████▉    | 119/200 [01:42<01:05,  1.23it/s] 60%|██████    | 120/200 [01:42<01:04,  1.24it/s] 60%|██████    | 121/200 [01:43<01:03,  1.24it/s] 61%|██████    | 122/200 [01:44<01:02,  1.24it/s] 62%|██████▏   | 123/200 [01:45<01:02,  1.22it/s] 62%|██████▏   | 124/200 [01:46<01:01,  1.23it/s] 62%|██████▎   | 125/200 [01:46<01:01,  1.23it/s] 63%|██████▎   | 126/200 [01:47<01:00,  1.22it/s] 64%|██████▎   | 127/200 [01:48<01:00,  1.21it/s] 64%|██████▍   | 128/200 [01:49<00:59,  1.21it/s] 64%|██████▍   | 129/200 [01:50<00:58,  1.22it/s] 65%|██████▌   | 130/200 [01:51<00:57,  1.22it/s] 66%|██████▌   | 131/200 [01:51<00:56,  1.21it/s] 66%|██████▌   | 132/200 [01:52<00:55,  1.22it/s] 66%|██████▋   | 133/200 [01:53<00:54,  1.22it/s] 67%|██████▋   | 134/200 [01:54<00:53,  1.23it/s] 68%|██████▊   | 135/200 [01:55<00:52,  1.25it/s] 68%|██████▊   | 136/200 [01:55<00:51,  1.24it/s] 68%|██████▊   | 137/200 [01:56<00:50,  1.25it/s] 69%|██████▉   | 138/200 [01:57<00:49,  1.24it/s] 70%|██████▉   | 139/200 [01:58<00:49,  1.23it/s] 70%|███████   | 140/200 [01:59<00:48,  1.24it/s] 70%|███████   | 141/200 [01:59<00:47,  1.25it/s] 71%|███████   | 142/200 [02:00<00:47,  1.23it/s] 72%|███████▏  | 143/200 [02:01<00:46,  1.23it/s] 72%|███████▏  | 144/200 [02:02<00:45,  1.24it/s] 72%|███████▎  | 145/200 [02:03<00:44,  1.25it/s] 73%|███████▎  | 146/200 [02:03<00:43,  1.25it/s] 74%|███████▎  | 147/200 [02:04<00:42,  1.25it/s] 74%|███████▍  | 148/200 [02:05<00:41,  1.25it/s] 74%|███████▍  | 149/200 [02:06<00:41,  1.24it/s] 75%|███████▌  | 150/200 [02:07<00:40,  1.23it/s] 76%|███████▌  | 151/200 [02:08<00:39,  1.24it/s] 76%|███████▌  | 152/200 [02:08<00:38,  1.25it/s] 76%|███████▋  | 153/200 [02:09<00:39,  1.19it/s] 77%|███████▋  | 154/200 [02:10<00:37,  1.22it/s] 78%|███████▊  | 155/200 [02:11<00:36,  1.23it/s] 78%|███████▊  | 156/200 [02:12<00:35,  1.23it/s] 78%|███████▊  | 157/200 [02:12<00:35,  1.23it/s] 79%|███████▉  | 158/200 [02:13<00:33,  1.24it/s] 80%|███████▉  | 159/200 [02:14<00:32,  1.25it/s] 80%|████████  | 160/200 [02:15<00:32,  1.24it/s] 80%|████████  | 161/200 [02:16<00:31,  1.22it/s] 81%|████████  | 162/200 [02:16<00:31,  1.22it/s] 82%|████████▏ | 163/200 [02:17<00:30,  1.23it/s] 82%|████████▏ | 164/200 [02:18<00:29,  1.22it/s] 82%|████████▎ | 165/200 [02:19<00:28,  1.22it/s] 83%|████████▎ | 166/200 [02:20<00:27,  1.23it/s] 84%|████████▎ | 167/200 [02:21<00:26,  1.24it/s] 84%|████████▍ | 168/200 [02:21<00:25,  1.23it/s] 84%|████████▍ | 169/200 [02:22<00:25,  1.24it/s] 85%|████████▌ | 170/200 [02:23<00:24,  1.21it/s] 86%|████████▌ | 171/200 [02:24<00:24,  1.20it/s] 86%|████████▌ | 172/200 [02:25<00:23,  1.21it/s] 86%|████████▋ | 173/200 [02:26<00:22,  1.20it/s] 87%|████████▋ | 174/200 [02:26<00:21,  1.20it/s] 88%|████████▊ | 175/200 [02:27<00:20,  1.22it/s] 88%|████████▊ | 176/200 [02:28<00:19,  1.22it/s] 88%|████████▊ | 177/200 [02:29<00:18,  1.21it/s] 89%|████████▉ | 178/200 [02:30<00:18,  1.22it/s] 90%|████████▉ | 179/200 [02:30<00:17,  1.20it/s] 90%|█████████ | 180/200 [02:31<00:16,  1.22it/s] 90%|█████████ | 181/200 [02:32<00:15,  1.20it/s] 91%|█████████ | 182/200 [02:33<00:14,  1.20it/s] 92%|█████████▏| 183/200 [02:34<00:13,  1.22it/s] 92%|█████████▏| 184/200 [02:35<00:13,  1.22it/s] 92%|█████████▎| 185/200 [02:35<00:12,  1.23it/s] 93%|█████████▎| 186/200 [02:36<00:11,  1.25it/s] 94%|█████████▎| 187/200 [02:37<00:10,  1.25it/s] 94%|█████████▍| 188/200 [02:38<00:09,  1.26it/s] 94%|█████████▍| 189/200 [02:39<00:08,  1.26it/s] 95%|█████████▌| 190/200 [02:39<00:07,  1.26it/s] 96%|█████████▌| 191/200 [02:40<00:07,  1.26it/s] 96%|█████████▌| 192/200 [02:41<00:06,  1.26it/s] 96%|█████████▋| 193/200 [02:42<00:05,  1.26it/s] 97%|█████████▋| 194/200 [02:42<00:04,  1.26it/s] 98%|█████████▊| 195/200 [02:43<00:03,  1.26it/s] 98%|█████████▊| 196/200 [02:44<00:03,  1.27it/s] 98%|█████████▊| 197/200 [02:45<00:02,  1.26it/s] 99%|█████████▉| 198/200 [02:46<00:01,  1.25it/s]100%|█████████▉| 199/200 [02:46<00:00,  1.26it/s]100%|██████████| 200/200 [02:47<00:00,  1.25it/s][INFO|trainer.py:3846] 2024-11-19 15:31:23,597 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-lagembed/checkpoint-200
[INFO|configuration_utils.py:416] 2024-11-19 15:31:23,598 >> Configuration saved in /tmp/test-clm-transfer-gpt2-lagembed/checkpoint-200/config.json
[INFO|configuration_utils.py:873] 2024-11-19 15:31:23,598 >> Configuration saved in /tmp/test-clm-transfer-gpt2-lagembed/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-19 15:31:24,649 >> Model weights saved in /tmp/test-clm-transfer-gpt2-lagembed/checkpoint-200/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-19 15:31:24,651 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-lagembed/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 15:31:24,651 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-lagembed/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-19 15:31:24,705 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [02:48<00:00,  1.25it/s]100%|██████████| 200/200 [02:48<00:00,  1.18it/s]
[INFO|trainer.py:3846] 2024-11-19 15:31:24,708 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-lagembed
[INFO|configuration_utils.py:416] 2024-11-19 15:31:24,709 >> Configuration saved in /tmp/test-clm-transfer-gpt2-lagembed/config.json
[INFO|configuration_utils.py:873] 2024-11-19 15:31:24,709 >> Configuration saved in /tmp/test-clm-transfer-gpt2-lagembed/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-19 15:31:25,750 >> Model weights saved in /tmp/test-clm-transfer-gpt2-lagembed/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-19 15:31:25,751 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-lagembed/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 15:31:25,752 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-lagembed/special_tokens_map.json
{'train_runtime': 168.8737, 'train_samples_per_second': 37.898, 'train_steps_per_second': 1.184, 'train_loss': 3.0729861450195313, 'epoch': 0.05}
***** train metrics *****
  epoch                    =     0.0549
  total_flos               =  3122261GF
  train_loss               =      3.073
  train_runtime            = 0:02:48.87
  train_samples            =     116582
  train_samples_per_second =     37.898
  train_steps_per_second   =      1.184
11/19/2024 15:31:25 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-19 15:31:25,799 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-19 15:31:25,799 >>   Num examples = 479
[INFO|trainer.py:4167] 2024-11-19 15:31:25,799 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/15 [00:00<?, ?it/s] 13%|█▎        | 2/15 [00:00<00:04,  2.83it/s] 20%|██        | 3/15 [00:01<00:06,  1.95it/s] 27%|██▋       | 4/15 [00:02<00:06,  1.69it/s] 33%|███▎      | 5/15 [00:02<00:06,  1.56it/s] 40%|████      | 6/15 [00:03<00:05,  1.52it/s] 47%|████▋     | 7/15 [00:04<00:05,  1.49it/s] 53%|█████▎    | 8/15 [00:04<00:04,  1.46it/s] 60%|██████    | 9/15 [00:05<00:04,  1.44it/s] 67%|██████▋   | 10/15 [00:06<00:03,  1.43it/s] 73%|███████▎  | 11/15 [00:07<00:02,  1.43it/s] 80%|████████  | 12/15 [00:07<00:02,  1.42it/s] 87%|████████▋ | 13/15 [00:08<00:01,  1.42it/s] 93%|█████████▎| 14/15 [00:09<00:00,  1.41it/s]100%|██████████| 15/15 [00:10<00:00,  1.37it/s]100%|██████████| 15/15 [00:10<00:00,  1.38it/s]
[INFO|modelcard.py:449] 2024-11-19 15:31:37,368 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.4229241026331739}]}
***** eval metrics *****
  epoch                   =     0.0549
  eval_accuracy           =     0.4229
  eval_loss               =     3.0702
  eval_runtime            = 0:00:11.56
  eval_samples            =        479
  eval_samples_per_second =     41.407
  eval_steps_per_second   =      1.297
  perplexity              =    21.5469
------------------------------------
LoRA: Fine-tuning openai-community/gpt2-medium on wikitext-2-raw-v1 and wikitext-103-raw-v1 using baseline model
11/19/2024 15:31:43 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/19/2024 15:31:43 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm-transfer-gpt2-medium-lora/runs/Nov19_15-31-42_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm-transfer-gpt2-medium-lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm-transfer-gpt2-medium-lora,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
11/19/2024 15:31:45 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:31:45 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:31:45 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:31:45 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Overwrite dataset info from restored data version if exists.
11/19/2024 15:31:46 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:31:46 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:31:46 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:31:46 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-19 15:31:46,293 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:31:46,294 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:31:46,305 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:31:46,305 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:31:46,305 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:31:46,305 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:31:46,305 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:31:46,305 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-19 15:31:46,415 >> loading weights file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-19 15:31:46,421 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-19 15:31:46,483 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-19 15:31:46,483 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-19 15:31:46,485 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-19 15:31:46,485 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-15b981d832943d08.arrow
11/19/2024 15:31:47 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-15b981d832943d08.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-55124ae0e62b76c7.arrow
11/19/2024 15:31:47 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-55124ae0e62b76c7.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-78d8587b065121b6.arrow
11/19/2024 15:31:47 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-78d8587b065121b6.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-07e11c46e56da03e.arrow
11/19/2024 15:31:47 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-07e11c46e56da03e.arrow
[WARNING|trainer.py:664] 2024-11-19 15:31:49,637 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-19 15:31:49,766 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-19 15:31:49,766 >>   Num examples = 116,582
[INFO|trainer.py:2337] 2024-11-19 15:31:49,766 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-19 15:31:49,766 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-19 15:31:49,766 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-19 15:31:49,766 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-19 15:31:49,766 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-19 15:31:49,766 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-19 15:31:49,766 >>   Number of trainable parameters = 393,216
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:08<27:01,  8.15s/it]  1%|          | 2/200 [00:09<13:38,  4.14s/it]  2%|▏         | 3/200 [00:10<09:23,  2.86s/it]  2%|▏         | 4/200 [00:12<07:20,  2.25s/it]  2%|▎         | 5/200 [00:13<06:16,  1.93s/it]  3%|▎         | 6/200 [00:14<05:37,  1.74s/it]  4%|▎         | 7/200 [00:16<05:09,  1.60s/it]  4%|▍         | 8/200 [00:17<04:50,  1.52s/it]  4%|▍         | 9/200 [00:18<04:37,  1.46s/it]  5%|▌         | 10/200 [00:20<04:30,  1.42s/it]  6%|▌         | 11/200 [00:21<04:24,  1.40s/it]  6%|▌         | 12/200 [00:22<04:19,  1.38s/it]  6%|▋         | 13/200 [00:24<04:13,  1.35s/it]  7%|▋         | 14/200 [00:25<04:07,  1.33s/it]  8%|▊         | 15/200 [00:26<04:09,  1.35s/it]  8%|▊         | 16/200 [00:28<04:08,  1.35s/it]  8%|▊         | 17/200 [00:29<04:08,  1.36s/it]  9%|▉         | 18/200 [00:30<04:07,  1.36s/it] 10%|▉         | 19/200 [00:32<04:02,  1.34s/it] 10%|█         | 20/200 [00:33<04:04,  1.36s/it] 10%|█         | 21/200 [00:34<03:58,  1.33s/it] 11%|█         | 22/200 [00:36<03:53,  1.31s/it] 12%|█▏        | 23/200 [00:37<03:54,  1.32s/it] 12%|█▏        | 24/200 [00:38<03:49,  1.31s/it] 12%|█▎        | 25/200 [00:40<03:46,  1.30s/it] 13%|█▎        | 26/200 [00:41<03:44,  1.29s/it] 14%|█▎        | 27/200 [00:42<03:41,  1.28s/it] 14%|█▍        | 28/200 [00:43<03:41,  1.29s/it] 14%|█▍        | 29/200 [00:45<03:42,  1.30s/it] 15%|█▌        | 30/200 [00:46<03:45,  1.33s/it] 16%|█▌        | 31/200 [00:47<03:44,  1.33s/it] 16%|█▌        | 32/200 [00:49<03:40,  1.31s/it] 16%|█▋        | 33/200 [00:50<03:41,  1.33s/it] 17%|█▋        | 34/200 [00:51<03:40,  1.33s/it] 18%|█▊        | 35/200 [00:53<03:36,  1.31s/it] 18%|█▊        | 36/200 [00:54<03:33,  1.30s/it] 18%|█▊        | 37/200 [00:55<03:32,  1.30s/it] 19%|█▉        | 38/200 [00:57<03:31,  1.31s/it] 20%|█▉        | 39/200 [00:58<03:32,  1.32s/it] 20%|██        | 40/200 [00:59<03:33,  1.33s/it] 20%|██        | 41/200 [01:01<03:34,  1.35s/it] 21%|██        | 42/200 [01:02<03:31,  1.34s/it] 22%|██▏       | 43/200 [01:03<03:30,  1.34s/it] 22%|██▏       | 44/200 [01:05<03:28,  1.34s/it] 22%|██▎       | 45/200 [01:06<03:34,  1.39s/it] 23%|██▎       | 46/200 [01:07<03:29,  1.36s/it] 24%|██▎       | 47/200 [01:09<03:24,  1.34s/it] 24%|██▍       | 48/200 [01:10<03:23,  1.34s/it] 24%|██▍       | 49/200 [01:11<03:22,  1.34s/it] 25%|██▌       | 50/200 [01:13<03:24,  1.36s/it] 26%|██▌       | 51/200 [01:14<03:23,  1.37s/it] 26%|██▌       | 52/200 [01:16<03:19,  1.35s/it] 26%|██▋       | 53/200 [01:17<03:17,  1.34s/it] 27%|██▋       | 54/200 [01:18<03:15,  1.34s/it] 28%|██▊       | 55/200 [01:20<03:13,  1.34s/it] 28%|██▊       | 56/200 [01:21<03:12,  1.33s/it] 28%|██▊       | 57/200 [01:22<03:11,  1.34s/it] 29%|██▉       | 58/200 [01:23<03:08,  1.32s/it] 30%|██▉       | 59/200 [01:25<03:07,  1.33s/it] 30%|███       | 60/200 [01:26<03:06,  1.33s/it] 30%|███       | 61/200 [01:28<03:07,  1.35s/it] 31%|███       | 62/200 [01:29<03:06,  1.35s/it] 32%|███▏      | 63/200 [01:30<03:02,  1.33s/it] 32%|███▏      | 64/200 [01:32<03:00,  1.33s/it] 32%|███▎      | 65/200 [01:33<03:00,  1.33s/it] 33%|███▎      | 66/200 [01:34<02:56,  1.32s/it] 34%|███▎      | 67/200 [01:36<02:56,  1.33s/it] 34%|███▍      | 68/200 [01:37<03:02,  1.38s/it] 34%|███▍      | 69/200 [01:38<02:57,  1.35s/it] 35%|███▌      | 70/200 [01:40<02:53,  1.33s/it] 36%|███▌      | 71/200 [01:41<02:50,  1.33s/it] 36%|███▌      | 72/200 [01:42<02:49,  1.33s/it] 36%|███▋      | 73/200 [01:44<02:48,  1.33s/it] 37%|███▋      | 74/200 [01:45<02:47,  1.33s/it] 38%|███▊      | 75/200 [01:46<02:51,  1.37s/it] 38%|███▊      | 76/200 [01:48<02:47,  1.35s/it] 38%|███▊      | 77/200 [01:49<02:46,  1.35s/it] 39%|███▉      | 78/200 [01:50<02:44,  1.34s/it] 40%|███▉      | 79/200 [01:52<02:41,  1.34s/it] 40%|████      | 80/200 [01:53<02:39,  1.33s/it] 40%|████      | 81/200 [01:54<02:38,  1.33s/it] 41%|████      | 82/200 [01:56<02:37,  1.33s/it] 42%|████▏     | 83/200 [01:57<02:36,  1.34s/it] 42%|████▏     | 84/200 [01:58<02:33,  1.33s/it] 42%|████▎     | 85/200 [02:00<02:30,  1.31s/it] 43%|████▎     | 86/200 [02:01<02:28,  1.30s/it] 44%|████▎     | 87/200 [02:02<02:26,  1.29s/it] 44%|████▍     | 88/200 [02:03<02:26,  1.31s/it] 44%|████▍     | 89/200 [02:05<02:25,  1.31s/it] 45%|████▌     | 90/200 [02:06<02:25,  1.32s/it] 46%|████▌     | 91/200 [02:07<02:24,  1.32s/it] 46%|████▌     | 92/200 [02:09<02:21,  1.31s/it] 46%|████▋     | 93/200 [02:10<02:19,  1.30s/it] 47%|████▋     | 94/200 [02:11<02:17,  1.30s/it] 48%|████▊     | 95/200 [02:13<02:15,  1.29s/it] 48%|████▊     | 96/200 [02:14<02:13,  1.29s/it] 48%|████▊     | 97/200 [02:15<02:12,  1.28s/it] 49%|████▉     | 98/200 [02:16<02:11,  1.29s/it] 50%|████▉     | 99/200 [02:18<02:09,  1.29s/it] 50%|█████     | 100/200 [02:19<02:08,  1.28s/it] 50%|█████     | 101/200 [02:20<02:07,  1.28s/it] 51%|█████     | 102/200 [02:22<02:05,  1.28s/it] 52%|█████▏    | 103/200 [02:23<02:04,  1.28s/it] 52%|█████▏    | 104/200 [02:24<02:03,  1.28s/it] 52%|█████▎    | 105/200 [02:25<02:02,  1.29s/it] 53%|█████▎    | 106/200 [02:27<02:06,  1.35s/it] 54%|█████▎    | 107/200 [02:28<02:04,  1.34s/it] 54%|█████▍    | 108/200 [02:30<02:04,  1.35s/it] 55%|█████▍    | 109/200 [02:31<02:02,  1.34s/it] 55%|█████▌    | 110/200 [02:32<01:59,  1.33s/it] 56%|█████▌    | 111/200 [02:34<01:57,  1.32s/it] 56%|█████▌    | 112/200 [02:35<01:56,  1.32s/it] 56%|█████▋    | 113/200 [02:36<01:54,  1.31s/it] 57%|█████▋    | 114/200 [02:38<01:57,  1.37s/it] 57%|█████▊    | 115/200 [02:39<01:56,  1.37s/it] 58%|█████▊    | 116/200 [02:40<01:56,  1.38s/it] 58%|█████▊    | 117/200 [02:42<01:55,  1.39s/it] 59%|█████▉    | 118/200 [02:43<01:53,  1.39s/it] 60%|█████▉    | 119/200 [02:45<01:52,  1.38s/it] 60%|██████    | 120/200 [02:46<01:51,  1.39s/it] 60%|██████    | 121/200 [02:47<01:51,  1.41s/it] 61%|██████    | 122/200 [02:49<01:47,  1.38s/it] 62%|██████▏   | 123/200 [02:50<01:43,  1.35s/it] 62%|██████▏   | 124/200 [02:51<01:41,  1.33s/it] 62%|██████▎   | 125/200 [02:53<01:39,  1.32s/it] 63%|██████▎   | 126/200 [02:54<01:37,  1.32s/it] 64%|██████▎   | 127/200 [02:55<01:37,  1.33s/it] 64%|██████▍   | 128/200 [02:57<01:37,  1.35s/it] 64%|██████▍   | 129/200 [02:58<01:40,  1.42s/it] 65%|██████▌   | 130/200 [03:00<01:38,  1.41s/it] 66%|██████▌   | 131/200 [03:01<01:36,  1.40s/it] 66%|██████▌   | 132/200 [03:02<01:35,  1.40s/it] 66%|██████▋   | 133/200 [03:04<01:32,  1.39s/it] 67%|██████▋   | 134/200 [03:05<01:31,  1.38s/it] 68%|██████▊   | 135/200 [03:06<01:28,  1.37s/it] 68%|██████▊   | 136/200 [03:08<01:28,  1.38s/it] 68%|██████▊   | 137/200 [03:09<01:27,  1.39s/it] 69%|██████▉   | 138/200 [03:11<01:26,  1.39s/it] 70%|██████▉   | 139/200 [03:12<01:24,  1.39s/it] 70%|███████   | 140/200 [03:14<01:23,  1.40s/it] 70%|███████   | 141/200 [03:15<01:21,  1.38s/it] 71%|███████   | 142/200 [03:16<01:19,  1.38s/it] 72%|███████▏  | 143/200 [03:18<01:17,  1.36s/it] 72%|███████▏  | 144/200 [03:19<01:16,  1.37s/it] 72%|███████▎  | 145/200 [03:20<01:15,  1.37s/it] 73%|███████▎  | 146/200 [03:22<01:12,  1.35s/it] 74%|███████▎  | 147/200 [03:23<01:11,  1.35s/it] 74%|███████▍  | 148/200 [03:24<01:11,  1.37s/it] 74%|███████▍  | 149/200 [03:26<01:10,  1.38s/it] 75%|███████▌  | 150/200 [03:27<01:09,  1.38s/it] 76%|███████▌  | 151/200 [03:29<01:07,  1.37s/it] 76%|███████▌  | 152/200 [03:30<01:08,  1.42s/it] 76%|███████▋  | 153/200 [03:31<01:05,  1.39s/it] 77%|███████▋  | 154/200 [03:33<01:03,  1.37s/it] 78%|███████▊  | 155/200 [03:34<01:02,  1.39s/it] 78%|███████▊  | 156/200 [03:36<01:01,  1.39s/it] 78%|███████▊  | 157/200 [03:37<01:00,  1.40s/it] 79%|███████▉  | 158/200 [03:38<00:57,  1.38s/it] 80%|███████▉  | 159/200 [03:40<00:55,  1.36s/it] 80%|████████  | 160/200 [03:41<00:53,  1.34s/it] 80%|████████  | 161/200 [03:42<00:51,  1.32s/it] 81%|████████  | 162/200 [03:43<00:49,  1.31s/it] 82%|████████▏ | 163/200 [03:45<00:48,  1.31s/it] 82%|████████▏ | 164/200 [03:46<00:46,  1.30s/it] 82%|████████▎ | 165/200 [03:47<00:45,  1.29s/it] 83%|████████▎ | 166/200 [03:49<00:44,  1.30s/it] 84%|████████▎ | 167/200 [03:50<00:45,  1.37s/it] 84%|████████▍ | 168/200 [03:51<00:43,  1.36s/it] 84%|████████▍ | 169/200 [03:53<00:41,  1.35s/it] 85%|████████▌ | 170/200 [03:54<00:41,  1.37s/it] 86%|████████▌ | 171/200 [03:56<00:39,  1.38s/it] 86%|████████▌ | 172/200 [03:57<00:38,  1.39s/it] 86%|████████▋ | 173/200 [03:58<00:37,  1.39s/it] 87%|████████▋ | 174/200 [04:00<00:35,  1.38s/it] 88%|████████▊ | 175/200 [04:01<00:34,  1.39s/it] 88%|████████▊ | 176/200 [04:02<00:32,  1.36s/it] 88%|████████▊ | 177/200 [04:04<00:30,  1.34s/it] 89%|████████▉ | 178/200 [04:05<00:29,  1.32s/it] 90%|████████▉ | 179/200 [04:06<00:27,  1.30s/it] 90%|█████████ | 180/200 [04:08<00:25,  1.30s/it] 90%|█████████ | 181/200 [04:09<00:24,  1.29s/it] 91%|█████████ | 182/200 [04:10<00:23,  1.30s/it] 92%|█████████▏| 183/200 [04:11<00:21,  1.29s/it] 92%|█████████▏| 184/200 [04:13<00:20,  1.29s/it] 92%|█████████▎| 185/200 [04:14<00:19,  1.29s/it] 93%|█████████▎| 186/200 [04:15<00:18,  1.31s/it] 94%|█████████▎| 187/200 [04:17<00:17,  1.33s/it] 94%|█████████▍| 188/200 [04:18<00:16,  1.33s/it] 94%|█████████▍| 189/200 [04:19<00:14,  1.33s/it] 95%|█████████▌| 190/200 [04:21<00:13,  1.36s/it] 96%|█████████▌| 191/200 [04:22<00:12,  1.33s/it] 96%|█████████▌| 192/200 [04:23<00:10,  1.32s/it] 96%|█████████▋| 193/200 [04:25<00:09,  1.31s/it] 97%|█████████▋| 194/200 [04:26<00:07,  1.30s/it] 98%|█████████▊| 195/200 [04:27<00:06,  1.29s/it] 98%|█████████▊| 196/200 [04:29<00:05,  1.29s/it] 98%|█████████▊| 197/200 [04:30<00:03,  1.29s/it] 99%|█████████▉| 198/200 [04:31<00:02,  1.29s/it]100%|█████████▉| 199/200 [04:32<00:01,  1.29s/it]100%|██████████| 200/200 [04:34<00:00,  1.29s/it][INFO|trainer.py:3846] 2024-11-19 15:36:23,953 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-medium-lora/checkpoint-200
[INFO|configuration_utils.py:690] 2024-11-19 15:36:23,965 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:36:23,965 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-medium",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-19 15:36:23,970 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-medium-lora/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 15:36:23,970 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-medium-lora/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-19 15:36:24,024 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [04:34<00:00,  1.29s/it]100%|██████████| 200/200 [04:34<00:00,  1.37s/it]
[INFO|trainer.py:3846] 2024-11-19 15:36:24,026 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-medium-lora
[INFO|configuration_utils.py:690] 2024-11-19 15:36:24,036 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:36:24,037 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-medium",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-19 15:36:24,041 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-medium-lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 15:36:24,041 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-medium-lora/special_tokens_map.json
{'train_runtime': 274.2582, 'train_samples_per_second': 11.668, 'train_steps_per_second': 0.729, 'train_loss': 2.8495208740234377, 'epoch': 0.03}
***** train metrics *****
  epoch                    =     0.0274
  total_flos               =  5542687GF
  train_loss               =     2.8495
  train_runtime            = 0:04:34.25
  train_samples            =     116582
  train_samples_per_second =     11.668
  train_steps_per_second   =      0.729
11/19/2024 15:36:24 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-19 15:36:24,085 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-19 15:36:24,085 >>   Num examples = 479
[INFO|trainer.py:4167] 2024-11-19 15:36:24,085 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/15 [00:00<?, ?it/s] 13%|█▎        | 2/15 [00:01<00:08,  1.60it/s] 20%|██        | 3/15 [00:02<00:10,  1.13it/s] 27%|██▋       | 4/15 [00:03<00:11,  1.04s/it] 33%|███▎      | 5/15 [00:05<00:11,  1.11s/it] 40%|████      | 6/15 [00:06<00:10,  1.17s/it] 47%|████▋     | 7/15 [00:07<00:09,  1.20s/it] 53%|█████▎    | 8/15 [00:08<00:08,  1.21s/it] 60%|██████    | 9/15 [00:10<00:07,  1.23s/it] 67%|██████▋   | 10/15 [00:11<00:06,  1.25s/it] 73%|███████▎  | 11/15 [00:12<00:05,  1.25s/it] 80%|████████  | 12/15 [00:13<00:03,  1.25s/it] 87%|████████▋ | 13/15 [00:15<00:02,  1.25s/it] 93%|█████████▎| 14/15 [00:16<00:01,  1.25s/it]100%|██████████| 15/15 [00:17<00:00,  1.25s/it]100%|██████████| 15/15 [00:18<00:00,  1.24s/it]
[INFO|modelcard.py:449] 2024-11-19 15:36:43,880 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.4556188866916862}]}
***** eval metrics *****
  epoch                   =     0.0274
  eval_accuracy           =     0.4556
  eval_loss               =     2.7783
  eval_runtime            = 0:00:19.79
  eval_samples            =        479
  eval_samples_per_second =       24.2
  eval_steps_per_second   =      0.758
  perplexity              =    16.0916
------------------------------------
AdaLoRA: Fine-tuning openai-community/gpt2-medium on wikitext-2-raw-v1 and wikitext-103-raw-v1 using baseline model
11/19/2024 15:36:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/19/2024 15:36:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm-transfer-gpt2-medium-adalora/runs/Nov19_15-36-50_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm-transfer-gpt2-medium-adalora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm-transfer-gpt2-medium-adalora,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
11/19/2024 15:36:52 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:36:52 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:36:52 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:36:52 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Overwrite dataset info from restored data version if exists.
11/19/2024 15:36:53 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:36:53 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:36:53 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:36:53 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-19 15:36:53,498 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:36:53,500 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:36:53,510 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:36:53,510 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:36:53,510 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:36:53,510 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:36:53,510 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:36:53,510 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-19 15:36:53,633 >> loading weights file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-19 15:36:53,639 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-19 15:36:53,698 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-19 15:36:53,698 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-19 15:36:53,700 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-19 15:36:53,700 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/peft/tuners/adalora/model.py:205: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-15b981d832943d08.arrow
11/19/2024 15:36:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-15b981d832943d08.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-55124ae0e62b76c7.arrow
11/19/2024 15:36:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-55124ae0e62b76c7.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-78d8587b065121b6.arrow
11/19/2024 15:36:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-78d8587b065121b6.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-07e11c46e56da03e.arrow
11/19/2024 15:36:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-07e11c46e56da03e.arrow
[WARNING|trainer.py:664] 2024-11-19 15:36:55,751 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-19 15:36:55,878 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-19 15:36:55,878 >>   Num examples = 116,582
[INFO|trainer.py:2337] 2024-11-19 15:36:55,878 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-19 15:36:55,878 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-19 15:36:55,878 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-19 15:36:55,878 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-19 15:36:55,878 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-19 15:36:55,878 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-19 15:36:55,879 >>   Number of trainable parameters = 393,312
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:06<20:56,  6.32s/it]  1%|          | 2/200 [00:07<10:35,  3.21s/it]  2%|▏         | 3/200 [00:08<07:16,  2.22s/it]  2%|▏         | 4/200 [00:09<05:42,  1.75s/it]  2%|▎         | 5/200 [00:10<04:50,  1.49s/it]  3%|▎         | 6/200 [00:11<04:19,  1.34s/it]  4%|▎         | 7/200 [00:12<04:05,  1.27s/it]  4%|▍         | 8/200 [00:13<04:00,  1.25s/it]  4%|▍         | 9/200 [00:15<04:00,  1.26s/it]  5%|▌         | 10/200 [00:16<04:01,  1.27s/it]  6%|▌         | 11/200 [00:17<04:00,  1.27s/it]  6%|▌         | 12/200 [00:18<04:00,  1.28s/it]  6%|▋         | 13/200 [00:20<04:00,  1.28s/it]  7%|▋         | 14/200 [00:21<03:59,  1.29s/it]  8%|▊         | 15/200 [00:22<04:00,  1.30s/it]  8%|▊         | 16/200 [00:24<03:58,  1.29s/it]  8%|▊         | 17/200 [00:25<03:56,  1.29s/it]  9%|▉         | 18/200 [00:26<03:54,  1.29s/it] 10%|▉         | 19/200 [00:28<03:53,  1.29s/it] 10%|█         | 20/200 [00:29<03:52,  1.29s/it] 10%|█         | 21/200 [00:30<03:50,  1.29s/it] 11%|█         | 22/200 [00:31<03:52,  1.30s/it] 12%|█▏        | 23/200 [00:33<03:49,  1.30s/it] 12%|█▏        | 24/200 [00:34<03:47,  1.29s/it] 12%|█▎        | 25/200 [00:35<03:46,  1.29s/it] 13%|█▎        | 26/200 [00:37<03:44,  1.29s/it] 14%|█▎        | 27/200 [00:38<03:46,  1.31s/it] 14%|█▍        | 28/200 [00:39<03:47,  1.32s/it] 14%|█▍        | 29/200 [00:41<03:54,  1.37s/it] 15%|█▌        | 30/200 [00:42<03:49,  1.35s/it] 16%|█▌        | 31/200 [00:43<03:48,  1.35s/it] 16%|█▌        | 32/200 [00:45<03:46,  1.35s/it] 16%|█▋        | 33/200 [00:46<03:42,  1.34s/it] 17%|█▋        | 34/200 [00:47<03:41,  1.33s/it] 18%|█▊        | 35/200 [00:49<03:38,  1.32s/it] 18%|█▊        | 36/200 [00:50<03:35,  1.31s/it] 18%|█▊        | 37/200 [00:51<03:35,  1.32s/it] 19%|█▉        | 38/200 [00:53<03:33,  1.32s/it] 20%|█▉        | 39/200 [00:54<03:35,  1.34s/it] 20%|██        | 40/200 [00:55<03:36,  1.35s/it] 20%|██        | 41/200 [00:57<03:33,  1.34s/it] 21%|██        | 42/200 [00:58<03:29,  1.32s/it] 22%|██▏       | 43/200 [00:59<03:30,  1.34s/it] 22%|██▏       | 44/200 [01:01<03:31,  1.35s/it] 22%|██▎       | 45/200 [01:02<03:26,  1.34s/it] 23%|██▎       | 46/200 [01:03<03:23,  1.32s/it] 24%|██▎       | 47/200 [01:05<03:22,  1.32s/it] 24%|██▍       | 48/200 [01:06<03:19,  1.31s/it] 24%|██▍       | 49/200 [01:07<03:17,  1.31s/it] 25%|██▌       | 50/200 [01:09<03:15,  1.30s/it] 26%|██▌       | 51/200 [01:10<03:13,  1.30s/it] 26%|██▌       | 52/200 [01:11<03:14,  1.32s/it] 26%|██▋       | 53/200 [01:13<03:12,  1.31s/it] 27%|██▋       | 54/200 [01:14<03:10,  1.31s/it] 28%|██▊       | 55/200 [01:15<03:11,  1.32s/it] 28%|██▊       | 56/200 [01:17<03:11,  1.33s/it] 28%|██▊       | 57/200 [01:18<03:11,  1.34s/it] 29%|██▉       | 58/200 [01:19<03:09,  1.33s/it] 30%|██▉       | 59/200 [01:21<03:11,  1.36s/it] 30%|███       | 60/200 [01:22<03:11,  1.37s/it] 30%|███       | 61/200 [01:23<03:09,  1.36s/it] 31%|███       | 62/200 [01:25<03:08,  1.37s/it] 32%|███▏      | 63/200 [01:26<03:06,  1.36s/it] 32%|███▏      | 64/200 [01:27<03:02,  1.34s/it] 32%|███▎      | 65/200 [01:29<03:02,  1.35s/it] 33%|███▎      | 66/200 [01:30<03:04,  1.38s/it] 34%|███▎      | 67/200 [01:32<03:02,  1.37s/it] 34%|███▍      | 68/200 [01:33<03:02,  1.38s/it] 34%|███▍      | 69/200 [01:34<02:57,  1.35s/it] 35%|███▌      | 70/200 [01:36<02:53,  1.34s/it] 36%|███▌      | 71/200 [01:37<02:53,  1.34s/it] 36%|███▌      | 72/200 [01:38<02:50,  1.33s/it] 36%|███▋      | 73/200 [01:40<02:51,  1.35s/it] 37%|███▋      | 74/200 [01:41<02:51,  1.36s/it] 38%|███▊      | 75/200 [01:42<02:47,  1.34s/it] 38%|███▊      | 76/200 [01:44<02:44,  1.33s/it] 38%|███▊      | 77/200 [01:45<02:41,  1.31s/it] 39%|███▉      | 78/200 [01:46<02:39,  1.31s/it] 40%|███▉      | 79/200 [01:47<02:38,  1.31s/it] 40%|████      | 80/200 [01:49<02:36,  1.30s/it] 40%|████      | 81/200 [01:50<02:36,  1.32s/it] 41%|████      | 82/200 [01:51<02:38,  1.34s/it] 42%|████▏     | 83/200 [01:53<02:38,  1.35s/it] 42%|████▏     | 84/200 [01:54<02:37,  1.36s/it] 42%|████▎     | 85/200 [01:56<02:36,  1.36s/it] 43%|████▎     | 86/200 [01:57<02:35,  1.37s/it] 44%|████▎     | 87/200 [01:58<02:34,  1.37s/it] 44%|████▍     | 88/200 [02:00<02:34,  1.38s/it] 44%|████▍     | 89/200 [02:01<02:32,  1.37s/it] 45%|████▌     | 90/200 [02:02<02:30,  1.37s/it] 46%|████▌     | 91/200 [02:04<02:27,  1.36s/it] 46%|████▌     | 92/200 [02:05<02:24,  1.34s/it] 46%|████▋     | 93/200 [02:06<02:22,  1.33s/it] 47%|████▋     | 94/200 [02:08<02:21,  1.34s/it] 48%|████▊     | 95/200 [02:09<02:20,  1.34s/it] 48%|████▊     | 96/200 [02:11<02:20,  1.35s/it] 48%|████▊     | 97/200 [02:12<02:19,  1.36s/it] 49%|████▉     | 98/200 [02:13<02:20,  1.37s/it] 50%|████▉     | 99/200 [02:15<02:17,  1.36s/it] 50%|█████     | 100/200 [02:16<02:14,  1.34s/it] 50%|█████     | 101/200 [02:17<02:11,  1.33s/it] 51%|█████     | 102/200 [02:19<02:13,  1.36s/it] 52%|█████▏    | 103/200 [02:20<02:12,  1.36s/it] 52%|█████▏    | 104/200 [02:21<02:11,  1.37s/it] 52%|█████▎    | 105/200 [02:23<02:08,  1.35s/it] 53%|█████▎    | 106/200 [02:24<02:05,  1.34s/it] 54%|█████▎    | 107/200 [02:25<02:04,  1.34s/it] 54%|█████▍    | 108/200 [02:27<02:05,  1.36s/it] 55%|█████▍    | 109/200 [02:28<02:06,  1.39s/it] 55%|█████▌    | 110/200 [02:30<02:04,  1.38s/it] 56%|█████▌    | 111/200 [02:31<02:00,  1.36s/it] 56%|█████▌    | 112/200 [02:32<01:57,  1.34s/it] 56%|█████▋    | 113/200 [02:33<01:55,  1.32s/it] 57%|█████▋    | 114/200 [02:35<01:53,  1.32s/it] 57%|█████▊    | 115/200 [02:36<01:51,  1.31s/it] 58%|█████▊    | 116/200 [02:37<01:51,  1.33s/it] 58%|█████▊    | 117/200 [02:39<01:50,  1.33s/it] 59%|█████▉    | 118/200 [02:40<01:48,  1.32s/it] 60%|█████▉    | 119/200 [02:41<01:46,  1.32s/it] 60%|██████    | 120/200 [02:43<01:45,  1.32s/it] 60%|██████    | 121/200 [02:44<01:45,  1.34s/it] 61%|██████    | 122/200 [02:45<01:43,  1.33s/it] 62%|██████▏   | 123/200 [02:47<01:43,  1.34s/it] 62%|██████▏   | 124/200 [02:48<01:41,  1.34s/it] 62%|██████▎   | 125/200 [02:49<01:40,  1.34s/it] 63%|██████▎   | 126/200 [02:51<01:37,  1.32s/it] 64%|██████▎   | 127/200 [02:52<01:35,  1.31s/it] 64%|██████▍   | 128/200 [02:53<01:34,  1.31s/it] 64%|██████▍   | 129/200 [02:55<01:33,  1.31s/it] 65%|██████▌   | 130/200 [02:56<01:31,  1.31s/it] 66%|██████▌   | 131/200 [02:57<01:29,  1.30s/it] 66%|██████▌   | 132/200 [02:59<01:28,  1.30s/it] 66%|██████▋   | 133/200 [03:00<01:26,  1.30s/it] 67%|██████▋   | 134/200 [03:01<01:25,  1.29s/it] 68%|██████▊   | 135/200 [03:02<01:24,  1.29s/it] 68%|██████▊   | 136/200 [03:04<01:22,  1.29s/it] 68%|██████▊   | 137/200 [03:05<01:22,  1.30s/it] 69%|██████▉   | 138/200 [03:06<01:20,  1.30s/it] 70%|██████▉   | 139/200 [03:08<01:19,  1.30s/it] 70%|███████   | 140/200 [03:09<01:17,  1.30s/it] 70%|███████   | 141/200 [03:10<01:16,  1.30s/it] 71%|███████   | 142/200 [03:12<01:16,  1.31s/it] 72%|███████▏  | 143/200 [03:13<01:16,  1.34s/it] 72%|███████▏  | 144/200 [03:14<01:16,  1.37s/it] 72%|███████▎  | 145/200 [03:16<01:16,  1.40s/it] 73%|███████▎  | 146/200 [03:17<01:14,  1.38s/it] 74%|███████▎  | 147/200 [03:18<01:11,  1.35s/it] 74%|███████▍  | 148/200 [03:20<01:09,  1.33s/it] 74%|███████▍  | 149/200 [03:21<01:07,  1.32s/it] 75%|███████▌  | 150/200 [03:22<01:05,  1.31s/it] 76%|███████▌  | 151/200 [03:24<01:03,  1.31s/it] 76%|███████▌  | 152/200 [03:25<01:02,  1.31s/it] 76%|███████▋  | 153/200 [03:26<01:01,  1.31s/it] 77%|███████▋  | 154/200 [03:28<00:59,  1.30s/it] 78%|███████▊  | 155/200 [03:29<00:58,  1.30s/it] 78%|███████▊  | 156/200 [03:30<00:57,  1.30s/it] 78%|███████▊  | 157/200 [03:31<00:55,  1.30s/it] 79%|███████▉  | 158/200 [03:33<00:54,  1.29s/it] 80%|███████▉  | 159/200 [03:34<00:53,  1.31s/it] 80%|████████  | 160/200 [03:35<00:52,  1.30s/it] 80%|████████  | 161/200 [03:37<00:50,  1.30s/it] 81%|████████  | 162/200 [03:38<00:49,  1.30s/it] 82%|████████▏ | 163/200 [03:39<00:47,  1.29s/it] 82%|████████▏ | 164/200 [03:40<00:46,  1.29s/it] 82%|████████▎ | 165/200 [03:42<00:45,  1.29s/it] 83%|████████▎ | 166/200 [03:43<00:44,  1.30s/it] 84%|████████▎ | 167/200 [03:44<00:42,  1.30s/it] 84%|████████▍ | 168/200 [03:46<00:41,  1.30s/it] 84%|████████▍ | 169/200 [03:47<00:40,  1.30s/it] 85%|████████▌ | 170/200 [03:48<00:38,  1.29s/it] 86%|████████▌ | 171/200 [03:50<00:37,  1.29s/it] 86%|████████▌ | 172/200 [03:51<00:36,  1.30s/it] 86%|████████▋ | 173/200 [03:52<00:35,  1.30s/it] 87%|████████▋ | 174/200 [03:53<00:33,  1.30s/it] 88%|████████▊ | 175/200 [03:55<00:32,  1.29s/it] 88%|████████▊ | 176/200 [03:56<00:31,  1.29s/it] 88%|████████▊ | 177/200 [03:57<00:29,  1.29s/it] 89%|████████▉ | 178/200 [03:59<00:28,  1.29s/it] 90%|████████▉ | 179/200 [04:00<00:27,  1.31s/it] 90%|█████████ | 180/200 [04:01<00:26,  1.30s/it] 90%|█████████ | 181/200 [04:03<00:24,  1.30s/it] 91%|█████████ | 182/200 [04:04<00:23,  1.30s/it] 92%|█████████▏| 183/200 [04:05<00:22,  1.29s/it] 92%|█████████▏| 184/200 [04:06<00:20,  1.29s/it] 92%|█████████▎| 185/200 [04:08<00:19,  1.29s/it] 93%|█████████▎| 186/200 [04:09<00:18,  1.30s/it] 94%|█████████▎| 187/200 [04:10<00:16,  1.30s/it] 94%|█████████▍| 188/200 [04:12<00:15,  1.30s/it] 94%|█████████▍| 189/200 [04:13<00:14,  1.30s/it] 95%|█████████▌| 190/200 [04:14<00:12,  1.29s/it] 96%|█████████▌| 191/200 [04:16<00:11,  1.29s/it] 96%|█████████▌| 192/200 [04:17<00:10,  1.29s/it] 96%|█████████▋| 193/200 [04:18<00:09,  1.30s/it] 97%|█████████▋| 194/200 [04:19<00:07,  1.30s/it] 98%|█████████▊| 195/200 [04:21<00:06,  1.30s/it] 98%|█████████▊| 196/200 [04:22<00:05,  1.29s/it] 98%|█████████▊| 197/200 [04:23<00:03,  1.29s/it] 99%|█████████▉| 198/200 [04:25<00:02,  1.30s/it]100%|█████████▉| 199/200 [04:26<00:01,  1.30s/it]100%|██████████| 200/200 [04:27<00:00,  1.32s/it][INFO|trainer.py:3846] 2024-11-19 15:41:23,628 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-medium-adalora/checkpoint-200
[INFO|configuration_utils.py:690] 2024-11-19 15:41:23,640 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:41:23,641 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-medium",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-19 15:41:23,646 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-medium-adalora/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 15:41:23,646 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-medium-adalora/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-19 15:41:23,705 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [04:27<00:00,  1.32s/it]100%|██████████| 200/200 [04:27<00:00,  1.34s/it]
[INFO|trainer.py:3846] 2024-11-19 15:41:23,707 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-medium-adalora
[INFO|configuration_utils.py:690] 2024-11-19 15:41:23,717 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:41:23,718 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-medium",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-19 15:41:23,723 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-medium-adalora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 15:41:23,723 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-medium-adalora/special_tokens_map.json
{'train_runtime': 267.8268, 'train_samples_per_second': 11.948, 'train_steps_per_second': 0.747, 'train_loss': 2.8504794311523436, 'epoch': 0.03}
***** train metrics *****
  epoch                    =     0.0274
  total_flos               =  5542689GF
  train_loss               =     2.8505
  train_runtime            = 0:04:27.82
  train_samples            =     116582
  train_samples_per_second =     11.948
  train_steps_per_second   =      0.747
11/19/2024 15:41:23 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-19 15:41:23,768 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-19 15:41:23,768 >>   Num examples = 479
[INFO|trainer.py:4167] 2024-11-19 15:41:23,768 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/15 [00:00<?, ?it/s] 13%|█▎        | 2/15 [00:01<00:08,  1.56it/s] 20%|██        | 3/15 [00:02<00:10,  1.11it/s] 27%|██▋       | 4/15 [00:03<00:11,  1.04s/it] 33%|███▎      | 5/15 [00:05<00:11,  1.11s/it] 40%|████      | 6/15 [00:06<00:10,  1.16s/it] 47%|████▋     | 7/15 [00:07<00:09,  1.20s/it] 53%|█████▎    | 8/15 [00:08<00:08,  1.23s/it] 60%|██████    | 9/15 [00:10<00:07,  1.24s/it] 67%|██████▋   | 10/15 [00:11<00:06,  1.26s/it] 73%|███████▎  | 11/15 [00:12<00:05,  1.28s/it] 80%|████████  | 12/15 [00:14<00:03,  1.27s/it] 87%|████████▋ | 13/15 [00:15<00:02,  1.27s/it] 93%|█████████▎| 14/15 [00:16<00:01,  1.27s/it]100%|██████████| 15/15 [00:17<00:00,  1.25s/it]100%|██████████| 15/15 [00:18<00:00,  1.25s/it]
[INFO|modelcard.py:449] 2024-11-19 15:41:43,801 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.4553597120099915}]}
***** eval metrics *****
  epoch                   =     0.0274
  eval_accuracy           =     0.4554
  eval_loss               =     2.8827
  eval_runtime            = 0:00:20.03
  eval_samples            =        479
  eval_samples_per_second =     23.912
  eval_steps_per_second   =      0.749
  perplexity              =    17.8629
------------------------------------
LagEmbed: Training openai-community/gpt2-medium on wikitext-2-raw-v1 and wikitext-103-raw-v1 with LagEmbed (in_channels=1024, n_components=2, dof=4)
11/19/2024 15:41:49 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/19/2024 15:41:49 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm-transfer-gpt2-medium-lagembed/runs/Nov19_15-41-49_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm-transfer-gpt2-medium-lagembed,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm-transfer-gpt2-medium-lagembed,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
11/19/2024 15:41:51 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:41:51 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:41:51 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:41:51 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Overwrite dataset info from restored data version if exists.
11/19/2024 15:41:53 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:41:53 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:41:53 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:41:53 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-19 15:41:53,152 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:41:53,153 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:41:53,162 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:41:53,162 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:41:53,162 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:41:53,162 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:41:53,162 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:41:53,162 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-19 15:41:53,272 >> loading weights file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-19 15:41:53,278 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-19 15:41:53,354 >> All model checkpoint weights were used when initializing LagGPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-19 15:41:53,354 >> All the weights of LagGPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LagGPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-19 15:41:53,356 >> loading configuration file /tmp/test-clm-gpt2-medium-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-19 15:41:53,357 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-15b981d832943d08.arrow
11/19/2024 15:41:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-15b981d832943d08.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-55124ae0e62b76c7.arrow
11/19/2024 15:41:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-55124ae0e62b76c7.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-78d8587b065121b6.arrow
11/19/2024 15:41:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-78d8587b065121b6.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-07e11c46e56da03e.arrow
11/19/2024 15:41:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-07e11c46e56da03e.arrow
[WARNING|trainer.py:664] 2024-11-19 15:41:55,391 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-19 15:41:55,516 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-19 15:41:55,517 >>   Num examples = 116,582
[INFO|trainer.py:2337] 2024-11-19 15:41:55,517 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-19 15:41:55,517 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:2340] 2024-11-19 15:41:55,517 >>   Training with DataParallel so batch size has been adjusted to: 32
[INFO|trainer.py:2341] 2024-11-19 15:41:55,517 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2342] 2024-11-19 15:41:55,517 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-19 15:41:55,517 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-19 15:41:55,517 >>   Number of trainable parameters = 404,106
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:07<25:20,  7.64s/it]  1%|          | 2/200 [00:09<13:08,  3.98s/it]  2%|▏         | 3/200 [00:10<09:13,  2.81s/it]  2%|▏         | 4/200 [00:11<07:24,  2.27s/it]  2%|▎         | 5/200 [00:13<06:23,  1.97s/it]  3%|▎         | 6/200 [00:14<05:45,  1.78s/it]  4%|▎         | 7/200 [00:16<05:23,  1.68s/it]  4%|▍         | 8/200 [00:17<05:06,  1.60s/it]  4%|▍         | 9/200 [00:19<04:54,  1.54s/it]  5%|▌         | 10/200 [00:20<04:46,  1.51s/it]  6%|▌         | 11/200 [00:21<04:41,  1.49s/it]  6%|▌         | 12/200 [00:23<04:37,  1.47s/it]  6%|▋         | 13/200 [00:24<04:33,  1.46s/it]  7%|▋         | 14/200 [00:26<04:30,  1.46s/it]  8%|▊         | 15/200 [00:27<04:28,  1.45s/it]  8%|▊         | 16/200 [00:29<04:25,  1.44s/it]  8%|▊         | 17/200 [00:30<04:25,  1.45s/it]  9%|▉         | 18/200 [00:32<04:22,  1.44s/it] 10%|▉         | 19/200 [00:33<04:20,  1.44s/it] 10%|█         | 20/200 [00:34<04:22,  1.46s/it] 10%|█         | 21/200 [00:36<04:27,  1.49s/it] 11%|█         | 22/200 [00:38<04:26,  1.50s/it] 12%|█▏        | 23/200 [00:39<04:29,  1.52s/it] 12%|█▏        | 24/200 [00:41<04:30,  1.54s/it] 12%|█▎        | 25/200 [00:42<04:28,  1.53s/it] 13%|█▎        | 26/200 [00:44<04:25,  1.52s/it] 14%|█▎        | 27/200 [00:45<04:25,  1.53s/it] 14%|█▍        | 28/200 [00:47<04:23,  1.53s/it] 14%|█▍        | 29/200 [00:48<04:22,  1.53s/it] 15%|█▌        | 30/200 [00:50<04:21,  1.54s/it] 16%|█▌        | 31/200 [00:52<04:24,  1.56s/it] 16%|█▌        | 32/200 [00:53<04:21,  1.56s/it] 16%|█▋        | 33/200 [00:55<04:17,  1.54s/it] 17%|█▋        | 34/200 [00:56<04:17,  1.55s/it] 18%|█▊        | 35/200 [00:58<04:14,  1.54s/it] 18%|█▊        | 36/200 [00:59<04:11,  1.54s/it] 18%|█▊        | 37/200 [01:01<04:11,  1.54s/it] 19%|█▉        | 38/200 [01:02<04:11,  1.55s/it] 20%|█▉        | 39/200 [01:04<04:11,  1.56s/it] 20%|██        | 40/200 [01:05<04:10,  1.57s/it] 20%|██        | 41/200 [01:07<04:09,  1.57s/it] 21%|██        | 42/200 [01:09<04:06,  1.56s/it] 22%|██▏       | 43/200 [01:10<04:04,  1.55s/it] 22%|██▏       | 44/200 [01:12<04:00,  1.54s/it] 22%|██▎       | 45/200 [01:13<03:58,  1.54s/it] 23%|██▎       | 46/200 [01:15<03:49,  1.49s/it] 24%|██▎       | 47/200 [01:16<03:45,  1.48s/it] 24%|██▍       | 48/200 [01:17<03:42,  1.46s/it] 24%|██▍       | 49/200 [01:19<03:40,  1.46s/it] 25%|██▌       | 50/200 [01:20<03:38,  1.46s/it] 26%|██▌       | 51/200 [01:22<03:35,  1.45s/it] 26%|██▌       | 52/200 [01:23<03:34,  1.45s/it] 26%|██▋       | 53/200 [01:25<03:35,  1.46s/it] 27%|██▋       | 54/200 [01:26<03:33,  1.46s/it] 28%|██▊       | 55/200 [01:28<03:31,  1.46s/it] 28%|██▊       | 56/200 [01:29<03:33,  1.48s/it] 28%|██▊       | 57/200 [01:31<03:34,  1.50s/it] 29%|██▉       | 58/200 [01:32<03:32,  1.50s/it] 30%|██▉       | 59/200 [01:34<03:32,  1.51s/it] 30%|███       | 60/200 [01:35<03:30,  1.51s/it] 30%|███       | 61/200 [01:37<03:30,  1.51s/it] 31%|███       | 62/200 [01:38<03:26,  1.50s/it] 32%|███▏      | 63/200 [01:40<03:23,  1.49s/it] 32%|███▏      | 64/200 [01:41<03:20,  1.47s/it] 32%|███▎      | 65/200 [01:43<03:17,  1.46s/it] 33%|███▎      | 66/200 [01:44<03:14,  1.45s/it] 34%|███▎      | 67/200 [01:45<03:13,  1.45s/it] 34%|███▍      | 68/200 [01:47<03:14,  1.47s/it] 34%|███▍      | 69/200 [01:48<03:14,  1.48s/it] 35%|███▌      | 70/200 [01:50<03:14,  1.50s/it] 36%|███▌      | 71/200 [01:52<03:13,  1.50s/it] 36%|███▌      | 72/200 [01:53<03:13,  1.51s/it] 36%|███▋      | 73/200 [01:54<03:10,  1.50s/it] 37%|███▋      | 74/200 [01:56<03:07,  1.49s/it] 38%|███▊      | 75/200 [01:57<03:06,  1.49s/it] 38%|███▊      | 76/200 [01:59<03:07,  1.51s/it] 38%|███▊      | 77/200 [02:01<03:07,  1.52s/it] 39%|███▉      | 78/200 [02:02<03:05,  1.52s/it] 40%|███▉      | 79/200 [02:04<03:03,  1.51s/it] 40%|████      | 80/200 [02:05<03:02,  1.52s/it] 40%|████      | 81/200 [02:07<02:59,  1.51s/it] 41%|████      | 82/200 [02:08<02:56,  1.49s/it] 42%|████▏     | 83/200 [02:09<02:52,  1.48s/it] 42%|████▏     | 84/200 [02:11<02:50,  1.47s/it] 42%|████▎     | 85/200 [02:12<02:47,  1.46s/it] 43%|████▎     | 86/200 [02:14<02:45,  1.45s/it] 44%|████▎     | 87/200 [02:15<02:43,  1.45s/it] 44%|████▍     | 88/200 [02:17<02:44,  1.47s/it] 44%|████▍     | 89/200 [02:18<02:45,  1.49s/it] 45%|████▌     | 90/200 [02:20<02:42,  1.48s/it] 46%|████▌     | 91/200 [02:21<02:46,  1.52s/it] 46%|████▌     | 92/200 [02:23<02:45,  1.53s/it] 46%|████▋     | 93/200 [02:24<02:41,  1.51s/it] 47%|████▋     | 94/200 [02:26<02:39,  1.51s/it] 48%|████▊     | 95/200 [02:27<02:37,  1.50s/it] 48%|████▊     | 96/200 [02:29<02:34,  1.49s/it] 48%|████▊     | 97/200 [02:30<02:31,  1.47s/it] 49%|████▉     | 98/200 [02:32<02:30,  1.47s/it] 50%|████▉     | 99/200 [02:33<02:29,  1.48s/it] 50%|█████     | 100/200 [02:35<02:30,  1.50s/it] 50%|█████     | 101/200 [02:36<02:28,  1.50s/it] 51%|█████     | 102/200 [02:38<02:27,  1.51s/it] 52%|█████▏    | 103/200 [02:39<02:26,  1.51s/it] 52%|█████▏    | 104/200 [02:41<02:24,  1.51s/it] 52%|█████▎    | 105/200 [02:42<02:24,  1.53s/it] 53%|█████▎    | 106/200 [02:44<02:26,  1.55s/it] 54%|█████▎    | 107/200 [02:46<02:22,  1.53s/it] 54%|█████▍    | 108/200 [02:47<02:22,  1.55s/it] 55%|█████▍    | 109/200 [02:49<02:20,  1.54s/it] 55%|█████▌    | 110/200 [02:50<02:17,  1.52s/it] 56%|█████▌    | 111/200 [02:52<02:14,  1.51s/it] 56%|█████▌    | 112/200 [02:53<02:11,  1.49s/it] 56%|█████▋    | 113/200 [02:54<02:08,  1.47s/it] 57%|█████▋    | 114/200 [02:56<02:06,  1.47s/it] 57%|█████▊    | 115/200 [02:57<02:04,  1.46s/it] 58%|█████▊    | 116/200 [02:59<02:02,  1.46s/it] 58%|█████▊    | 117/200 [03:00<02:01,  1.47s/it] 59%|█████▉    | 118/200 [03:02<02:01,  1.49s/it] 60%|█████▉    | 119/200 [03:03<02:00,  1.49s/it] 60%|██████    | 120/200 [03:05<01:58,  1.48s/it] 60%|██████    | 121/200 [03:06<01:56,  1.47s/it] 61%|██████    | 122/200 [03:08<01:54,  1.46s/it] 62%|██████▏   | 123/200 [03:09<01:52,  1.46s/it] 62%|██████▏   | 124/200 [03:11<01:50,  1.46s/it] 62%|██████▎   | 125/200 [03:12<01:49,  1.46s/it] 63%|██████▎   | 126/200 [03:13<01:47,  1.45s/it] 64%|██████▎   | 127/200 [03:15<01:47,  1.48s/it] 64%|██████▍   | 128/200 [03:17<01:48,  1.51s/it] 64%|██████▍   | 129/200 [03:18<01:48,  1.53s/it] 65%|██████▌   | 130/200 [03:20<01:45,  1.50s/it] 66%|██████▌   | 131/200 [03:21<01:43,  1.50s/it] 66%|██████▌   | 132/200 [03:23<01:41,  1.49s/it] 66%|██████▋   | 133/200 [03:24<01:39,  1.48s/it] 67%|██████▋   | 134/200 [03:25<01:36,  1.46s/it] 68%|██████▊   | 135/200 [03:27<01:34,  1.46s/it] 68%|██████▊   | 136/200 [03:28<01:33,  1.46s/it] 68%|██████▊   | 137/200 [03:30<01:32,  1.47s/it] 69%|██████▉   | 138/200 [03:31<01:30,  1.47s/it] 70%|██████▉   | 139/200 [03:33<01:29,  1.47s/it] 70%|███████   | 140/200 [03:34<01:27,  1.47s/it] 70%|███████   | 141/200 [03:36<01:26,  1.47s/it] 71%|███████   | 142/200 [03:37<01:25,  1.48s/it] 72%|███████▏  | 143/200 [03:39<01:24,  1.48s/it] 72%|███████▏  | 144/200 [03:40<01:22,  1.47s/it] 72%|███████▎  | 145/200 [03:42<01:20,  1.46s/it] 73%|███████▎  | 146/200 [03:43<01:18,  1.46s/it] 74%|███████▎  | 147/200 [03:45<01:17,  1.46s/it] 74%|███████▍  | 148/200 [03:46<01:15,  1.45s/it] 74%|███████▍  | 149/200 [03:47<01:14,  1.45s/it] 75%|███████▌  | 150/200 [03:49<01:13,  1.46s/it] 76%|███████▌  | 151/200 [03:50<01:12,  1.49s/it] 76%|███████▌  | 152/200 [03:52<01:11,  1.50s/it] 76%|███████▋  | 153/200 [03:53<01:10,  1.49s/it] 77%|███████▋  | 154/200 [03:55<01:08,  1.49s/it] 78%|███████▊  | 155/200 [03:56<01:07,  1.51s/it] 78%|███████▊  | 156/200 [03:58<01:05,  1.49s/it] 78%|███████▊  | 157/200 [03:59<01:03,  1.48s/it] 79%|███████▉  | 158/200 [04:01<01:01,  1.47s/it] 80%|███████▉  | 159/200 [04:02<00:59,  1.45s/it] 80%|████████  | 160/200 [04:04<00:58,  1.46s/it] 80%|████████  | 161/200 [04:05<00:56,  1.45s/it] 81%|████████  | 162/200 [04:07<00:55,  1.45s/it] 82%|████████▏ | 163/200 [04:08<00:53,  1.45s/it] 82%|████████▏ | 164/200 [04:09<00:52,  1.45s/it] 82%|████████▎ | 165/200 [04:11<00:50,  1.44s/it] 83%|████████▎ | 166/200 [04:12<00:49,  1.45s/it] 84%|████████▎ | 167/200 [04:14<00:48,  1.45s/it] 84%|████████▍ | 168/200 [04:15<00:45,  1.43s/it] 84%|████████▍ | 169/200 [04:17<00:44,  1.45s/it] 85%|████████▌ | 170/200 [04:18<00:43,  1.46s/it] 86%|████████▌ | 171/200 [04:20<00:42,  1.46s/it] 86%|████████▌ | 172/200 [04:21<00:41,  1.47s/it] 86%|████████▋ | 173/200 [04:23<00:40,  1.50s/it] 87%|████████▋ | 174/200 [04:24<00:39,  1.51s/it] 88%|████████▊ | 175/200 [04:26<00:37,  1.49s/it] 88%|████████▊ | 176/200 [04:27<00:35,  1.48s/it] 88%|████████▊ | 177/200 [04:29<00:33,  1.48s/it] 89%|████████▉ | 178/200 [04:30<00:32,  1.47s/it] 90%|████████▉ | 179/200 [04:32<00:30,  1.47s/it] 90%|█████████ | 180/200 [04:33<00:29,  1.47s/it] 90%|█████████ | 181/200 [04:34<00:27,  1.47s/it] 91%|█████████ | 182/200 [04:36<00:26,  1.46s/it] 92%|█████████▏| 183/200 [04:37<00:24,  1.46s/it] 92%|█████████▏| 184/200 [04:39<00:23,  1.46s/it] 92%|█████████▎| 185/200 [04:40<00:21,  1.45s/it] 93%|█████████▎| 186/200 [04:42<00:20,  1.45s/it] 94%|█████████▎| 187/200 [04:43<00:18,  1.45s/it] 94%|█████████▍| 188/200 [04:45<00:17,  1.45s/it] 94%|█████████▍| 189/200 [04:46<00:15,  1.45s/it] 95%|█████████▌| 190/200 [04:48<00:14,  1.45s/it] 96%|█████████▌| 191/200 [04:49<00:13,  1.45s/it] 96%|█████████▌| 192/200 [04:50<00:11,  1.45s/it] 96%|█████████▋| 193/200 [04:52<00:10,  1.45s/it] 97%|█████████▋| 194/200 [04:53<00:08,  1.45s/it] 98%|█████████▊| 195/200 [04:55<00:07,  1.45s/it] 98%|█████████▊| 196/200 [04:56<00:05,  1.47s/it] 98%|█████████▊| 197/200 [04:58<00:04,  1.48s/it] 99%|█████████▉| 198/200 [04:59<00:02,  1.48s/it]100%|█████████▉| 199/200 [05:01<00:01,  1.47s/it]100%|██████████| 200/200 [05:02<00:00,  1.47s/it][INFO|trainer.py:3846] 2024-11-19 15:46:58,197 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-medium-lagembed/checkpoint-200
[INFO|configuration_utils.py:416] 2024-11-19 15:46:58,198 >> Configuration saved in /tmp/test-clm-transfer-gpt2-medium-lagembed/checkpoint-200/config.json
[INFO|configuration_utils.py:873] 2024-11-19 15:46:58,198 >> Configuration saved in /tmp/test-clm-transfer-gpt2-medium-lagembed/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-19 15:47:00,961 >> Model weights saved in /tmp/test-clm-transfer-gpt2-medium-lagembed/checkpoint-200/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-19 15:47:00,962 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-medium-lagembed/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 15:47:00,962 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-medium-lagembed/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-19 15:47:01,016 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [05:05<00:00,  1.47s/it]100%|██████████| 200/200 [05:05<00:00,  1.53s/it]
[INFO|trainer.py:3846] 2024-11-19 15:47:01,018 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-medium-lagembed
[INFO|configuration_utils.py:416] 2024-11-19 15:47:01,019 >> Configuration saved in /tmp/test-clm-transfer-gpt2-medium-lagembed/config.json
[INFO|configuration_utils.py:873] 2024-11-19 15:47:01,020 >> Configuration saved in /tmp/test-clm-transfer-gpt2-medium-lagembed/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-19 15:47:03,117 >> Model weights saved in /tmp/test-clm-transfer-gpt2-medium-lagembed/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-19 15:47:03,117 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-medium-lagembed/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 15:47:03,118 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-medium-lagembed/special_tokens_map.json
{'train_runtime': 305.499, 'train_samples_per_second': 20.949, 'train_steps_per_second': 0.655, 'train_loss': 2.7691558837890624, 'epoch': 0.05}
***** train metrics *****
  epoch                    =     0.0549
  total_flos               = 11085773GF
  train_loss               =     2.7692
  train_runtime            = 0:05:05.49
  train_samples            =     116582
  train_samples_per_second =     20.949
  train_steps_per_second   =      0.655
11/19/2024 15:47:03 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-19 15:47:03,155 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-19 15:47:03,155 >>   Num examples = 479
[INFO|trainer.py:4167] 2024-11-19 15:47:03,156 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/15 [00:00<?, ?it/s] 13%|█▎        | 2/15 [00:01<00:06,  1.93it/s] 20%|██        | 3/15 [00:02<00:08,  1.36it/s] 27%|██▋       | 4/15 [00:03<00:09,  1.18it/s] 33%|███▎      | 5/15 [00:04<00:09,  1.10it/s] 40%|████      | 6/15 [00:05<00:08,  1.05it/s] 47%|████▋     | 7/15 [00:06<00:07,  1.02it/s] 53%|█████▎    | 8/15 [00:07<00:06,  1.00it/s] 60%|██████    | 9/15 [00:08<00:06,  1.01s/it] 67%|██████▋   | 10/15 [00:09<00:05,  1.02s/it] 73%|███████▎  | 11/15 [00:10<00:04,  1.03s/it] 80%|████████  | 12/15 [00:11<00:03,  1.03s/it] 87%|████████▋ | 13/15 [00:12<00:02,  1.03s/it] 93%|█████████▎| 14/15 [00:13<00:01,  1.03s/it]100%|██████████| 15/15 [00:14<00:00,  1.03s/it]100%|██████████| 15/15 [00:15<00:00,  1.04s/it]
[INFO|modelcard.py:449] 2024-11-19 15:47:19,749 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.45581887975315144}]}
***** eval metrics *****
  epoch                   =     0.0549
  eval_accuracy           =     0.4558
  eval_loss               =     2.7773
  eval_runtime            = 0:00:16.59
  eval_samples            =        479
  eval_samples_per_second =     28.868
  eval_steps_per_second   =      0.904
  perplexity              =    16.0752
------------------------------------
LoRA: Fine-tuning openai-community/gpt2-large on wikitext-2-raw-v1 and wikitext-103-raw-v1 using baseline model
11/19/2024 15:47:25 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/19/2024 15:47:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm-transfer-gpt2-large-lora/runs/Nov19_15-47-24_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm-transfer-gpt2-large-lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm-transfer-gpt2-large-lora,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
11/19/2024 15:47:27 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:47:27 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:47:27 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:47:27 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Overwrite dataset info from restored data version if exists.
11/19/2024 15:47:29 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:47:29 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:47:29 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:47:29 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-19 15:47:29,308 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:47:29,310 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:47:29,321 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:47:29,321 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:47:29,321 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:47:29,321 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:47:29,321 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:47:29,321 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-19 15:47:29,433 >> loading weights file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-19 15:47:29,441 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-19 15:47:29,593 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-19 15:47:29,593 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-19 15:47:29,596 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-19 15:47:29,597 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5d1fd704fc8fd842.arrow
11/19/2024 15:47:30 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5d1fd704fc8fd842.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-3ef4922073f8db54.arrow
11/19/2024 15:47:30 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-3ef4922073f8db54.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-aa9bb39585b0b8e5.arrow
11/19/2024 15:47:31 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-aa9bb39585b0b8e5.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-904b3824cd400b17.arrow
11/19/2024 15:47:31 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-904b3824cd400b17.arrow
[WARNING|trainer.py:664] 2024-11-19 15:47:33,393 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-19 15:47:33,540 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-19 15:47:33,541 >>   Num examples = 116,582
[INFO|trainer.py:2337] 2024-11-19 15:47:33,541 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-19 15:47:33,541 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-19 15:47:33,541 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-19 15:47:33,541 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-19 15:47:33,541 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-19 15:47:33,541 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-19 15:47:33,542 >>   Number of trainable parameters = 737,280
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:09<30:53,  9.32s/it]  1%|          | 2/200 [00:11<17:46,  5.39s/it]  2%|▏         | 3/200 [00:14<13:24,  4.08s/it]  2%|▏         | 4/200 [00:16<11:08,  3.41s/it]  2%|▎         | 5/200 [00:19<09:50,  3.03s/it]  3%|▎         | 6/200 [00:21<09:04,  2.81s/it]  4%|▎         | 7/200 [00:23<08:35,  2.67s/it]  4%|▍         | 8/200 [00:26<08:14,  2.58s/it]  4%|▍         | 9/200 [00:28<08:06,  2.55s/it]  5%|▌         | 10/200 [00:31<07:49,  2.47s/it]  6%|▌         | 11/200 [00:33<07:50,  2.49s/it]  6%|▌         | 12/200 [00:36<07:54,  2.52s/it]  6%|▋         | 13/200 [00:38<07:59,  2.57s/it]  7%|▋         | 14/200 [00:41<07:43,  2.49s/it]  8%|▊         | 15/200 [00:43<07:35,  2.46s/it]  8%|▊         | 16/200 [00:46<07:29,  2.44s/it]  8%|▊         | 17/200 [00:48<07:28,  2.45s/it]  9%|▉         | 18/200 [00:51<07:32,  2.49s/it] 10%|▉         | 19/200 [00:53<07:32,  2.50s/it] 10%|█         | 20/200 [00:56<07:30,  2.51s/it] 10%|█         | 21/200 [00:58<07:21,  2.47s/it] 11%|█         | 22/200 [01:00<07:12,  2.43s/it] 12%|█▏        | 23/200 [01:03<07:07,  2.42s/it] 12%|█▏        | 24/200 [01:05<07:00,  2.39s/it] 12%|█▎        | 25/200 [01:07<06:58,  2.39s/it] 13%|█▎        | 26/200 [01:10<06:56,  2.39s/it] 14%|█▎        | 27/200 [01:12<06:53,  2.39s/it] 14%|█▍        | 28/200 [01:15<06:48,  2.37s/it] 14%|█▍        | 29/200 [01:17<06:46,  2.38s/it] 15%|█▌        | 30/200 [01:19<06:45,  2.38s/it] 16%|█▌        | 31/200 [01:22<06:43,  2.39s/it] 16%|█▌        | 32/200 [01:24<06:42,  2.39s/it] 16%|█▋        | 33/200 [01:27<06:39,  2.39s/it] 17%|█▋        | 34/200 [01:29<06:36,  2.39s/it] 18%|█▊        | 35/200 [01:31<06:33,  2.39s/it] 18%|█▊        | 36/200 [01:34<06:31,  2.39s/it] 18%|█▊        | 37/200 [01:36<06:29,  2.39s/it] 19%|█▉        | 38/200 [01:38<06:23,  2.37s/it] 20%|█▉        | 39/200 [01:41<06:21,  2.37s/it] 20%|██        | 40/200 [01:43<06:21,  2.38s/it] 20%|██        | 41/200 [01:46<06:19,  2.39s/it] 21%|██        | 42/200 [01:48<06:12,  2.36s/it] 22%|██▏       | 43/200 [01:50<06:13,  2.38s/it] 22%|██▏       | 44/200 [01:53<06:17,  2.42s/it] 22%|██▎       | 45/200 [01:55<06:13,  2.41s/it] 23%|██▎       | 46/200 [01:58<06:16,  2.45s/it] 24%|██▎       | 47/200 [02:00<06:18,  2.47s/it] 24%|██▍       | 48/200 [02:03<06:18,  2.49s/it] 24%|██▍       | 49/200 [02:05<06:23,  2.54s/it] 25%|██▌       | 50/200 [02:08<06:13,  2.49s/it] 26%|██▌       | 51/200 [02:10<06:06,  2.46s/it] 26%|██▌       | 52/200 [02:13<06:09,  2.50s/it] 26%|██▋       | 53/200 [02:15<06:14,  2.55s/it] 27%|██▋       | 54/200 [02:18<06:05,  2.51s/it] 28%|██▊       | 55/200 [02:20<05:59,  2.48s/it] 28%|██▊       | 56/200 [02:23<05:54,  2.46s/it] 28%|██▊       | 57/200 [02:25<05:56,  2.49s/it] 29%|██▉       | 58/200 [02:28<05:59,  2.53s/it] 30%|██▉       | 59/200 [02:30<05:58,  2.54s/it] 30%|███       | 60/200 [02:33<06:04,  2.60s/it] 30%|███       | 61/200 [02:36<05:58,  2.58s/it] 31%|███       | 62/200 [02:38<05:49,  2.53s/it] 32%|███▏      | 63/200 [02:41<05:44,  2.51s/it] 32%|███▏      | 64/200 [02:43<05:38,  2.49s/it] 32%|███▎      | 65/200 [02:46<05:37,  2.50s/it] 33%|███▎      | 66/200 [02:48<05:41,  2.55s/it] 34%|███▎      | 67/200 [02:51<05:34,  2.52s/it] 34%|███▍      | 68/200 [02:53<05:34,  2.53s/it] 34%|███▍      | 69/200 [02:56<05:28,  2.51s/it] 35%|███▌      | 70/200 [02:58<05:21,  2.47s/it] 36%|███▌      | 71/200 [03:01<05:18,  2.47s/it] 36%|███▌      | 72/200 [03:03<05:13,  2.45s/it] 36%|███▋      | 73/200 [03:05<05:09,  2.43s/it] 37%|███▋      | 74/200 [03:07<04:52,  2.32s/it] 38%|███▊      | 75/200 [03:09<04:34,  2.20s/it] 38%|███▊      | 76/200 [03:11<04:21,  2.11s/it] 38%|███▊      | 77/200 [03:14<04:25,  2.16s/it] 39%|███▉      | 78/200 [03:16<04:42,  2.32s/it] 40%|███▉      | 79/200 [03:19<04:53,  2.42s/it] 40%|████      | 80/200 [03:21<04:55,  2.47s/it] 40%|████      | 81/200 [03:24<04:53,  2.46s/it] 41%|████      | 82/200 [03:26<04:51,  2.47s/it] 42%|████▏     | 83/200 [03:29<04:50,  2.48s/it] 42%|████▏     | 84/200 [03:31<04:47,  2.48s/it] 42%|████▎     | 85/200 [03:34<04:46,  2.50s/it] 43%|████▎     | 86/200 [03:36<04:43,  2.49s/it] 44%|████▎     | 87/200 [03:39<04:41,  2.49s/it] 44%|████▍     | 88/200 [03:41<04:36,  2.47s/it] 44%|████▍     | 89/200 [03:44<04:34,  2.47s/it] 45%|████▌     | 90/200 [03:46<04:31,  2.47s/it] 46%|████▌     | 91/200 [03:49<04:28,  2.47s/it] 46%|████▌     | 92/200 [03:51<04:26,  2.47s/it] 46%|████▋     | 93/200 [03:54<04:25,  2.48s/it] 47%|████▋     | 94/200 [03:56<04:20,  2.46s/it] 48%|████▊     | 95/200 [03:59<04:17,  2.45s/it] 48%|████▊     | 96/200 [04:01<04:12,  2.43s/it] 48%|████▊     | 97/200 [04:03<04:11,  2.44s/it] 49%|████▉     | 98/200 [04:06<04:13,  2.48s/it] 50%|████▉     | 99/200 [04:09<04:16,  2.54s/it] 50%|█████     | 100/200 [04:11<04:10,  2.50s/it] 50%|█████     | 101/200 [04:13<04:03,  2.46s/it] 51%|█████     | 102/200 [04:16<04:02,  2.48s/it] 52%|█████▏    | 103/200 [04:18<03:59,  2.47s/it] 52%|█████▏    | 104/200 [04:21<04:03,  2.53s/it] 52%|█████▎    | 105/200 [04:24<04:03,  2.56s/it] 53%|█████▎    | 106/200 [04:26<03:58,  2.54s/it] 54%|█████▎    | 107/200 [04:29<04:00,  2.59s/it] 54%|█████▍    | 108/200 [04:31<03:58,  2.60s/it] 55%|█████▍    | 109/200 [04:34<03:53,  2.57s/it] 55%|█████▌    | 110/200 [04:37<03:50,  2.56s/it] 56%|█████▌    | 111/200 [04:39<03:47,  2.56s/it] 56%|█████▌    | 112/200 [04:42<03:47,  2.58s/it] 56%|█████▋    | 113/200 [04:44<03:44,  2.58s/it] 57%|█████▋    | 114/200 [04:47<03:40,  2.57s/it] 57%|█████▊    | 115/200 [04:49<03:36,  2.55s/it] 58%|█████▊    | 116/200 [04:52<03:33,  2.55s/it] 58%|█████▊    | 117/200 [04:54<03:28,  2.51s/it] 59%|█████▉    | 118/200 [04:57<03:26,  2.52s/it] 60%|█████▉    | 119/200 [04:59<03:24,  2.52s/it] 60%|██████    | 120/200 [05:02<03:22,  2.53s/it] 60%|██████    | 121/200 [05:04<03:18,  2.51s/it] 61%|██████    | 122/200 [05:07<03:17,  2.53s/it] 62%|██████▏   | 123/200 [05:10<03:17,  2.56s/it] 62%|██████▏   | 124/200 [05:12<03:14,  2.56s/it] 62%|██████▎   | 125/200 [05:15<03:13,  2.58s/it] 63%|██████▎   | 126/200 [05:17<03:11,  2.59s/it] 64%|██████▎   | 127/200 [05:20<03:09,  2.60s/it] 64%|██████▍   | 128/200 [05:23<03:08,  2.62s/it] 64%|██████▍   | 129/200 [05:25<03:06,  2.63s/it] 65%|██████▌   | 130/200 [05:28<02:59,  2.57s/it] 66%|██████▌   | 131/200 [05:30<02:55,  2.54s/it] 66%|██████▌   | 132/200 [05:33<02:51,  2.52s/it] 66%|██████▋   | 133/200 [05:35<02:49,  2.52s/it] 67%|██████▋   | 134/200 [05:38<02:46,  2.52s/it] 68%|██████▊   | 135/200 [05:40<02:45,  2.54s/it] 68%|██████▊   | 136/200 [05:43<02:42,  2.53s/it] 68%|██████▊   | 137/200 [05:45<02:39,  2.53s/it] 69%|██████▉   | 138/200 [05:48<02:39,  2.57s/it] 70%|██████▉   | 139/200 [05:50<02:34,  2.53s/it] 70%|███████   | 140/200 [05:53<02:32,  2.54s/it] 70%|███████   | 141/200 [05:55<02:27,  2.50s/it] 71%|███████   | 142/200 [05:58<02:23,  2.48s/it] 72%|███████▏  | 143/200 [06:00<02:20,  2.46s/it] 72%|███████▏  | 144/200 [06:03<02:17,  2.45s/it] 72%|███████▎  | 145/200 [06:05<02:12,  2.41s/it] 73%|███████▎  | 146/200 [06:07<02:10,  2.41s/it] 74%|███████▎  | 147/200 [06:10<02:07,  2.40s/it] 74%|███████▍  | 148/200 [06:12<02:05,  2.41s/it] 74%|███████▍  | 149/200 [06:15<02:03,  2.42s/it] 75%|███████▌  | 150/200 [06:17<02:01,  2.42s/it] 76%|███████▌  | 151/200 [06:20<01:59,  2.44s/it] 76%|███████▌  | 152/200 [06:22<01:57,  2.44s/it] 76%|███████▋  | 153/200 [06:25<01:56,  2.47s/it] 77%|███████▋  | 154/200 [06:27<01:58,  2.57s/it] 78%|███████▊  | 155/200 [06:30<01:57,  2.60s/it] 78%|███████▊  | 156/200 [06:33<01:55,  2.63s/it] 78%|███████▊  | 157/200 [06:35<01:53,  2.65s/it] 79%|███████▉  | 158/200 [06:38<01:51,  2.65s/it] 80%|███████▉  | 159/200 [06:41<01:47,  2.63s/it] 80%|████████  | 160/200 [06:43<01:44,  2.61s/it] 80%|████████  | 161/200 [06:46<01:41,  2.59s/it] 81%|████████  | 162/200 [06:49<01:40,  2.64s/it] 82%|████████▏ | 163/200 [06:51<01:36,  2.60s/it] 82%|████████▏ | 164/200 [06:54<01:32,  2.57s/it] 82%|████████▎ | 165/200 [06:56<01:30,  2.59s/it] 83%|████████▎ | 166/200 [06:59<01:29,  2.62s/it] 84%|████████▎ | 167/200 [07:01<01:26,  2.61s/it] 84%|████████▍ | 168/200 [07:04<01:22,  2.59s/it] 84%|████████▍ | 169/200 [07:07<01:20,  2.61s/it] 85%|████████▌ | 170/200 [07:09<01:18,  2.62s/it] 86%|████████▌ | 171/200 [07:12<01:16,  2.64s/it] 86%|████████▌ | 172/200 [07:15<01:14,  2.65s/it] 86%|████████▋ | 173/200 [07:17<01:11,  2.67s/it] 87%|████████▋ | 174/200 [07:20<01:09,  2.66s/it] 88%|████████▊ | 175/200 [07:23<01:06,  2.64s/it] 88%|████████▊ | 176/200 [07:25<01:02,  2.62s/it] 88%|████████▊ | 177/200 [07:28<01:00,  2.62s/it] 89%|████████▉ | 178/200 [07:30<00:57,  2.60s/it] 90%|████████▉ | 179/200 [07:33<00:54,  2.57s/it] 90%|█████████ | 180/200 [07:35<00:50,  2.55s/it] 90%|█████████ | 181/200 [07:38<00:47,  2.49s/it] 91%|█████████ | 182/200 [07:40<00:44,  2.47s/it] 92%|█████████▏| 183/200 [07:43<00:41,  2.46s/it] 92%|█████████▏| 184/200 [07:45<00:39,  2.47s/it] 92%|█████████▎| 185/200 [07:47<00:36,  2.45s/it] 93%|█████████▎| 186/200 [07:50<00:34,  2.46s/it] 94%|█████████▎| 187/200 [07:53<00:33,  2.54s/it] 94%|█████████▍| 188/200 [07:55<00:30,  2.57s/it] 94%|█████████▍| 189/200 [07:58<00:28,  2.57s/it] 95%|█████████▌| 190/200 [08:00<00:24,  2.49s/it] 96%|█████████▌| 191/200 [08:03<00:22,  2.45s/it] 96%|█████████▌| 192/200 [08:05<00:19,  2.41s/it] 96%|█████████▋| 193/200 [08:07<00:16,  2.40s/it] 97%|█████████▋| 194/200 [08:10<00:14,  2.42s/it] 98%|█████████▊| 195/200 [08:12<00:12,  2.44s/it] 98%|█████████▊| 196/200 [08:15<00:09,  2.47s/it] 98%|█████████▊| 197/200 [08:17<00:07,  2.48s/it] 99%|█████████▉| 198/200 [08:20<00:05,  2.52s/it]100%|█████████▉| 199/200 [08:22<00:02,  2.49s/it]100%|██████████| 200/200 [08:25<00:00,  2.48s/it][INFO|trainer.py:3846] 2024-11-19 15:55:58,815 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-large-lora/checkpoint-200
[INFO|configuration_utils.py:690] 2024-11-19 15:55:58,828 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:55:58,828 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-large",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-19 15:55:58,836 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-large-lora/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 15:55:58,836 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-large-lora/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-19 15:55:58,902 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [08:25<00:00,  2.48s/it]100%|██████████| 200/200 [08:25<00:00,  2.53s/it]
[INFO|trainer.py:3846] 2024-11-19 15:55:58,904 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-large-lora
[INFO|configuration_utils.py:690] 2024-11-19 15:55:58,916 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:55:58,917 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-large",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-19 15:55:58,924 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-large-lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 15:55:58,924 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-large-lora/special_tokens_map.json
{'train_runtime': 505.3608, 'train_samples_per_second': 6.332, 'train_steps_per_second': 0.396, 'train_loss': 2.6915109252929685, 'epoch': 0.03}
***** train metrics *****
  epoch                    =     0.0274
  total_flos               = 12984515GF
  train_loss               =     2.6915
  train_runtime            = 0:08:25.36
  train_samples            =     116582
  train_samples_per_second =      6.332
  train_steps_per_second   =      0.396
11/19/2024 15:55:58 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-19 15:55:58,975 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-19 15:55:58,975 >>   Num examples = 479
[INFO|trainer.py:4167] 2024-11-19 15:55:58,975 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/15 [00:00<?, ?it/s] 13%|█▎        | 2/15 [00:02<00:14,  1.10s/it] 20%|██        | 3/15 [00:04<00:19,  1.61s/it] 27%|██▋       | 4/15 [00:06<00:20,  1.89s/it] 33%|███▎      | 5/15 [00:09<00:20,  2.00s/it] 40%|████      | 6/15 [00:11<00:18,  2.10s/it] 47%|████▋     | 7/15 [00:13<00:17,  2.13s/it] 53%|█████▎    | 8/15 [00:15<00:14,  2.06s/it] 60%|██████    | 9/15 [00:17<00:12,  2.01s/it] 67%|██████▋   | 10/15 [00:19<00:09,  1.98s/it] 73%|███████▎  | 11/15 [00:21<00:07,  1.96s/it] 80%|████████  | 12/15 [00:23<00:05,  1.97s/it] 87%|████████▋ | 13/15 [00:25<00:03,  1.94s/it] 93%|█████████▎| 14/15 [00:26<00:01,  1.90s/it]100%|██████████| 15/15 [00:28<00:00,  1.86s/it]100%|██████████| 15/15 [00:29<00:00,  1.97s/it]
[INFO|modelcard.py:449] 2024-11-19 15:56:31,376 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.4725162596399717}]}
***** eval metrics *****
  epoch                   =     0.0274
  eval_accuracy           =     0.4725
  eval_loss               =     2.6347
  eval_runtime            = 0:00:32.40
  eval_samples            =        479
  eval_samples_per_second =     14.784
  eval_steps_per_second   =      0.463
  perplexity              =    13.9387
------------------------------------
AdaLoRA: Fine-tuning openai-community/gpt2-large on wikitext-2-raw-v1 and wikitext-103-raw-v1 using baseline model
11/19/2024 15:56:36 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/19/2024 15:56:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm-transfer-gpt2-large-adalora/runs/Nov19_15-56-36_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm-transfer-gpt2-large-adalora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm-transfer-gpt2-large-adalora,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
11/19/2024 15:56:38 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:56:38 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:56:38 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:56:38 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Overwrite dataset info from restored data version if exists.
11/19/2024 15:56:39 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:56:39 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
11/19/2024 15:56:39 - INFO - datasets.builder - Found cached dataset wikitext (/home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 15:56:39 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-19 15:56:39,461 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 15:56:39,462 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:56:39,473 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:56:39,473 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:56:39,473 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:56:39,473 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:56:39,473 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 15:56:39,473 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-19 15:56:39,568 >> loading weights file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-19 15:56:39,576 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-19 15:56:39,668 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-19 15:56:39,668 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-19 15:56:39,670 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-19 15:56:39,670 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/peft/tuners/adalora/model.py:205: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5d1fd704fc8fd842.arrow
11/19/2024 15:56:39 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5d1fd704fc8fd842.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-3ef4922073f8db54.arrow
11/19/2024 15:56:39 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-3ef4922073f8db54.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-aa9bb39585b0b8e5.arrow
11/19/2024 15:56:39 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-aa9bb39585b0b8e5.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-904b3824cd400b17.arrow
11/19/2024 15:56:39 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-904b3824cd400b17.arrow
[WARNING|trainer.py:664] 2024-11-19 15:56:41,559 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-19 15:56:41,700 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-19 15:56:41,700 >>   Num examples = 116,582
[INFO|trainer.py:2337] 2024-11-19 15:56:41,700 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-19 15:56:41,700 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-19 15:56:41,700 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-19 15:56:41,700 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-19 15:56:41,700 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-19 15:56:41,700 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-19 15:56:41,701 >>   Number of trainable parameters = 737,424
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:09<30:16,  9.13s/it]  1%|          | 2/200 [00:11<17:09,  5.20s/it]  2%|▏         | 3/200 [00:14<12:59,  3.96s/it]  2%|▏         | 4/200 [00:16<10:59,  3.37s/it]  2%|▎         | 5/200 [00:19<09:59,  3.07s/it]  3%|▎         | 6/200 [00:21<09:17,  2.87s/it]  4%|▎         | 7/200 [00:24<08:51,  2.76s/it]  4%|▍         | 8/200 [00:26<08:40,  2.71s/it]  4%|▍         | 9/200 [00:29<08:43,  2.74s/it]  5%|▌         | 10/200 [00:32<08:31,  2.69s/it]  6%|▌         | 11/200 [00:34<08:14,  2.62s/it]  6%|▌         | 12/200 [00:37<08:05,  2.58s/it]  6%|▋         | 13/200 [00:39<07:57,  2.55s/it]  7%|▋         | 14/200 [00:42<07:53,  2.54s/it]  8%|▊         | 15/200 [00:44<07:50,  2.55s/it]  8%|▊         | 16/200 [00:47<07:51,  2.56s/it]  8%|▊         | 17/200 [00:49<07:48,  2.56s/it]  9%|▉         | 18/200 [00:52<07:47,  2.57s/it] 10%|▉         | 19/200 [00:54<07:42,  2.56s/it] 10%|█         | 20/200 [00:57<07:41,  2.56s/it] 10%|█         | 21/200 [01:00<07:44,  2.60s/it] 11%|█         | 22/200 [01:02<07:37,  2.57s/it] 12%|█▏        | 23/200 [01:05<07:29,  2.54s/it] 12%|█▏        | 24/200 [01:07<07:20,  2.50s/it] 12%|█▎        | 25/200 [01:10<07:19,  2.51s/it] 13%|█▎        | 26/200 [01:12<07:21,  2.54s/it] 14%|█▎        | 27/200 [01:15<07:25,  2.58s/it] 14%|█▍        | 28/200 [01:17<07:22,  2.57s/it] 14%|█▍        | 29/200 [01:20<07:24,  2.60s/it] 15%|█▌        | 30/200 [01:23<07:24,  2.61s/it] 16%|█▌        | 31/200 [01:25<07:13,  2.57s/it] 16%|█▌        | 32/200 [01:28<07:04,  2.52s/it] 16%|█▋        | 33/200 [01:30<06:58,  2.51s/it] 17%|█▋        | 34/200 [01:33<06:58,  2.52s/it] 18%|█▊        | 35/200 [01:35<06:59,  2.54s/it] 18%|█▊        | 36/200 [01:38<06:58,  2.55s/it] 18%|█▊        | 37/200 [01:40<06:57,  2.56s/it] 19%|█▉        | 38/200 [01:43<06:52,  2.55s/it] 20%|█▉        | 39/200 [01:45<06:54,  2.57s/it] 20%|██        | 40/200 [01:48<06:43,  2.52s/it] 20%|██        | 41/200 [01:50<06:37,  2.50s/it] 21%|██        | 42/200 [01:53<06:34,  2.50s/it] 22%|██▏       | 43/200 [01:56<06:41,  2.56s/it] 22%|██▏       | 44/200 [01:58<06:44,  2.59s/it] 22%|██▎       | 45/200 [02:01<06:39,  2.58s/it] 23%|██▎       | 46/200 [02:03<06:28,  2.53s/it] 24%|██▎       | 47/200 [02:06<06:23,  2.51s/it] 24%|██▍       | 48/200 [02:08<06:30,  2.57s/it] 24%|██▍       | 49/200 [02:11<06:32,  2.60s/it] 25%|██▌       | 50/200 [02:14<06:30,  2.61s/it] 26%|██▌       | 51/200 [02:16<06:28,  2.61s/it] 26%|██▌       | 52/200 [02:19<06:29,  2.63s/it] 26%|██▋       | 53/200 [02:21<06:22,  2.60s/it] 27%|██▋       | 54/200 [02:24<06:16,  2.58s/it] 28%|██▊       | 55/200 [02:26<06:11,  2.56s/it] 28%|██▊       | 56/200 [02:29<06:10,  2.57s/it] 28%|██▊       | 57/200 [02:32<06:07,  2.57s/it] 29%|██▉       | 58/200 [02:34<06:09,  2.60s/it] 30%|██▉       | 59/200 [02:37<06:02,  2.57s/it] 30%|███       | 60/200 [02:39<05:55,  2.54s/it] 30%|███       | 61/200 [02:42<05:55,  2.55s/it] 31%|███       | 62/200 [02:44<05:51,  2.55s/it] 32%|███▏      | 63/200 [02:47<05:47,  2.54s/it] 32%|███▏      | 64/200 [02:49<05:44,  2.53s/it] 32%|███▎      | 65/200 [02:52<05:41,  2.53s/it] 33%|███▎      | 66/200 [02:54<05:38,  2.53s/it] 34%|███▎      | 67/200 [02:57<05:36,  2.53s/it] 34%|███▍      | 68/200 [03:00<05:35,  2.54s/it] 34%|███▍      | 69/200 [03:02<05:34,  2.56s/it] 35%|███▌      | 70/200 [03:05<05:32,  2.56s/it] 36%|███▌      | 71/200 [03:07<05:31,  2.57s/it] 36%|███▌      | 72/200 [03:10<05:25,  2.54s/it] 36%|███▋      | 73/200 [03:12<05:24,  2.55s/it] 37%|███▋      | 74/200 [03:15<05:19,  2.53s/it] 38%|███▊      | 75/200 [03:17<05:18,  2.55s/it] 38%|███▊      | 76/200 [03:20<05:17,  2.56s/it] 38%|███▊      | 77/200 [03:23<05:17,  2.58s/it] 39%|███▉      | 78/200 [03:25<05:14,  2.57s/it] 40%|███▉      | 79/200 [03:28<05:07,  2.54s/it] 40%|████      | 80/200 [03:30<05:02,  2.52s/it] 40%|████      | 81/200 [03:33<05:03,  2.55s/it] 41%|████      | 82/200 [03:35<05:00,  2.55s/it] 42%|████▏     | 83/200 [03:38<04:57,  2.54s/it] 42%|████▏     | 84/200 [03:40<04:55,  2.54s/it] 42%|████▎     | 85/200 [03:43<04:57,  2.59s/it] 43%|████▎     | 86/200 [03:46<04:52,  2.57s/it] 44%|████▎     | 87/200 [03:48<04:45,  2.53s/it] 44%|████▍     | 88/200 [03:51<04:44,  2.54s/it] 44%|████▍     | 89/200 [03:53<04:42,  2.54s/it] 45%|████▌     | 90/200 [03:56<04:42,  2.57s/it] 46%|████▌     | 91/200 [03:58<04:42,  2.60s/it] 46%|████▌     | 92/200 [04:01<04:41,  2.61s/it] 46%|████▋     | 93/200 [04:04<04:36,  2.59s/it] 47%|████▋     | 94/200 [04:06<04:31,  2.56s/it] 48%|████▊     | 95/200 [04:09<04:30,  2.58s/it] 48%|████▊     | 96/200 [04:11<04:24,  2.54s/it] 48%|████▊     | 97/200 [04:14<04:23,  2.56s/it] 49%|████▉     | 98/200 [04:16<04:23,  2.59s/it] 50%|████▉     | 99/200 [04:19<04:24,  2.62s/it] 50%|█████     | 100/200 [04:22<04:21,  2.61s/it] 50%|█████     | 101/200 [04:24<04:20,  2.63s/it] 51%|█████     | 102/200 [04:27<04:11,  2.57s/it] 52%|█████▏    | 103/200 [04:29<04:07,  2.55s/it] 52%|█████▏    | 104/200 [04:32<04:05,  2.55s/it] 52%|█████▎    | 105/200 [04:34<03:59,  2.53s/it] 53%|█████▎    | 106/200 [04:37<03:58,  2.54s/it] 54%|█████▎    | 107/200 [04:39<03:55,  2.53s/it] 54%|█████▍    | 108/200 [04:42<03:51,  2.51s/it] 55%|█████▍    | 109/200 [04:44<03:46,  2.49s/it] 55%|█████▌    | 110/200 [04:47<03:42,  2.47s/it] 56%|█████▌    | 111/200 [04:49<03:40,  2.48s/it] 56%|█████▌    | 112/200 [04:52<03:43,  2.54s/it] 56%|█████▋    | 113/200 [04:55<03:44,  2.58s/it] 57%|█████▋    | 114/200 [04:57<03:42,  2.58s/it] 57%|█████▊    | 115/200 [05:00<03:42,  2.62s/it] 58%|█████▊    | 116/200 [05:03<03:38,  2.60s/it] 58%|█████▊    | 117/200 [05:05<03:32,  2.56s/it] 59%|█████▉    | 118/200 [05:07<03:25,  2.51s/it] 60%|█████▉    | 119/200 [05:10<03:23,  2.52s/it] 60%|██████    | 120/200 [05:12<03:19,  2.49s/it] 60%|██████    | 121/200 [05:15<03:15,  2.47s/it] 61%|██████    | 122/200 [05:17<03:11,  2.46s/it] 62%|██████▏   | 123/200 [05:20<03:07,  2.43s/it] 62%|██████▏   | 124/200 [05:22<03:04,  2.43s/it] 62%|██████▎   | 125/200 [05:24<03:01,  2.42s/it] 63%|██████▎   | 126/200 [05:27<02:59,  2.43s/it] 64%|██████▎   | 127/200 [05:29<02:55,  2.41s/it] 64%|██████▍   | 128/200 [05:32<02:52,  2.40s/it] 64%|██████▍   | 129/200 [05:34<02:50,  2.41s/it] 65%|██████▌   | 130/200 [05:36<02:49,  2.42s/it] 66%|██████▌   | 131/200 [05:39<02:46,  2.41s/it] 66%|██████▌   | 132/200 [05:41<02:42,  2.39s/it] 66%|██████▋   | 133/200 [05:44<02:41,  2.41s/it] 67%|██████▋   | 134/200 [05:46<02:38,  2.41s/it] 68%|██████▊   | 135/200 [05:48<02:36,  2.41s/it] 68%|██████▊   | 136/200 [05:51<02:33,  2.40s/it] 68%|██████▊   | 137/200 [05:53<02:31,  2.41s/it] 69%|██████▉   | 138/200 [05:56<02:29,  2.41s/it] 70%|██████▉   | 139/200 [05:58<02:27,  2.42s/it] 70%|███████   | 140/200 [06:01<02:25,  2.42s/it] 70%|███████   | 141/200 [06:03<02:21,  2.41s/it] 71%|███████   | 142/200 [06:05<02:19,  2.41s/it] 72%|███████▏  | 143/200 [06:08<02:17,  2.41s/it] 72%|███████▏  | 144/200 [06:10<02:14,  2.40s/it] 72%|███████▎  | 145/200 [06:12<02:11,  2.39s/it] 73%|███████▎  | 146/200 [06:15<02:09,  2.40s/it] 74%|███████▎  | 147/200 [06:17<02:07,  2.41s/it] 74%|███████▍  | 148/200 [06:20<02:05,  2.42s/it] 74%|███████▍  | 149/200 [06:22<02:02,  2.41s/it] 75%|███████▌  | 150/200 [06:25<01:59,  2.40s/it] 76%|███████▌  | 151/200 [06:27<01:57,  2.41s/it] 76%|███████▌  | 152/200 [06:29<01:55,  2.40s/it] 76%|███████▋  | 153/200 [06:32<01:53,  2.41s/it] 77%|███████▋  | 154/200 [06:34<01:50,  2.40s/it] 78%|███████▊  | 155/200 [06:37<01:48,  2.41s/it] 78%|███████▊  | 156/200 [06:39<01:46,  2.41s/it] 78%|███████▊  | 157/200 [06:41<01:43,  2.42s/it] 79%|███████▉  | 158/200 [06:44<01:41,  2.43s/it] 80%|███████▉  | 159/200 [06:46<01:38,  2.41s/it] 80%|████████  | 160/200 [06:49<01:36,  2.41s/it] 80%|████████  | 161/200 [06:51<01:34,  2.42s/it] 81%|████████  | 162/200 [06:54<01:32,  2.42s/it] 82%|████████▏ | 163/200 [06:56<01:29,  2.41s/it] 82%|████████▏ | 164/200 [06:58<01:26,  2.41s/it] 82%|████████▎ | 165/200 [07:01<01:24,  2.41s/it] 83%|████████▎ | 166/200 [07:03<01:22,  2.42s/it] 84%|████████▎ | 167/200 [07:06<01:19,  2.40s/it] 84%|████████▍ | 168/200 [07:08<01:16,  2.39s/it] 84%|████████▍ | 169/200 [07:10<01:14,  2.39s/it] 85%|████████▌ | 170/200 [07:13<01:12,  2.40s/it] 86%|████████▌ | 171/200 [07:15<01:09,  2.41s/it] 86%|████████▌ | 172/200 [07:18<01:07,  2.40s/it] 86%|████████▋ | 173/200 [07:20<01:04,  2.40s/it] 87%|████████▋ | 174/200 [07:22<01:02,  2.41s/it] 88%|████████▊ | 175/200 [07:25<01:00,  2.42s/it] 88%|████████▊ | 176/200 [07:27<00:57,  2.40s/it] 88%|████████▊ | 177/200 [07:30<00:55,  2.39s/it] 89%|████████▉ | 178/200 [07:32<00:52,  2.38s/it] 90%|████████▉ | 179/200 [07:34<00:49,  2.37s/it] 90%|█████████ | 180/200 [07:37<00:47,  2.38s/it] 90%|█████████ | 181/200 [07:39<00:43,  2.28s/it] 91%|█████████ | 182/200 [07:41<00:39,  2.18s/it] 92%|█████████▏| 183/200 [07:43<00:37,  2.20s/it] 92%|█████████▏| 184/200 [07:45<00:34,  2.15s/it] 92%|█████████▎| 185/200 [07:47<00:31,  2.09s/it] 93%|█████████▎| 186/200 [07:49<00:28,  2.07s/it] 94%|█████████▎| 187/200 [07:51<00:26,  2.04s/it] 94%|█████████▍| 188/200 [07:53<00:24,  2.01s/it] 94%|█████████▍| 189/200 [07:55<00:22,  2.08s/it] 95%|█████████▌| 190/200 [07:57<00:21,  2.17s/it] 96%|█████████▌| 191/200 [08:00<00:20,  2.23s/it] 96%|█████████▌| 192/200 [08:02<00:18,  2.29s/it] 96%|█████████▋| 193/200 [08:05<00:16,  2.33s/it] 97%|█████████▋| 194/200 [08:07<00:14,  2.36s/it] 98%|█████████▊| 195/200 [08:09<00:11,  2.36s/it] 98%|█████████▊| 196/200 [08:12<00:09,  2.38s/it] 98%|█████████▊| 197/200 [08:14<00:07,  2.38s/it] 99%|█████████▉| 198/200 [08:17<00:04,  2.37s/it]100%|█████████▉| 199/200 [08:19<00:02,  2.37s/it]100%|██████████| 200/200 [08:21<00:00,  2.38s/it][INFO|trainer.py:3846] 2024-11-19 16:05:03,583 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-large-adalora/checkpoint-200
[INFO|configuration_utils.py:690] 2024-11-19 16:05:03,597 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 16:05:03,597 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-large",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-19 16:05:03,606 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-large-adalora/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 16:05:03,606 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-large-adalora/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-19 16:05:03,672 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [08:21<00:00,  2.38s/it]100%|██████████| 200/200 [08:21<00:00,  2.51s/it]
[INFO|trainer.py:3846] 2024-11-19 16:05:03,675 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-large-adalora
[INFO|configuration_utils.py:690] 2024-11-19 16:05:03,686 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 16:05:03,687 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2-large",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2462] 2024-11-19 16:05:03,695 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-large-adalora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 16:05:03,695 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-large-adalora/special_tokens_map.json
{'train_runtime': 501.9711, 'train_samples_per_second': 6.375, 'train_steps_per_second': 0.398, 'train_loss': 2.6937576293945313, 'epoch': 0.03}
***** train metrics *****
  epoch                    =     0.0274
  total_flos               = 12984518GF
  train_loss               =     2.6938
  train_runtime            = 0:08:21.97
  train_samples            =     116582
  train_samples_per_second =      6.375
  train_steps_per_second   =      0.398
11/19/2024 16:05:03 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-19 16:05:03,744 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-19 16:05:03,744 >>   Num examples = 479
[INFO|trainer.py:4167] 2024-11-19 16:05:03,744 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/15 [00:00<?, ?it/s] 13%|█▎        | 2/15 [00:02<00:14,  1.08s/it] 20%|██        | 3/15 [00:04<00:17,  1.42s/it] 27%|██▋       | 4/15 [00:05<00:17,  1.61s/it] 33%|███▎      | 5/15 [00:07<00:16,  1.69s/it] 40%|████      | 6/15 [00:09<00:15,  1.74s/it] 47%|████▋     | 7/15 [00:11<00:14,  1.77s/it] 53%|█████▎    | 8/15 [00:13<00:12,  1.79s/it] 60%|██████    | 9/15 [00:15<00:11,  1.84s/it] 67%|██████▋   | 10/15 [00:17<00:09,  1.84s/it] 73%|███████▎  | 11/15 [00:18<00:07,  1.84s/it] 80%|████████  | 12/15 [00:20<00:05,  1.84s/it] 87%|████████▋ | 13/15 [00:22<00:03,  1.84s/it] 93%|█████████▎| 14/15 [00:24<00:01,  1.91s/it]100%|██████████| 15/15 [00:26<00:00,  1.97s/it]100%|██████████| 15/15 [00:27<00:00,  1.84s/it]
[INFO|modelcard.py:449] 2024-11-19 16:05:34,049 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.47223871824855057}]}
***** eval metrics *****
  epoch                   =     0.0274
  eval_accuracy           =     0.4722
  eval_loss               =     2.7662
  eval_runtime            = 0:00:30.30
  eval_samples            =        479
  eval_samples_per_second =     15.807
  eval_steps_per_second   =      0.495
  perplexity              =    15.8975
------------------------------------
LagEmbed: Training openai-community/gpt2-large on wikitext-2-raw-v1 and wikitext-103-raw-v1 with LagEmbed (in_channels=1280, n_components=3, dof=4)
11/19/2024 16:05:39 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/19/2024 16:05:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm-transfer-gpt2-large-lagembed/runs/Nov19_16-05-39_cs-Precision-7960-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=200,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/test-clm-transfer-gpt2-large-lagembed,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/test-clm-transfer-gpt2-large-lagembed,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
11/19/2024 16:05:39 - WARNING - datasets.load - Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Mon Nov 18 18:39:39 2024).
11/19/2024 16:05:39 - WARNING - datasets.packaged_modules.cache.cache - Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Mon Nov 18 18:39:39 2024).
Using custom data configuration wikitext-2-raw-v1
11/19/2024 16:05:39 - INFO - datasets.builder - Using custom data configuration wikitext-2-raw-v1
Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
11/19/2024 16:05:39 - INFO - datasets.info - Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
Overwrite dataset info from restored data version if exists.
11/19/2024 16:05:39 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 16:05:39 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
11/19/2024 16:05:39 - WARNING - datasets.load - Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-103-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Tue Nov 19 14:43:00 2024).
11/19/2024 16:05:39 - WARNING - datasets.packaged_modules.cache.cache - Found the latest cached dataset configuration 'wikitext-103-raw-v1' at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Tue Nov 19 14:43:00 2024).
Using custom data configuration wikitext-103-raw-v1
11/19/2024 16:05:39 - INFO - datasets.builder - Using custom data configuration wikitext-103-raw-v1
Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
11/19/2024 16:05:39 - INFO - datasets.info - Loading Dataset Infos from /home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/datasets/packaged_modules/cache
Overwrite dataset info from restored data version if exists.
11/19/2024 16:05:39 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
11/19/2024 16:05:39 - INFO - datasets.info - Loading Dataset info from /home/cs/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
[INFO|configuration_utils.py:690] 2024-11-19 16:05:39,442 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/config.json
[INFO|configuration_utils.py:759] 2024-11-19 16:05:39,443 >> Model config GPT2Config {
  "_name_or_path": "/tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.47.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2024-11-19 16:05:39,453 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 16:05:39,453 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-11-19 16:05:39,453 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 16:05:39,453 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 16:05:39,453 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-11-19 16:05:39,453 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3944] 2024-11-19 16:05:39,549 >> loading weights file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/model.safetensors
[INFO|configuration_utils.py:1104] 2024-11-19 16:05:39,557 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4813] 2024-11-19 16:05:39,655 >> All model checkpoint weights were used when initializing LagGPT2LMHeadModel.

[INFO|modeling_utils.py:4821] 2024-11-19 16:05:39,655 >> All the weights of LagGPT2LMHeadModel were initialized from the model checkpoint at /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LagGPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1057] 2024-11-19 16:05:39,658 >> loading configuration file /tmp/test-clm-gpt2-large-base-wikitext-2-raw-v1/generation_config.json
[INFO|configuration_utils.py:1104] 2024-11-19 16:05:39,658 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5d1fd704fc8fd842.arrow
11/19/2024 16:05:39 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5d1fd704fc8fd842.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-3ef4922073f8db54.arrow
11/19/2024 16:05:39 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-3ef4922073f8db54.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-aa9bb39585b0b8e5.arrow
11/19/2024 16:05:39 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-aa9bb39585b0b8e5.arrow
Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-904b3824cd400b17.arrow
11/19/2024 16:05:39 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cs/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-904b3824cd400b17.arrow
11/19/2024 16:05:41 - WARNING - evaluate.loading - Using the latest cached version of the module from /home/cs/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Nov 18 09:05:53 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
[WARNING|trainer.py:664] 2024-11-19 16:05:42,748 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2335] 2024-11-19 16:05:42,869 >> ***** Running training *****
[INFO|trainer.py:2336] 2024-11-19 16:05:42,869 >>   Num examples = 116,582
[INFO|trainer.py:2337] 2024-11-19 16:05:42,869 >>   Num Epochs = 1
[INFO|trainer.py:2338] 2024-11-19 16:05:42,869 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2340] 2024-11-19 16:05:42,869 >>   Training with DataParallel so batch size has been adjusted to: 16
[INFO|trainer.py:2341] 2024-11-19 16:05:42,869 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2342] 2024-11-19 16:05:42,869 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2343] 2024-11-19 16:05:42,869 >>   Total optimization steps = 200
[INFO|trainer.py:2344] 2024-11-19 16:05:42,870 >>   Number of trainable parameters = 606,927
  0%|          | 0/200 [00:00<?, ?it/s]/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/200 [00:05<19:47,  5.97s/it]  1%|          | 2/200 [00:07<10:29,  3.18s/it]  2%|▏         | 3/200 [00:08<07:30,  2.29s/it]  2%|▏         | 4/200 [00:09<06:05,  1.87s/it]  2%|▎         | 5/200 [00:10<05:18,  1.63s/it]  3%|▎         | 6/200 [00:12<04:49,  1.49s/it]  4%|▎         | 7/200 [00:13<04:30,  1.40s/it]  4%|▍         | 8/200 [00:14<04:18,  1.35s/it]  4%|▍         | 9/200 [00:15<04:14,  1.33s/it]  5%|▌         | 10/200 [00:17<04:05,  1.29s/it]  6%|▌         | 11/200 [00:18<04:00,  1.27s/it]  6%|▌         | 12/200 [00:19<03:56,  1.26s/it]  6%|▋         | 13/200 [00:20<03:53,  1.25s/it]  7%|▋         | 14/200 [00:21<03:50,  1.24s/it]  8%|▊         | 15/200 [00:23<03:48,  1.23s/it]  8%|▊         | 16/200 [00:24<03:49,  1.25s/it]  8%|▊         | 17/200 [00:25<03:47,  1.24s/it]  9%|▉         | 18/200 [00:26<03:45,  1.24s/it] 10%|▉         | 19/200 [00:28<03:43,  1.23s/it] 10%|█         | 20/200 [00:29<03:41,  1.23s/it] 10%|█         | 21/200 [00:30<03:39,  1.23s/it] 11%|█         | 22/200 [00:31<03:38,  1.22s/it] 12%|█▏        | 23/200 [00:32<03:36,  1.22s/it] 12%|█▏        | 24/200 [00:34<03:35,  1.22s/it] 12%|█▎        | 25/200 [00:35<03:37,  1.24s/it] 13%|█▎        | 26/200 [00:36<03:34,  1.23s/it] 14%|█▎        | 27/200 [00:37<03:32,  1.23s/it] 14%|█▍        | 28/200 [00:39<03:30,  1.23s/it] 14%|█▍        | 29/200 [00:40<03:29,  1.23s/it] 15%|█▌        | 30/200 [00:41<03:27,  1.22s/it] 16%|█▌        | 31/200 [00:42<03:26,  1.22s/it] 16%|█▌        | 32/200 [00:44<03:25,  1.22s/it] 16%|█▋        | 33/200 [00:45<03:24,  1.22s/it] 17%|█▋        | 34/200 [00:46<03:26,  1.24s/it] 18%|█▊        | 35/200 [00:47<03:23,  1.24s/it] 18%|█▊        | 36/200 [00:48<03:21,  1.23s/it] 18%|█▊        | 37/200 [00:50<03:20,  1.23s/it] 19%|█▉        | 38/200 [00:51<03:18,  1.23s/it] 20%|█▉        | 39/200 [00:52<03:17,  1.23s/it] 20%|██        | 40/200 [00:53<03:15,  1.22s/it] 20%|██        | 41/200 [00:55<03:14,  1.22s/it] 21%|██        | 42/200 [00:56<03:13,  1.22s/it] 22%|██▏       | 43/200 [00:57<03:15,  1.24s/it] 22%|██▏       | 44/200 [00:58<03:12,  1.24s/it] 22%|██▎       | 45/200 [01:00<03:11,  1.23s/it] 23%|██▎       | 46/200 [01:01<03:08,  1.23s/it] 24%|██▎       | 47/200 [01:02<03:07,  1.23s/it] 24%|██▍       | 48/200 [01:03<03:06,  1.23s/it] 24%|██▍       | 49/200 [01:04<03:05,  1.23s/it] 25%|██▌       | 50/200 [01:06<03:03,  1.22s/it] 26%|██▌       | 51/200 [01:07<03:07,  1.26s/it] 26%|██▌       | 52/200 [01:08<03:04,  1.25s/it] 26%|██▋       | 53/200 [01:09<03:02,  1.24s/it] 27%|██▋       | 54/200 [01:11<02:59,  1.23s/it] 28%|██▊       | 55/200 [01:12<02:58,  1.23s/it] 28%|██▊       | 56/200 [01:13<02:56,  1.23s/it] 28%|██▊       | 57/200 [01:14<02:55,  1.23s/it] 29%|██▉       | 58/200 [01:16<02:53,  1.22s/it] 30%|██▉       | 59/200 [01:17<02:52,  1.22s/it] 30%|███       | 60/200 [01:18<02:51,  1.22s/it] 30%|███       | 61/200 [01:19<02:52,  1.24s/it] 31%|███       | 62/200 [01:20<02:50,  1.23s/it] 32%|███▏      | 63/200 [01:22<02:48,  1.23s/it] 32%|███▏      | 64/200 [01:23<02:46,  1.23s/it] 32%|███▎      | 65/200 [01:24<02:45,  1.23s/it] 33%|███▎      | 66/200 [01:25<02:43,  1.22s/it] 34%|███▎      | 67/200 [01:27<02:42,  1.22s/it] 34%|███▍      | 68/200 [01:28<02:41,  1.22s/it] 34%|███▍      | 69/200 [01:29<02:40,  1.22s/it] 35%|███▌      | 70/200 [01:30<02:41,  1.24s/it] 36%|███▌      | 71/200 [01:32<02:39,  1.24s/it] 36%|███▌      | 72/200 [01:33<02:37,  1.23s/it] 36%|███▋      | 73/200 [01:34<02:36,  1.23s/it] 37%|███▋      | 74/200 [01:35<02:34,  1.23s/it] 38%|███▊      | 75/200 [01:36<02:33,  1.23s/it] 38%|███▊      | 76/200 [01:38<02:31,  1.22s/it] 38%|███▊      | 77/200 [01:39<02:30,  1.22s/it] 39%|███▉      | 78/200 [01:40<02:31,  1.24s/it] 40%|███▉      | 79/200 [01:41<02:29,  1.24s/it] 40%|████      | 80/200 [01:43<02:27,  1.23s/it] 40%|████      | 81/200 [01:44<02:26,  1.23s/it] 41%|████      | 82/200 [01:45<02:24,  1.23s/it] 42%|████▏     | 83/200 [01:46<02:23,  1.23s/it] 42%|████▏     | 84/200 [01:48<02:21,  1.22s/it] 42%|████▎     | 85/200 [01:49<02:20,  1.22s/it] 43%|████▎     | 86/200 [01:50<02:19,  1.22s/it] 44%|████▎     | 87/200 [01:51<02:20,  1.24s/it] 44%|████▍     | 88/200 [01:52<02:18,  1.24s/it] 44%|████▍     | 89/200 [01:54<02:16,  1.23s/it] 45%|████▌     | 90/200 [01:55<02:15,  1.23s/it] 46%|████▌     | 91/200 [01:56<02:13,  1.23s/it] 46%|████▌     | 92/200 [01:57<02:12,  1.23s/it] 46%|████▋     | 93/200 [01:59<02:11,  1.23s/it] 47%|████▋     | 94/200 [02:00<02:09,  1.22s/it] 48%|████▊     | 95/200 [02:01<02:08,  1.22s/it] 48%|████▊     | 96/200 [02:02<02:09,  1.24s/it] 48%|████▊     | 97/200 [02:04<02:07,  1.24s/it] 49%|████▉     | 98/200 [02:05<02:05,  1.23s/it] 50%|████▉     | 99/200 [02:06<02:04,  1.23s/it] 50%|█████     | 100/200 [02:07<02:02,  1.23s/it] 50%|█████     | 101/200 [02:08<02:01,  1.23s/it] 51%|█████     | 102/200 [02:10<01:59,  1.22s/it] 52%|█████▏    | 103/200 [02:11<01:58,  1.22s/it] 52%|█████▏    | 104/200 [02:12<01:57,  1.22s/it] 52%|█████▎    | 105/200 [02:13<01:58,  1.25s/it] 53%|█████▎    | 106/200 [02:15<01:56,  1.24s/it] 54%|█████▎    | 107/200 [02:16<01:54,  1.23s/it] 54%|█████▍    | 108/200 [02:17<01:53,  1.23s/it] 55%|█████▍    | 109/200 [02:18<01:51,  1.23s/it] 55%|█████▌    | 110/200 [02:19<01:50,  1.22s/it] 56%|█████▌    | 111/200 [02:21<01:48,  1.22s/it] 56%|█████▌    | 112/200 [02:22<01:47,  1.22s/it] 56%|█████▋    | 113/200 [02:23<01:46,  1.23s/it] 57%|█████▋    | 114/200 [02:24<01:46,  1.24s/it] 57%|█████▊    | 115/200 [02:26<01:45,  1.24s/it] 58%|█████▊    | 116/200 [02:27<01:43,  1.23s/it] 58%|█████▊    | 117/200 [02:28<01:42,  1.23s/it] 59%|█████▉    | 118/200 [02:29<01:40,  1.23s/it] 60%|█████▉    | 119/200 [02:31<01:39,  1.23s/it] 60%|██████    | 120/200 [02:32<01:38,  1.23s/it] 60%|██████    | 121/200 [02:33<01:36,  1.23s/it] 61%|██████    | 122/200 [02:34<01:35,  1.22s/it] 62%|██████▏   | 123/200 [02:36<01:35,  1.24s/it] 62%|██████▏   | 124/200 [02:37<01:33,  1.23s/it] 62%|██████▎   | 125/200 [02:38<01:32,  1.23s/it] 63%|██████▎   | 126/200 [02:39<01:31,  1.23s/it] 64%|██████▎   | 127/200 [02:40<01:29,  1.23s/it] 64%|██████▍   | 128/200 [02:42<01:28,  1.23s/it] 64%|██████▍   | 129/200 [02:43<01:27,  1.23s/it] 65%|██████▌   | 130/200 [02:44<01:25,  1.23s/it] 66%|██████▌   | 131/200 [02:45<01:25,  1.24s/it] 66%|██████▌   | 132/200 [02:47<01:26,  1.27s/it] 66%|██████▋   | 133/200 [02:48<01:24,  1.26s/it] 67%|██████▋   | 134/200 [02:49<01:22,  1.26s/it] 68%|██████▊   | 135/200 [02:50<01:21,  1.25s/it] 68%|██████▊   | 136/200 [02:52<01:19,  1.24s/it] 68%|██████▊   | 137/200 [02:53<01:18,  1.24s/it] 69%|██████▉   | 138/200 [02:54<01:16,  1.23s/it] 70%|██████▉   | 139/200 [02:55<01:15,  1.23s/it] 70%|███████   | 140/200 [02:57<01:13,  1.23s/it] 70%|███████   | 141/200 [02:58<01:13,  1.25s/it] 71%|███████   | 142/200 [02:59<01:12,  1.25s/it] 72%|███████▏  | 143/200 [03:00<01:10,  1.25s/it] 72%|███████▏  | 144/200 [03:02<01:09,  1.24s/it] 72%|███████▎  | 145/200 [03:03<01:07,  1.24s/it] 73%|███████▎  | 146/200 [03:04<01:06,  1.23s/it] 74%|███████▎  | 147/200 [03:05<01:05,  1.23s/it] 74%|███████▍  | 148/200 [03:06<01:04,  1.23s/it] 74%|███████▍  | 149/200 [03:08<01:02,  1.23s/it] 75%|███████▌  | 150/200 [03:09<01:02,  1.25s/it] 76%|███████▌  | 151/200 [03:10<01:00,  1.24s/it] 76%|███████▌  | 152/200 [03:11<00:59,  1.24s/it] 76%|███████▋  | 153/200 [03:13<00:58,  1.24s/it] 77%|███████▋  | 154/200 [03:14<00:56,  1.24s/it] 78%|███████▊  | 155/200 [03:15<00:55,  1.23s/it] 78%|███████▊  | 156/200 [03:16<00:54,  1.23s/it] 78%|███████▊  | 157/200 [03:18<00:52,  1.23s/it] 79%|███████▉  | 158/200 [03:19<00:51,  1.23s/it] 80%|███████▉  | 159/200 [03:20<00:51,  1.25s/it] 80%|████████  | 160/200 [03:21<00:49,  1.25s/it] 80%|████████  | 161/200 [03:23<00:48,  1.24s/it] 81%|████████  | 162/200 [03:24<00:47,  1.24s/it] 82%|████████▏ | 163/200 [03:25<00:45,  1.24s/it] 82%|████████▏ | 164/200 [03:26<00:44,  1.23s/it] 82%|████████▎ | 165/200 [03:27<00:43,  1.23s/it] 83%|████████▎ | 166/200 [03:29<00:41,  1.23s/it] 84%|████████▎ | 167/200 [03:30<00:40,  1.23s/it] 84%|████████▍ | 168/200 [03:31<00:39,  1.25s/it] 84%|████████▍ | 169/200 [03:33<00:39,  1.26s/it] 85%|████████▌ | 170/200 [03:34<00:38,  1.27s/it] 86%|████████▌ | 171/200 [03:35<00:37,  1.29s/it] 86%|████████▌ | 172/200 [03:36<00:36,  1.29s/it] 86%|████████▋ | 173/200 [03:38<00:34,  1.28s/it] 87%|████████▋ | 174/200 [03:39<00:32,  1.26s/it] 88%|████████▊ | 175/200 [03:40<00:31,  1.25s/it] 88%|████████▊ | 176/200 [03:41<00:29,  1.24s/it] 88%|████████▊ | 177/200 [03:43<00:29,  1.26s/it] 89%|████████▉ | 178/200 [03:44<00:27,  1.25s/it] 90%|████████▉ | 179/200 [03:45<00:26,  1.25s/it] 90%|█████████ | 180/200 [03:46<00:24,  1.24s/it] 90%|█████████ | 181/200 [03:48<00:23,  1.24s/it] 91%|█████████ | 182/200 [03:49<00:22,  1.23s/it] 92%|█████████▏| 183/200 [03:50<00:20,  1.23s/it] 92%|█████████▏| 184/200 [03:51<00:19,  1.23s/it] 92%|█████████▎| 185/200 [03:53<00:18,  1.24s/it] 93%|█████████▎| 186/200 [03:54<00:17,  1.27s/it] 94%|█████████▎| 187/200 [03:55<00:16,  1.27s/it] 94%|█████████▍| 188/200 [03:56<00:15,  1.27s/it] 94%|█████████▍| 189/200 [03:58<00:13,  1.26s/it] 95%|█████████▌| 190/200 [03:59<00:12,  1.25s/it] 96%|█████████▌| 191/200 [04:00<00:11,  1.24s/it] 96%|█████████▌| 192/200 [04:01<00:09,  1.24s/it] 96%|█████████▋| 193/200 [04:03<00:08,  1.24s/it] 97%|█████████▋| 194/200 [04:04<00:07,  1.23s/it] 98%|█████████▊| 195/200 [04:05<00:06,  1.25s/it] 98%|█████████▊| 196/200 [04:06<00:04,  1.24s/it] 98%|█████████▊| 197/200 [04:07<00:03,  1.24s/it] 99%|█████████▉| 198/200 [04:09<00:02,  1.23s/it]100%|█████████▉| 199/200 [04:10<00:01,  1.23s/it]100%|██████████| 200/200 [04:11<00:00,  1.23s/it][INFO|trainer.py:3846] 2024-11-19 16:09:54,519 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-large-lagembed/checkpoint-200
[INFO|configuration_utils.py:416] 2024-11-19 16:09:54,520 >> Configuration saved in /tmp/test-clm-transfer-gpt2-large-lagembed/checkpoint-200/config.json
[INFO|configuration_utils.py:873] 2024-11-19 16:09:54,521 >> Configuration saved in /tmp/test-clm-transfer-gpt2-large-lagembed/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-19 16:09:58,203 >> Model weights saved in /tmp/test-clm-transfer-gpt2-large-lagembed/checkpoint-200/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-19 16:09:58,204 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-large-lagembed/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 16:09:58,204 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-large-lagembed/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2605] 2024-11-19 16:09:58,246 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [04:15<00:00,  1.23s/it]100%|██████████| 200/200 [04:15<00:00,  1.28s/it]
[INFO|trainer.py:3846] 2024-11-19 16:09:58,248 >> Saving model checkpoint to /tmp/test-clm-transfer-gpt2-large-lagembed
[INFO|configuration_utils.py:416] 2024-11-19 16:09:58,249 >> Configuration saved in /tmp/test-clm-transfer-gpt2-large-lagembed/config.json
[INFO|configuration_utils.py:873] 2024-11-19 16:09:58,249 >> Configuration saved in /tmp/test-clm-transfer-gpt2-large-lagembed/generation_config.json
[INFO|modeling_utils.py:3045] 2024-11-19 16:10:02,179 >> Model weights saved in /tmp/test-clm-transfer-gpt2-large-lagembed/model.safetensors
[INFO|tokenization_utils_base.py:2462] 2024-11-19 16:10:02,180 >> tokenizer config file saved in /tmp/test-clm-transfer-gpt2-large-lagembed/tokenizer_config.json
[INFO|tokenization_utils_base.py:2471] 2024-11-19 16:10:02,180 >> Special tokens file saved in /tmp/test-clm-transfer-gpt2-large-lagembed/special_tokens_map.json
{'train_runtime': 255.377, 'train_samples_per_second': 12.53, 'train_steps_per_second': 0.783, 'train_loss': 2.62610107421875, 'epoch': 0.03}
***** train metrics *****
  epoch                    =     0.0274
  total_flos               = 12982128GF
  train_loss               =     2.6261
  train_runtime            = 0:04:15.37
  train_samples            =     116582
  train_samples_per_second =      12.53
  train_steps_per_second   =      0.783
11/19/2024 16:10:02 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4162] 2024-11-19 16:10:02,216 >> 
***** Running Evaluation *****
[INFO|trainer.py:4164] 2024-11-19 16:10:02,216 >>   Num examples = 479
[INFO|trainer.py:4167] 2024-11-19 16:10:02,216 >>   Batch size = 32
/home/cs/anaconda3/envs/torch121/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/15 [00:00<?, ?it/s] 13%|█▎        | 2/15 [00:01<00:11,  1.13it/s] 20%|██        | 3/15 [00:03<00:15,  1.26s/it] 27%|██▋       | 4/15 [00:05<00:16,  1.50s/it] 33%|███▎      | 5/15 [00:07<00:15,  1.60s/it] 40%|████      | 6/15 [00:09<00:14,  1.66s/it] 47%|████▋     | 7/15 [00:10<00:13,  1.69s/it] 53%|█████▎    | 8/15 [00:12<00:12,  1.72s/it] 60%|██████    | 9/15 [00:14<00:10,  1.74s/it] 67%|██████▋   | 10/15 [00:16<00:08,  1.75s/it] 73%|███████▎  | 11/15 [00:17<00:07,  1.76s/it] 80%|████████  | 12/15 [00:19<00:05,  1.77s/it] 87%|████████▋ | 13/15 [00:21<00:03,  1.79s/it] 93%|█████████▎| 14/15 [00:23<00:01,  1.78s/it]100%|██████████| 15/15 [00:25<00:00,  1.77s/it]100%|██████████| 15/15 [00:25<00:00,  1.73s/it]
[INFO|modelcard.py:449] 2024-11-19 16:10:29,973 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.47242034460028937}]}
***** eval metrics *****
  epoch                   =     0.0274
  eval_accuracy           =     0.4724
  eval_loss               =     2.6369
  eval_runtime            = 0:00:27.75
  eval_samples            =        479
  eval_samples_per_second =     17.257
  eval_steps_per_second   =       0.54
  perplexity              =      13.97
------------------------------------
